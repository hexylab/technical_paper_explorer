title,id,arxiv_url,published,summary,citations
GroupContrast: Semantic-aware Self-supervised Representation Learning for 3D Understanding,2403.09639v1,http://arxiv.org/abs/2403.09639v1,2024-03-14 17:59:59+00:00,"Self-supervised 3D representation learning aims to learn effective
representations from large-scale unlabeled point clouds. Most existing
approaches adopt point discrimination as the pretext task, which assigns
matched points in two distinct views as positive pairs and unmatched points as
negative pairs. However, this approach often results in semantically identical
points having dissimilar representations, leading to a high number of false
negatives and introducing a ""semantic conflict"" problem. To address this issue,
we propose GroupContrast, a novel approach that combines segment grouping and
semantic-aware contrastive learning. Segment grouping partitions points into
semantically meaningful regions, which enhances semantic coherence and provides
semantic guidance for the subsequent contrastive representation learning.
Semantic-aware contrastive learning augments the semantic information extracted
from segment grouping and helps to alleviate the issue of ""semantic conflict"".
We conducted extensive experiments on multiple 3D scene understanding tasks.
The results demonstrate that GroupContrast learns semantically meaningful
representations and achieves promising transfer learning performance.",0
Which LLM to Play? Convergence-Aware Online Model Selection with Time-Increasing Bandits,2403.07213v1,http://arxiv.org/abs/2403.07213v1,2024-03-11 23:52:46+00:00,"Web-based applications such as chatbots, search engines and news
recommendations continue to grow in scale and complexity with the recent surge
in the adoption of LLMs. Online model selection has thus garnered increasing
attention due to the need to choose the best model among a diverse set while
balancing task reward and exploration cost. Organizations faces decisions like
whether to employ a costly API-based LLM or a locally finetuned small LLM,
weighing cost against performance. Traditional selection methods often evaluate
every candidate model before choosing one, which are becoming impractical given
the rising costs of training and finetuning LLMs. Moreover, it is undesirable
to allocate excessive resources towards exploring poor-performing models. While
some recent works leverage online bandit algorithm to manage such
exploration-exploitation trade-off in model selection, they tend to overlook
the increasing-then-converging trend in model performances as the model is
iteratively finetuned, leading to less accurate predictions and suboptimal
model selections.
  In this paper, we propose a time-increasing bandit algorithm TI-UCB, which
effectively predicts the increase of model performances due to finetuning and
efficiently balances exploration and exploitation in model selection. To
further capture the converging points of models, we develop a change detection
mechanism by comparing consecutive increase predictions. We theoretically prove
that our algorithm achieves a logarithmic regret upper bound in a typical
increasing bandit setting, which implies a fast convergence rate. The advantage
of our method is also empirically validated through extensive experiments on
classification model selection and online selection of LLMs. Our results
highlight the importance of utilizing increasing-then-converging pattern for
more efficient and economic model selection in the deployment of LLMs.",0
Towards Zero-shot Human-Object Interaction Detection via Vision-Language Integration,2403.07246v1,http://arxiv.org/abs/2403.07246v1,2024-03-12 02:07:23+00:00,"Human-object interaction (HOI) detection aims to locate human-object pairs
and identify their interaction categories in images. Most existing methods
primarily focus on supervised learning, which relies on extensive manual HOI
annotations. In this paper, we propose a novel framework, termed Knowledge
Integration to HOI (KI2HOI), that effectively integrates the knowledge of
visual-language model to improve zero-shot HOI detection. Specifically, the
verb feature learning module is designed based on visual semantics, by
employing the verb extraction decoder to convert corresponding verb queries
into interaction-specific category representations. We develop an effective
additive self-attention mechanism to generate more comprehensive visual
representations. Moreover, the innovative interaction representation decoder
effectively extracts informative regions by integrating spatial and visual
feature information through a cross-attention mechanism. To deal with zero-shot
learning in low-data, we leverage a priori knowledge from the CLIP text encoder
to initialize the linear classifier for enhanced interaction understanding.
Extensive experiments conducted on the mainstream HICO-DET and V-COCO datasets
demonstrate that our model outperforms the previous methods in various
zero-shot and full-supervised settings.",0
Dataset Condensation for Time Series Classification via Dual Domain Matching,2403.07245v1,http://arxiv.org/abs/2403.07245v1,2024-03-12 02:05:06+00:00,"Time series data has been demonstrated to be crucial in various research
fields. The management of large quantities of time series data presents
challenges in terms of deep learning tasks, particularly for training a deep
neural network. Recently, a technique named \textit{Dataset Condensation} has
emerged as a solution to this problem. This technique generates a smaller
synthetic dataset that has comparable performance to the full real dataset in
downstream tasks such as classification. However, previous methods are
primarily designed for image and graph datasets, and directly adapting them to
the time series dataset leads to suboptimal performance due to their inability
to effectively leverage the rich information inherent in time series data,
particularly in the frequency domain. In this paper, we propose a novel
framework named Dataset \textit{\textbf{Cond}}ensation for
\textit{\textbf{T}}ime \textit{\textbf{S}}eries
\textit{\textbf{C}}lassification via Dual Domain Matching (\textbf{CondTSC})
which focuses on the time series classification dataset condensation task.
Different from previous methods, our proposed framework aims to generate a
condensed dataset that matches the surrogate objectives in both the time and
frequency domains. Specifically, CondTSC incorporates multi-view data
augmentation, dual domain training, and dual surrogate objectives to enhance
the dataset condensation process in the time and frequency domains. Through
extensive experiments, we demonstrate the effectiveness of our proposed
framework, which outperforms other baselines and learns a condensed synthetic
dataset that exhibits desirable characteristics such as conforming to the
distribution of the original data.",0
Time-Efficient Light-Field Acquisition Using Coded Aperture and Events,2403.07244v1,http://arxiv.org/abs/2403.07244v1,2024-03-12 02:04:17+00:00,"We propose a computational imaging method for time-efficient light-field
acquisition that combines a coded aperture with an event-based camera.
Different from the conventional coded-aperture imaging method, our method
applies a sequence of coding patterns during a single exposure for an image
frame. The parallax information, which is related to the differences in coding
patterns, is recorded as events. The image frame and events, all of which are
measured in a single exposure, are jointly used to computationally reconstruct
a light field. We also designed an algorithm pipeline for our method that is
end-to-end trainable on the basis of deep optics and compatible with real
camera hardware. We experimentally showed that our method can achieve more
accurate reconstruction than several other imaging methods with a single
exposure. We also developed a hardware prototype with the potential to complete
the measurement on the camera within 22 msec and demonstrated that light fields
from real 3-D scenes can be obtained with convincing visual quality. Our
software and supplementary video are available from our project website.",0
Calibrating Multi-modal Representations: A Pursuit of Group Robustness without Annotations,2403.07241v1,http://arxiv.org/abs/2403.07241v1,2024-03-12 01:47:17+00:00,"Fine-tuning pre-trained vision-language models, like CLIP, has yielded
success on diverse downstream tasks. However, several pain points persist for
this paradigm: (i) directly tuning entire pre-trained models becomes both
time-intensive and computationally costly. Additionally, these tuned models
tend to become highly specialized, limiting their practicality for real-world
deployment; (ii) recent studies indicate that pre-trained vision-language
classifiers may overly depend on spurious features -- patterns that correlate
with the target in training data, but are not related to the true labeling
function; and (iii) existing studies on mitigating the reliance on spurious
features, largely based on the assumption that we can identify such features,
does not provide definitive assurance for real-world applications. As a
piloting study, this work focuses on exploring mitigating the reliance on
spurious features for CLIP without using any group annotation. To this end, we
systematically study the existence of spurious correlation on CLIP and
CILP+ERM. We first, following recent work on Deep Feature Reweighting (DFR),
verify that last-layer retraining can greatly improve group robustness on
pretrained CLIP. In view of them, we advocate a lightweight representation
calibration method for fine-tuning CLIP, by first generating a calibration set
using the pretrained CLIP, and then calibrating representations of samples
within this set through contrastive learning, all without the need for group
labels. Extensive experiments and in-depth visualizations on several benchmarks
validate the effectiveness of our proposals, largely reducing reliance and
significantly boosting the model generalization.",0
Frequency-Aware Deepfake Detection: Improving Generalizability through Frequency Space Learning,2403.07240v1,http://arxiv.org/abs/2403.07240v1,2024-03-12 01:28:00+00:00,"This research addresses the challenge of developing a universal deepfake
detector that can effectively identify unseen deepfake images despite limited
training data. Existing frequency-based paradigms have relied on
frequency-level artifacts introduced during the up-sampling in GAN pipelines to
detect forgeries. However, the rapid advancements in synthesis technology have
led to specific artifacts for each generation model. Consequently, these
detectors have exhibited a lack of proficiency in learning the frequency domain
and tend to overfit to the artifacts present in the training data, leading to
suboptimal performance on unseen sources. To address this issue, we introduce a
novel frequency-aware approach called FreqNet, centered around frequency domain
learning, specifically designed to enhance the generalizability of deepfake
detectors. Our method forces the detector to continuously focus on
high-frequency information, exploiting high-frequency representation of
features across spatial and channel dimensions. Additionally, we incorporate a
straightforward frequency domain learning module to learn source-agnostic
features. It involves convolutional layers applied to both the phase spectrum
and amplitude spectrum between the Fast Fourier Transform (FFT) and Inverse
Fast Fourier Transform (iFFT). Extensive experimentation involving 17 GANs
demonstrates the effectiveness of our proposed method, showcasing
state-of-the-art performance (+9.8\%) while requiring fewer parameters. The
code is available at {\cred
\url{https://github.com/chuangchuangtan/FreqNet-DeepfakeDetection}}.",0
Partial Identification of Individual-Level Parameters Using Aggregate Data in a Nonparametric Binary Outcome Model,2403.07236v1,http://arxiv.org/abs/2403.07236v1,2024-03-12 01:14:35+00:00,"It is well known that the relationship between variables at the individual
level can be different from the relationship between those same variables
aggregated over individuals. This problem of aggregation becomes relevant when
the researcher wants to learn individual-level relationships, but only has
access to data that has been aggregated. In this paper, I develop a methodology
to partially identify linear combinations of conditional average outcomes from
aggregate data when the outcome of interest is binary, while imposing as few
restrictions on the underlying data generating process as possible. I construct
identified sets using an optimization program that allows for researchers to
impose additional shape restrictions. I also provide consistency results and
construct an inference procedure that is valid with aggregate data, which only
provides marginal information about each variable. I apply the methodology to
simulated and real-world data sets and find that the estimated identified sets
are too wide to be useful. This suggests that to obtain useful information from
aggregate data sets about individual-level relationships, researchers must
impose further assumptions that are carefully justified.",0
Tractable Joint Prediction and Planning over Discrete Behavior Modes for Urban Driving,2403.07232v1,http://arxiv.org/abs/2403.07232v1,2024-03-12 01:00:52+00:00,"Significant progress has been made in training multimodal trajectory
forecasting models for autonomous driving. However, effectively integrating
these models with downstream planners and model-based control approaches is
still an open problem. Although these models have conventionally been evaluated
for open-loop prediction, we show that they can be used to parameterize
autoregressive closed-loop models without retraining. We consider recent
trajectory prediction approaches which leverage learned anchor embeddings to
predict multiple trajectories, finding that these anchor embeddings can
parameterize discrete and distinct modes representing high-level driving
behaviors. We propose to perform fully reactive closed-loop planning over these
discrete latent modes, allowing us to tractably model the causal interactions
between agents at each step. We validate our approach on a suite of more
dynamic merging scenarios, finding that our approach avoids the $\textit{frozen
robot problem}$ which is pervasive in conventional planners. Our approach also
outperforms the previous state-of-the-art in CARLA on challenging dense traffic
scenarios when evaluated at realistic speeds.",0
Curry-DPO: Enhancing Alignment using Curriculum Learning & Ranked Preferences,2403.07230v1,http://arxiv.org/abs/2403.07230v1,2024-03-12 00:58:19+00:00,"Direct Preference Optimization (DPO) is an effective technique that leverages
pairwise preference data (usually one chosen and rejected response pair per
user prompt) to align LLMs to human preferences. In practice, multiple
responses can exist for a given prompt with varying quality relative to each
other. With availability of such quality ratings for multiple responses, we
propose utilizing these responses to create multiple preference pairs for a
given prompt. Our work focuses on systematically using the constructed multiple
preference pair in DPO training via curriculum learning methodology. In
particular, we order these multiple pairs of preference data from easy to hard
(emulating curriculum training) according to various criteria. We show detailed
comparisons of our proposed approach to the standard single-pair DPO setting.
Our method, which we call Curry-DPO consistently shows increased performance
gains on MTbench, Vicuna, WizardLM, and the UltraFeedback test set,
highlighting its effectiveness. More specifically, Curry-DPO achieves a score
of 7.43 on MT-bench with Zephy-7B model outperforming majority of existing LLMs
with similar parameter size. Curry-DPO also achieves the highest adjusted win
rates on Vicuna, WizardLM, and UltraFeedback test datasets (90.7%, 87.1%, and
87.9% respectively) in our experiments, with notable gains of upto 7.5% when
compared to standard DPO technique.",0
Learn and Search: An Elegant Technique for Object Lookup using Contrastive Learning,2403.07231v1,http://arxiv.org/abs/2403.07231v1,2024-03-12 00:58:19+00:00,"The rapid proliferation of digital content and the ever-growing need for
precise object recognition and segmentation have driven the advancement of
cutting-edge techniques in the field of object classification and segmentation.
This paper introduces ""Learn and Search"", a novel approach for object lookup
that leverages the power of contrastive learning to enhance the efficiency and
effectiveness of retrieval systems.
  In this study, we present an elegant and innovative methodology that
integrates deep learning principles and contrastive learning to tackle the
challenges of object search. Our extensive experimentation reveals compelling
results, with ""Learn and Search"" achieving superior Similarity Grid Accuracy,
showcasing its efficacy in discerning regions of utmost similarity within an
image relative to a cropped image.
  The seamless fusion of deep learning and contrastive learning to address the
intricacies of object identification not only promises transformative
applications in image recognition, recommendation systems, and content tagging
but also revolutionizes content-based search and retrieval. The amalgamation of
these techniques, as exemplified by ""Learn and Search,"" represents a
significant stride in the ongoing evolution of methodologies in the dynamic
realm of object classification and segmentation.",0
Physics-constrained Active Learning for Soil Moisture Estimation and Optimal Sensor Placement,2403.07228v1,http://arxiv.org/abs/2403.07228v1,2024-03-12 00:45:34+00:00,"Soil moisture is a crucial hydrological state variable that has significant
importance to the global environment and agriculture. Precise monitoring of
soil moisture in crop fields is critical to reducing agricultural drought and
improving crop yield. In-situ soil moisture sensors, which are buried at
pre-determined depths and distributed across the field, are promising solutions
for monitoring soil moisture. However, high-density sensor deployment is
neither economically feasible nor practical. Thus, to achieve a higher spatial
resolution of soil moisture dynamics using a limited number of sensors, we
integrate a physics-based agro-hydrological model based on Richards' equation
in a physics-constrained deep learning framework to accurately predict soil
moisture dynamics in the soil's root zone. This approach ensures that soil
moisture estimates align well with sensor observations while obeying physical
laws at the same time. Furthermore, to strategically identify the locations for
sensor placement, we introduce a novel active learning framework that combines
space-filling design and physics residual-based sampling to maximize data
acquisition potential with limited sensors. Our numerical results demonstrate
that integrating Physics-constrained Deep Learning (P-DL) with an active
learning strategy within a unified framework--named the Physics-constrained
Active Learning (P-DAL) framework--significantly improves the predictive
accuracy and effectiveness of field-scale soil moisture monitoring using
in-situ sensors.",0
LookupFFN: Making Transformers Compute-lite for CPU inference,2403.07221v1,http://arxiv.org/abs/2403.07221v1,2024-03-12 00:26:16+00:00,"While GPU clusters are the de facto choice for training large deep neural
network (DNN) models today, several reasons including ease of workflow,
security and cost have led to efforts investigating whether CPUs may be viable
for inference in routine use in many sectors of the industry. But the imbalance
between the compute capabilities of GPUs and CPUs is huge. Motivated by these
considerations, we study a module which is a workhorse within modern DNN
architectures, GEMM based Feed Forward Networks (FFNs), and assess the extent
to which it can be made compute- (or FLOP-) lite. Specifically, we propose an
alternative formulation (we call it LookupFFN) to GEMM based FFNs inspired by
the recent studies of using Locality Sensitive Hashing (LSH) to approximate
FFNs. Our formulation recasts most essential operations as a memory look-up,
leveraging the trade-off between the two resources on any platform: compute and
memory (since CPUs offer it in abundance). For RoBERTa language model
pretraining, our formulation achieves similar performance compared to GEMM
based FFNs, while dramatically reducing the required FLOP. Our development is
complemented with a detailed hardware profiling of strategies that will
maximize efficiency -- not just on contemporary hardware but on products that
will be offered in the near/medium term future. Code is avaiable at
\url{https://github.com/mlpen/LookupFFN}.",0
SoK: Can Trajectory Generation Combine Privacy and Utility?,2403.07218v1,http://arxiv.org/abs/2403.07218v1,2024-03-12 00:25:14+00:00,"While location trajectories represent a valuable data source for analyses and
location-based services, they can reveal sensitive information, such as
political and religious preferences. Differentially private publication
mechanisms have been proposed to allow for analyses under rigorous privacy
guarantees. However, the traditional protection schemes suffer from a limiting
privacy-utility trade-off and are vulnerable to correlation and reconstruction
attacks. Synthetic trajectory data generation and release represent a promising
alternative to protection algorithms. While initial proposals achieve
remarkable utility, they fail to provide rigorous privacy guarantees. This
paper proposes a framework for designing a privacy-preserving trajectory
publication approach by defining five design goals, particularly stressing the
importance of choosing an appropriate Unit of Privacy. Based on this framework,
we briefly discuss the existing trajectory protection approaches, emphasising
their shortcomings. This work focuses on the systematisation of the
state-of-the-art generative models for trajectories in the context of the
proposed framework. We find that no existing solution satisfies all
requirements. Thus, we perform an experimental study evaluating the
applicability of six sequential generative models to the trajectory domain.
Finally, we conclude that a generative trajectory model providing semantic
guarantees remains an open research question and propose concrete next steps
for future research.",0
Adaptive Gain Scheduling using Reinforcement Learning for Quadcopter Control,2403.07216v1,http://arxiv.org/abs/2403.07216v1,2024-03-12 00:08:54+00:00,"The paper presents a technique using reinforcement learning (RL) to adapt the
control gains of a quadcopter controller. Specifically, we employed Proximal
Policy Optimization (PPO) to train a policy which adapts the gains of a
cascaded feedback controller in-flight. The primary goal of this controller is
to minimize tracking error while following a specified trajectory. The paper's
key objective is to analyze the effectiveness of the adaptive gain policy and
compare it to the performance of a static gain control algorithm, where the
Integral Squared Error and Integral Time Squared Error are used as metrics. The
results show that the adaptive gain scheme achieves over 40$\%$ decrease in
tracking error as compared to the static gain controller.",0
Tracking Dynamic Gaussian Density with a Theoretically Optimal Sliding Window Approach,2403.07207v1,http://arxiv.org/abs/2403.07207v1,2024-03-11 23:21:26+00:00,"Dynamic density estimation is ubiquitous in many applications, including
computer vision and signal processing. One popular method to tackle this
problem is the ""sliding window"" kernel density estimator. There exist various
implementations of this method that use heuristically defined weight sequences
for the observed data. The weight sequence, however, is a key aspect of the
estimator affecting the tracking performance significantly. In this work, we
study the exact mean integrated squared error (MISE) of ""sliding window""
Gaussian Kernel Density Estimators for evolving Gaussian densities. We provide
a principled guide for choosing the optimal weight sequence by theoretically
characterizing the exact MISE, which can be formulated as constrained quadratic
programming. We present empirical evidence with synthetic datasets to show that
our weighting scheme indeed improves the tracking performance compared to
heuristic approaches.",0
Stochastic Extragradient with Random Reshuffling: Improved Convergence for Variational Inequalities,2403.07148v1,http://arxiv.org/abs/2403.07148v1,2024-03-11 20:35:52+00:00,"The Stochastic Extragradient (SEG) method is one of the most popular
algorithms for solving finite-sum min-max optimization and variational
inequality problems (VIPs) appearing in various machine learning tasks.
However, existing convergence analyses of SEG focus on its with-replacement
variants, while practical implementations of the method randomly reshuffle
components and sequentially use them. Unlike the well-studied with-replacement
variants, SEG with Random Reshuffling (SEG-RR) lacks established theoretical
guarantees. In this work, we provide a convergence analysis of SEG-RR for three
classes of VIPs: (i) strongly monotone, (ii) affine, and (iii) monotone. We
derive conditions under which SEG-RR achieves a faster convergence rate than
the uniform with-replacement sampling SEG. In the monotone setting, our
analysis of SEG-RR guarantees convergence to an arbitrary accuracy without
large batch sizes, a strong requirement needed in the classical
with-replacement SEG. As a byproduct of our results, we provide convergence
guarantees for Shuffle Once SEG (shuffles the data only at the beginning of the
algorithm) and the Incremental Extragradient (does not shuffle the data). We
supplement our analysis with experiments validating empirically the superior
performance of SEG-RR over the classical with-replacement sampling SEG.",0
How to Handle Sketch-Abstraction in Sketch-Based Image Retrieval?,2403.07203v1,http://arxiv.org/abs/2403.07203v1,2024-03-11 23:08:29+00:00,"In this paper, we propose a novel abstraction-aware sketch-based image
retrieval framework capable of handling sketch abstraction at varied levels.
Prior works had mainly focused on tackling sub-factors such as drawing style
and order, we instead attempt to model abstraction as a whole, and propose
feature-level and retrieval granularity-level designs so that the system builds
into its DNA the necessary means to interpret abstraction. On learning
abstraction-aware features, we for the first-time harness the rich semantic
embedding of pre-trained StyleGAN model, together with a novel
abstraction-level mapper that deciphers the level of abstraction and
dynamically selects appropriate dimensions in the feature matrix
correspondingly, to construct a feature matrix embedding that can be freely
traversed to accommodate different levels of abstraction. For granularity-level
abstraction understanding, we dictate that the retrieval model should not treat
all abstraction-levels equally and introduce a differentiable surrogate Acc.@q
loss to inject that understanding into the system. Different to the
gold-standard triplet loss, our Acc.@q loss uniquely allows a sketch to
narrow/broaden its focus in terms of how stringent the evaluation should be -
the more abstract a sketch, the less stringent (higher $q$). Extensive
experiments depict our method to outperform existing state-of-the-arts in
standard SBIR tasks along with challenging scenarios like early retrieval,
forensic sketch-photo matching, and style-invariant retrieval.",0
A multi-cohort study on prediction of acute brain dysfunction states using selective state space models,2403.07201v1,http://arxiv.org/abs/2403.07201v1,2024-03-11 22:58:11+00:00,"Assessing acute brain dysfunction (ABD), including delirium and coma in the
intensive care unit (ICU), is a critical challenge due to its prevalence and
severe implications for patient outcomes. Current diagnostic methods rely on
infrequent clinical observations, which can only determine a patient's ABD
status after onset. Our research attempts to solve these problems by harnessing
Electronic Health Records (EHR) data to develop automated methods for ABD
prediction for patients in the ICU. Existing models solely predict a single
state (e.g., either delirium or coma), require at least 24 hours of observation
data to make predictions, do not dynamically predict fluctuating ABD conditions
during ICU stay (typically a one-time prediction), and use small sample size,
proprietary single-hospital datasets. Our research fills these gaps in the
existing literature by dynamically predicting delirium, coma, and mortality for
12-hour intervals throughout an ICU stay and validating on two public datasets.
Our research also introduces the concept of dynamically predicting critical
transitions from non-ABD to ABD and between different ABD states in real time,
which could be clinically more informative for the hospital staff. We compared
the predictive performance of two state-of-the-art neural network models, the
MAMBA selective state space model and the Longformer Transformer model. Using
the MAMBA model, we achieved a mean area under the receiving operator
characteristic curve (AUROC) of 0.95 on outcome prediction of ABD for 12-hour
intervals. The model achieves a mean AUROC of 0.79 when predicting transitions
between ABD states. Our study uses a curated dataset from the University of
Florida Health Shands Hospital for internal validation and two publicly
available datasets, MIMIC-IV and eICU, for external validation, demonstrating
robustness across ICU stays from 203 hospitals and 140,945 patients.",0
Accelerating Interface Adaptation with User-Friendly Priors,2403.07192v1,http://arxiv.org/abs/2403.07192v1,2024-03-11 22:26:45+00:00,"Robots often need to convey information to human users. For example, robots
can leverage visual, auditory, and haptic interfaces to display their intent or
express their internal state. In some scenarios there are socially agreed upon
conventions for what these signals mean: e.g., a red light indicates an
autonomous car is slowing down. But as robots develop new capabilities and seek
to convey more complex data, the meaning behind their signals is not always
mutually understood: one user might think a flashing light indicates the
autonomous car is an aggressive driver, while another user might think the same
signal means the autonomous car is defensive. In this paper we enable robots to
adapt their interfaces to the current user so that the human's personalized
interpretation is aligned with the robot's meaning. We start with an
information theoretic end-to-end approach, which automatically tunes the
interface policy to optimize the correlation between human and robot. But to
ensure that this learning policy is intuitive -- and to accelerate how quickly
the interface adapts to the human -- we recognize that humans have priors over
how interfaces should function. For instance, humans expect interface signals
to be proportional and convex. Our approach biases the robot's interface
towards these priors, resulting in signals that are adapted to the current user
while still following social expectations. Our simulations and user study
results across $15$ participants suggest that these priors improve
robot-to-human communication. See videos here: https://youtu.be/Re3OLg57hp8",0
"$\mathbf{(N,K)}$-Puzzle: A Cost-Efficient Testbed for Benchmarking Reinforcement Learning Algorithms in Generative Language Model",2403.07191v1,http://arxiv.org/abs/2403.07191v1,2024-03-11 22:24:14+00:00,"Recent advances in reinforcement learning (RL) algorithms aim to enhance the
performance of language models at scale. Yet, there is a noticeable absence of
a cost-effective and standardized testbed tailored to evaluating and comparing
these algorithms. To bridge this gap, we present a generalized version of the
24-Puzzle: the $(N,K)$-Puzzle, which challenges language models to reach a
target value $K$ with $N$ integers. We evaluate the effectiveness of
established RL algorithms such as Proximal Policy Optimization (PPO), alongside
novel approaches like Identity Policy Optimization (IPO) and Direct Policy
Optimization (DPO).",0
Exploring gender differences in the Force Concept Inventory using a random effects meta-analysis of international studies,2403.07190v1,http://arxiv.org/abs/2403.07190v1,2024-03-11 22:23:20+00:00,"The force concept inventory (FCI) is one of the research-based assessments
(RBAs) established by the physics education research (PER) community to measure
students' understanding of Newtonian mechanics. Former works have often
recorded the notion of gendered mean FCI scores favoring male students notably
in the North America (NA) based studies. Nevertheless, these performance gaps
remain inconclusive and unexplored outside the NA context. This paper aims to
fill this gap by meta-analyzing the mean FCI scores between gender based on the
existing PER literature beyond the NA context. We analyzed the magnitude and
direction on the mean FCI scores between gender on the basis of primary
international studies published over the last two decades. We also explored the
moderating impact of international study characteristics on the meta-analytic
findings by performing a subgroup analysis to study the different study regions
stratified by two subgroups (NA vs non-NA authors). Thirty-eight studies
reporting the mean FCI scores by gender were included in the present
meta-analysis. We employed Hedges' g statistic to estimate to what degree the
mean FCI scores may be different between male and female students on each
study. Under a random effects model, we meta-analyzed the findings and
conducted a subgroup analysis to answer the research questions. In summary, our
meta-analysis indicated a significantly positive and moderate amount of
gendered mean FCI scores in favor of male students both in NA- and non-NA based
regions, and the performance gaps were wider in the NA-based studies.
Suggestions are discussed for promoting gender fairness in the FCI when
interpreting its scores for teaching, learning, and forthcoming studies.",0
UPS: Towards Foundation Models for PDE Solving via Cross-Modal Adaptation,2403.07187v1,http://arxiv.org/abs/2403.07187v1,2024-03-11 22:00:39+00:00,"We introduce UPS (Unified PDE Solver), an effective and data-efficient
approach to solve diverse spatiotemporal PDEs defined over various domains,
dimensions, and resolutions. UPS unifies different PDEs into a consistent
representation space and processes diverse collections of PDE data using a
unified network architecture that combines LLMs with domain-specific neural
operators. We train the network via a two-stage cross-modal adaptation process,
leveraging ideas of modality alignment and multi-task learning. By adapting
from pretrained LLMs and exploiting text-form meta information, we are able to
use considerably fewer training samples than previous methods while obtaining
strong empirical results. UPS outperforms existing baselines, often by a large
margin, on a wide range of 1D and 2D datasets in PDEBench, achieving
state-of-the-art results on 8 of 10 tasks considered. Meanwhile, it is capable
of few-shot transfer to different PDE families, coefficients, and resolutions.",0
Uncertainty in Graph Neural Networks: A Survey,2403.07185v1,http://arxiv.org/abs/2403.07185v1,2024-03-11 21:54:52+00:00,"Graph Neural Networks (GNNs) have been extensively used in various real-world
applications. However, the predictive uncertainty of GNNs stemming from diverse
sources such as inherent randomness in data and model training errors can lead
to unstable and erroneous predictions. Therefore, identifying, quantifying, and
utilizing uncertainty are essential to enhance the performance of the model for
the downstream tasks as well as the reliability of the GNN predictions. This
survey aims to provide a comprehensive overview of the GNNs from the
perspective of uncertainty with an emphasis on its integration in graph
learning. We compare and summarize existing graph uncertainty theory and
methods, alongside the corresponding downstream tasks. Thereby, we bridge the
gap between theory and practice, meanwhile connecting different GNN
communities. Moreover, our work provides valuable insights into promising
directions in this field.",0
Monitoring AI-Modified Content at Scale: A Case Study on the Impact of ChatGPT on AI Conference Peer Reviews,2403.07183v1,http://arxiv.org/abs/2403.07183v1,2024-03-11 21:51:39+00:00,"We present an approach for estimating the fraction of text in a large corpus
which is likely to be substantially modified or produced by a large language
model (LLM). Our maximum likelihood model leverages expert-written and
AI-generated reference texts to accurately and efficiently examine real-world
LLM-use at the corpus level. We apply this approach to a case study of
scientific peer review in AI conferences that took place after the release of
ChatGPT: ICLR 2024, NeurIPS 2023, CoRL 2023 and EMNLP 2023. Our results suggest
that between 6.5% and 16.9% of text submitted as peer reviews to these
conferences could have been substantially modified by LLMs, i.e. beyond
spell-checking or minor writing updates. The circumstances in which generated
text occurs offer insight into user behavior: the estimated fraction of
LLM-generated text is higher in reviews which report lower confidence, were
submitted close to the deadline, and from reviewers who are less likely to
respond to author rebuttals. We also observe corpus-level trends in generated
text which may be too subtle to detect at the individual level, and discuss the
implications of such trends on peer review. We call for future
interdisciplinary work to examine how LLM use is changing our information and
knowledge practices.",0
MAP-Elites with Transverse Assessment for Multimodal Problems in Creative Domains,2403.07182v1,http://arxiv.org/abs/2403.07182v1,2024-03-11 21:50:22+00:00,"The recent advances in language-based generative models have paved the way
for the orchestration of multiple generators of different artefact types (text,
image, audio, etc.) into one system. Presently, many open-source pre-trained
models combine text with other modalities, thus enabling shared vector
embeddings to be compared across different generators. Within this context we
propose a novel approach to handle multimodal creative tasks using Quality
Diversity evolution. Our contribution is a variation of the MAP-Elites
algorithm, MAP-Elites with Transverse Assessment (MEliTA), which is tailored
for multimodal creative tasks and leverages deep learned models that assess
coherence across modalities. MEliTA decouples the artefacts' modalities and
promotes cross-pollination between elites. As a test bed for this algorithm, we
generate text descriptions and cover images for a hypothetical video game and
assign each artefact a unique modality-specific behavioural characteristic.
Results indicate that MEliTA can improve text-to-image mappings within the
solution space, compared to a baseline MAP-Elites algorithm that strictly
treats each image-text pair as one solution. Our approach represents a
significant step forward in multimodal bottom-up orchestration and lays the
groundwork for more complex systems coordinating multimodal creative agents in
the future.",0
Study of the Impact of the Big Data Era on Accounting and Auditing,2403.07180v1,http://arxiv.org/abs/2403.07180v1,2024-03-11 21:45:48+00:00,"Big data revolutionizes accounting and auditing, offering deep insights but
also introducing challenges like data privacy and security. With data from IoT,
social media, and transactions, traditional practices are evolving.
Professionals must adapt to these changes, utilizing AI and machine learning
for efficient data analysis and anomaly detection. Key to overcoming these
challenges are enhanced analytics tools, continuous learning, and industry
collaboration. By addressing these areas, the accounting and auditing fields
can harness big data's potential while ensuring accuracy, transparency, and
integrity in financial reporting. Keywords: Big Data, Accounting, Audit, Data
Privacy, AI, Machine Learning, Transparency.",0
3M-Diffusion: Latent Multi-Modal Diffusion for Text-Guided Generation of Molecular Graphs,2403.07179v1,http://arxiv.org/abs/2403.07179v1,2024-03-11 21:44:54+00:00,"Generating molecules with desired properties is a critical task with broad
applications in drug discovery and materials design. Inspired by recent
advances in large language models, there is a growing interest in using natural
language descriptions of molecules to generate molecules with the desired
properties. Most existing methods focus on generating molecules that precisely
match the text description. However, practical applications call for methods
that generate diverse, and ideally novel, molecules with the desired
properties. We propose 3M-Diffusion, a novel multi-modal molecular graph
generation method, to address this challenge. 3M-Diffusion first encodes
molecular graphs into a graph latent space aligned with text descriptions. It
then reconstructs the molecular structure and atomic attributes based on the
given text descriptions using the molecule decoder. It then learns a
probabilistic mapping from the text space to the latent molecular graph space
using a diffusion model. The results of our extensive experiments on several
datasets demonstrate that 3M-Diffusion can generate high-quality, novel and
diverse molecular graphs that semantically match the textual description
provided.",0
Water Isotope Separation using Deep Learning and a Catalytically Active Ultrathin Membrane,2403.07163v1,http://arxiv.org/abs/2403.07163v1,2024-03-11 21:06:51+00:00,"Water isotope separation, specifically separating heavy from light water, is
a socially significant issue due to the usage of heavy water in applications
such as nuclear magnetic resonance, nuclear power, and spectroscopy. Separation
of heavy water from light water is difficult due to very similar physical and
chemical properties between the isotopes. We show that a catalytically active
ultrathin membrane (e.g., a nanopore in MoS2) can enable chemical exchange
processes and physicochemical mechanisms that lead to efficient separation of
deuterium from hydrogen, quantified as the D2O and deuterium separation ratio
of 4.5 and 1.73, respectively. The separation process is inherently multiscale
in nature with the shorter times representing chemical exchange processes and
the longer timescales representing the transport phenomena. To bridge the
timescales, we employ a deep learning methodology which uses short time scale
ab-initio molecular dynamics data for training and extends the timescales to
classical molecular dynamics regime to demonstrate isotope separation and
reveal the underlying complex physicochemical processes.",0
Don't Forget What I did?: Assessing Client Contributions in Federated Learning,2403.07151v1,http://arxiv.org/abs/2403.07151v1,2024-03-11 20:39:32+00:00,"Federated Learning (FL) is a collaborative machine learning (ML) approach,
where multiple clients participate in training an ML model without exposing the
private data. Fair and accurate assessment of client contributions is an
important problem in FL to facilitate incentive allocation and encouraging
diverse clients to participate in a unified model training. Existing methods
for assessing client contribution adopts co-operative game-theoretic concepts,
such as Shapley values, but under simplified assumptions. In this paper, we
propose a history-aware game-theoretic framework, called FLContrib, to assess
client contributions when a subset of (potentially non-i.i.d.) clients
participate in each epoch of FL training. By exploiting the FL training process
and linearity of Shapley value, we develop FLContrib that yields a historical
timeline of client contributions as FL training progresses over epochs.
Additionally, to assess client contribution under limited computational budget,
we propose a scheduling procedure that considers a two-sided fairness criteria
to perform expensive Shapley value computation only in a subset of training
epochs. In experiments, we demonstrate a controlled trade-off between the
correctness and efficiency of client contributions assessed via FLContrib. To
demonstrate the benefits of history-aware client contributions, we apply
FLContrib to detect dishonest clients conducting data poisoning in FL training.",0
GuideGen: A Text-guided Framework for Joint CT Volume and Anatomical structure Generation,2403.07247v1,http://arxiv.org/abs/2403.07247v1,2024-03-12 02:09:39+00:00,"The annotation burden and extensive labor for gathering a large medical
dataset with images and corresponding labels are rarely cost-effective and
highly intimidating. This results in a lack of abundant training data that
undermines downstream tasks and partially contributes to the challenge image
analysis faces in the medical field. As a workaround, given the recent success
of generative neural models, it is now possible to synthesize image datasets at
a high fidelity guided by external constraints. This paper explores this
possibility and presents \textbf{GuideGen}: a pipeline that jointly generates
CT images and tissue masks for abdominal organs and colorectal cancer
conditioned on a text prompt. Firstly, we introduce Volumetric Mask Sampler to
fit the discrete distribution of mask labels and generate low-resolution 3D
tissue masks. Secondly, our Conditional Image Generator autoregressively
generates CT slices conditioned on a corresponding mask slice to incorporate
both style information and anatomical guidance. This pipeline guarantees high
fidelity and variability as well as exact alignment between generated CT
volumes and tissue masks. Both qualitative and quantitative experiments on 3D
abdominal CTs demonstrate a high performance of our proposed pipeline, thereby
proving our method can serve as a dataset generator and provide potential
benefits to downstream tasks. It is hoped that our work will offer a promising
solution on the multimodality generation of CT and its anatomical mask. Our
source code is publicly available at
https://github.com/OvO1111/JointImageGeneration.",0
Deep Learning-Assisted Parallel Interference Cancellation for Grant-Free NOMA in Machine-Type Communication,2403.07255v1,http://arxiv.org/abs/2403.07255v1,2024-03-12 02:24:37+00:00,"In this paper, we present a novel approach for joint activity detection (AD),
channel estimation (CE), and data detection (DD) in uplink grant-free
non-orthogonal multiple access (NOMA) systems. Our approach employs an
iterative and parallel interference removal strategy inspired by parallel
interference cancellation (PIC), enhanced with deep learning to jointly tackle
the AD, CE, and DD problems. Based on this approach, we develop three PIC
frameworks, each of which is designed for either coherent or non-coherence
schemes. The first framework performs joint AD and CE using received pilot
signals in the coherent scheme. Building upon this framework, the second
framework utilizes both the received pilot and data signals for CE, further
enhancing the performances of AD, CE, and DD in the coherent scheme. The third
framework is designed to accommodate the non-coherent scheme involving a small
number of data bits, which simultaneously performs AD and DD. Through joint
loss functions and interference cancellation modules, our approach supports
end-to-end training, contributing to enhanced performances of AD, CE, and DD
for both coherent and non-coherent schemes. Simulation results demonstrate the
superiority of our approach over traditional techniques, exhibiting enhanced
performances of AD, CE, and DD while maintaining lower computational
complexity.",0
The Dawn of AI-Native EDA: Promises and Challenges of Large Circuit Models,2403.07257v1,http://arxiv.org/abs/2403.07257v1,2024-03-12 02:26:30+00:00,"Within the Electronic Design Automation (EDA) domain, AI-driven solutions
have emerged as formidable tools, yet they typically augment rather than
redefine existing methodologies. These solutions often repurpose deep learning
models from other domains, such as vision, text, and graph analytics, applying
them to circuit design without tailoring to the unique complexities of
electronic circuits. Such an AI4EDA approach falls short of achieving a
holistic design synthesis and understanding, overlooking the intricate
interplay of electrical, logical, and physical facets of circuit data. This
perspective paper argues for a paradigm shift from AI4EDA towards AI-native
EDA, integrating AI at the core of the design process. Pivotal to this vision
is the development of a multimodal circuit representation learning technique,
poised to provide a comprehensive understanding by harmonizing and extracting
insights from varied data sources, such as functional specifications, RTL
designs, circuit netlists, and physical layouts.
  We champion the creation of large circuit models (LCMs) that are inherently
multimodal, crafted to decode and express the rich semantics and structures of
circuit data, thus fostering more resilient, efficient, and inventive design
methodologies. Embracing this AI-native philosophy, we foresee a trajectory
that transcends the current innovation plateau in EDA, igniting a profound
shift-left in electronic design methodology. The envisioned advancements herald
not just an evolution of existing EDA tools but a revolution, giving rise to
novel instruments of design-tools that promise to radically enhance design
productivity and inaugurate a new epoch where the optimization of circuit
performance, power, and area (PPA) is achieved not incrementally, but through
leaps that redefine the benchmarks of electronic systems' capabilities.",0
SAMDA: Leveraging SAM on Few-Shot Domain Adaptation for Electronic Microscopy Segmentation,2403.07951v1,http://arxiv.org/abs/2403.07951v1,2024-03-12 02:28:29+00:00,"It has been shown that traditional deep learning methods for electronic
microscopy segmentation usually suffer from low transferability when samples
and annotations are limited, while large-scale vision foundation models are
more robust when transferring between different domains but facing sub-optimal
improvement under fine-tuning. In this work, we present a new few-shot domain
adaptation framework SAMDA, which combines the Segment Anything Model(SAM) with
nnUNet in the embedding space to achieve high transferability and accuracy.
Specifically, we choose the Unet-based network as the ""expert"" component to
learn segmentation features efficiently and design a SAM-based adaptation
module as the ""generic"" component for domain transfer. By amalgamating the
""generic"" and ""expert"" components, we mitigate the modality imbalance in the
complex pre-training knowledge inherent to large-scale Vision Foundation models
and the challenge of transferability inherent to traditional neural networks.
The effectiveness of our model is evaluated on two electron microscopic image
datasets with different modalities for mitochondria segmentation, which
improves the dice coefficient on the target domain by 6.7%. Also, the SAM-based
adaptor performs significantly better with only a single annotated image than
the 10-shot domain adaptation on nnUNet. We further verify our model on four
MRI datasets from different sources to prove its generalization ability.",0
LIST: Learning to Index Spatio-Textual Data for Embedding based Spatial Keyword Queries,2403.07331v1,http://arxiv.org/abs/2403.07331v1,2024-03-12 05:32:33+00:00,"With the proliferation of spatio-textual data, Top-k KNN spatial keyword
queries (TkQs), which return a list of objects based on a ranking function that
evaluates both spatial and textual relevance, have found many real-life
applications. Existing geo-textual indexes for TkQs use traditional retrieval
models like BM25 to compute text relevance and usually exploit a simple linear
function to compute spatial relevance, but its effectiveness is limited. To
improve effectiveness, several deep learning models have recently been
proposed, but they suffer severe efficiency issues. To the best of our
knowledge, there are no efficient indexes specifically designed to accelerate
the top-k search process for these deep learning models.
  To tackle these issues, we propose a novel technique, which Learns to Index
the Spatio-Textual data for answering embedding based spatial keyword queries
(called LIST). LIST is featured with two novel components. Firstly, we propose
a lightweight and effective relevance model that is capable of learning both
textual and spatial relevance. Secondly, we introduce a novel machine learning
based Approximate Nearest Neighbor Search (ANNS) index, which utilizes a new
learning-to-cluster technique to group relevant queries and objects together
while separating irrelevant queries and objects. Two key challenges in building
an effective and efficient index are the absence of high-quality labels and
unbalanced clustering results. We develop a novel pseudo-label generation
technique to address the two challenges. Experimental results show that LIST
significantly outperforms state-of-the-art methods on effectiveness, with
improvements up to 19.21% and 12.79% in terms of NDCG@1 and Recall@10, and is
three orders of magnitude faster than the most effective baseline.",0
Unknown Domain Inconsistency Minimization for Domain Generalization,2403.07329v1,http://arxiv.org/abs/2403.07329v1,2024-03-12 05:29:48+00:00,"The objective of domain generalization (DG) is to enhance the transferability
of the model learned from a source domain to unobserved domains. To prevent
overfitting to a specific domain, Sharpness-Aware Minimization (SAM) reduces
source domain's loss sharpness. Although SAM variants have delivered
significant improvements in DG, we highlight that there's still potential for
improvement in generalizing to unknown domains through the exploration on data
space. This paper introduces an objective rooted in both parameter and data
perturbed regions for domain generalization, coined Unknown Domain
Inconsistency Minimization (UDIM). UDIM reduces the loss landscape
inconsistency between source domain and unknown domains. As unknown domains are
inaccessible, these domains are empirically crafted by perturbing instances
from the source domain dataset. In particular, by aligning the loss landscape
acquired in the source domain to the loss landscape of perturbed domains, we
expect to achieve generalization grounded on these flat minima for the unknown
domains. Theoretically, we validate that merging SAM optimization with the UDIM
objective establishes an upper bound for the true objective of the DG task. In
an empirical aspect, UDIM consistently outperforms SAM variants across multiple
DG benchmark datasets. Notably, UDIM shows statistically significant
improvements in scenarios with more restrictive domain information,
underscoring UDIM's generalization capability in unseen domains. Our code is
available at \url{https://github.com/SJShin-AI/UDIM}.",0
A Question-centric Multi-experts Contrastive Learning Framework for Improving the Accuracy and Interpretability of Deep Sequential Knowledge Tracing Models,2403.07322v1,http://arxiv.org/abs/2403.07322v1,2024-03-12 05:15:42+00:00,"Knowledge tracing (KT) plays a crucial role in predicting students' future
performance by analyzing their historical learning processes. Deep neural
networks (DNNs) have shown great potential in solving the KT problem. However,
there still exist some important challenges when applying deep learning
techniques to model the KT process. The first challenge lies in taking the
individual information of the question into modeling. This is crucial because,
despite questions sharing the same knowledge component (KC), students'
knowledge acquisition on homogeneous questions can vary significantly. The
second challenge lies in interpreting the prediction results from existing deep
learning-based KT models. In real-world applications, while it may not be
necessary to have complete transparency and interpretability of the model
parameters, it is crucial to present the model's prediction results in a manner
that teachers find interpretable. This makes teachers accept the rationale
behind the prediction results and utilize them to design teaching activities
and tailored learning strategies for students. However, the inherent black-box
nature of deep learning techniques often poses a hurdle for teachers to fully
embrace the model's prediction results. To address these challenges, we propose
a Question-centric Multi-experts Contrastive Learning framework for KT called
Q-MCKT.",0
Approaching Rate-Distortion Limits in Neural Compression with Lattice Transform Coding,2403.07320v1,http://arxiv.org/abs/2403.07320v1,2024-03-12 05:09:25+00:00,"Neural compression has brought tremendous progress in designing lossy
compressors with good rate-distortion (RD) performance at low complexity. Thus
far, neural compression design involves transforming the source to a latent
vector, which is then rounded to integers and entropy coded. While this
approach has been shown to be optimal in a one-shot sense on certain sources,
we show that it is highly sub-optimal on i.i.d. sequences, and in fact always
recovers scalar quantization of the original source sequence. We demonstrate
that the sub-optimality is due to the choice of quantization scheme in the
latent space, and not the transform design. By employing lattice quantization
instead of scalar quantization in the latent space, we demonstrate that Lattice
Transform Coding (LTC) is able to recover optimal vector quantization at
various dimensions and approach the asymptotically-achievable rate-distortion
function at reasonable complexity. On general vector sources, LTC improves upon
standard neural compressors in one-shot coding performance. LTC also enables
neural compressors that perform block coding on i.i.d. vector sources, which
yields coding gain over optimal one-shot coding.",0
Customizable Avatars with Dynamic Facial Action Coded Expressions (CADyFACE) for Improved User Engagement,2403.07314v1,http://arxiv.org/abs/2403.07314v1,2024-03-12 05:00:38+00:00,"Customizable 3D avatar-based facial expression stimuli may improve user
engagement in behavioral biomarker discovery and therapeutic intervention for
autism, Alzheimer's disease, facial palsy, and more. However, there is a lack
of customizable avatar-based stimuli with Facial Action Coding System (FACS)
action unit (AU) labels. Therefore, this study focuses on (1) FACS-labeled,
customizable avatar-based expression stimuli for maintaining subjects'
engagement, (2) learning-based measurements that quantify subjects' facial
responses to such stimuli, and (3) validation of constructs represented by
stimulus-measurement pairs. We propose Customizable Avatars with Dynamic Facial
Action Coded Expressions (CADyFACE) labeled with AUs by a certified FACS
expert. To measure subjects' AUs in response to CADyFACE, we propose a novel
Beta-guided Correlation and Multi-task Expression learning neural network
(BeCoME-Net) for multi-label AU detection. The beta-guided correlation loss
encourages feature correlation with AUs while discouraging correlation with
subject identities for improved generalization. We train BeCoME-Net for
unilateral and bilateral AU detection and compare with state-of-the-art
approaches. To assess construct validity of CADyFACE and BeCoME-Net, twenty
healthy adult volunteers complete expression recognition and mimicry tasks in
an online feasibility study while webcam-based eye-tracking and video are
collected. We test validity of multiple constructs, including face preference
during recognition and AUs during mimicry.",0
Knowledge Graph Large Language Model (KG-LLM) for Link Prediction,2403.07311v2,http://arxiv.org/abs/2403.07311v2,2024-03-12 04:47:29+00:00,"The task of predicting multiple links within knowledge graphs (KGs) stands as
a challenge in the field of knowledge graph analysis, a challenge increasingly
resolvable due to advancements in natural language processing (NLP) and KG
embedding techniques. This paper introduces a novel methodology, the Knowledge
Graph Large Language Model Framework (KG-LLM), which leverages pivotal NLP
paradigms, including chain-of-thought (CoT) prompting and in-context learning
(ICL), to enhance multi-hop link prediction in KGs. By converting the KG to a
CoT prompt, our framework is designed to discern and learn the latent
representations of entities and their interrelations. To show the efficacy of
the KG-LLM Framework, we fine-tune three leading Large Language Models (LLMs)
within this framework, employing both non-ICL and ICL tasks for a comprehensive
evaluation. Further, we explore the framework's potential to provide LLMs with
zero-shot capabilities for handling previously unseen prompts. Our experimental
findings discover that integrating ICL and CoT not only augments the
performance of our approach but also significantly boosts the models'
generalization capacity, thereby ensuring more precise predictions in
unfamiliar scenarios.",0
How does promoting the minority fraction affect generalization? A theoretical study of the one-hidden-layer neural network on group imbalance,2403.07310v1,http://arxiv.org/abs/2403.07310v1,2024-03-12 04:38:05+00:00,"Group imbalance has been a known problem in empirical risk minimization
(ERM), where the achieved high average accuracy is accompanied by low accuracy
in a minority group. Despite algorithmic efforts to improve the minority group
accuracy, a theoretical generalization analysis of ERM on individual groups
remains elusive. By formulating the group imbalance problem with the Gaussian
Mixture Model, this paper quantifies the impact of individual groups on the
sample complexity, the convergence rate, and the average and group-level
testing performance. Although our theoretical framework is centered on binary
classification using a one-hidden-layer neural network, to the best of our
knowledge, we provide the first theoretical analysis of the group-level
generalization of ERM in addition to the commonly studied average
generalization performance. Sample insights of our theoretical results include
that when all group-level co-variance is in the medium regime and all mean are
close to zero, the learning performance is most desirable in the sense of a
small sample complexity, a fast training rate, and a high average and
group-level testing accuracy. Moreover, we show that increasing the fraction of
the minority group in the training data does not necessarily improve the
generalization performance of the minority group. Our theoretical results are
validated on both synthetic and empirical datasets, such as CelebA and CIFAR-10
in image classification.",0
Reinforced Sequential Decision-Making for Sepsis Treatment: The POSNEGDM Framework with Mortality Classifier and Transformer,2403.07309v1,http://arxiv.org/abs/2403.07309v1,2024-03-12 04:36:41+00:00,"Sepsis, a life-threatening condition triggered by the body's exaggerated
response to infection, demands urgent intervention to prevent severe
complications. Existing machine learning methods for managing sepsis struggle
in offline scenarios, exhibiting suboptimal performance with survival rates
below 50%. This paper introduces the POSNEGDM -- ``Reinforcement Learning with
Positive and Negative Demonstrations for Sequential Decision-Making"" framework
utilizing an innovative transformer-based model and a feedback reinforcer to
replicate expert actions while considering individual patient characteristics.
A mortality classifier with 96.7\% accuracy guides treatment decisions towards
positive outcomes. The POSNEGDM framework significantly improves patient
survival, saving 97.39% of patients, outperforming established machine learning
algorithms (Decision Transformer and Behavioral Cloning) with survival rates of
33.4% and 43.5%, respectively. Additionally, ablation studies underscore the
critical role of the transformer-based decision maker and the integration of a
mortality classifier in enhancing overall survival rates. In summary, our
proposed approach presents a promising avenue for enhancing sepsis treatment
outcomes, contributing to improved patient care and reduced healthcare costs.",0
Verification-Aided Learning of Neural Network Barrier Functions with Termination Guarantees,2403.07308v1,http://arxiv.org/abs/2403.07308v1,2024-03-12 04:29:43+00:00,"Barrier functions are a general framework for establishing a safety guarantee
for a system. However, there is no general method for finding these functions.
To address this shortcoming, recent approaches use self-supervised learning
techniques to learn these functions using training data that are periodically
generated by a verification procedure, leading to a verification-aided learning
framework. Despite its immense potential in automating barrier function
synthesis, the verification-aided learning framework does not have termination
guarantees and may suffer from a low success rate of finding a valid barrier
function in practice. In this paper, we propose a holistic approach to address
these drawbacks. With a convex formulation of the barrier function synthesis,
we propose to first learn an empirically well-behaved NN basis function and
then apply a fine-tuning algorithm that exploits the convexity and
counterexamples from the verification failure to find a valid barrier function
with finite-step termination guarantees: if there exist valid barrier
functions, the fine-tuning algorithm is guaranteed to find one in a finite
number of iterations. We demonstrate that our fine-tuning method can
significantly boost the performance of the verification-aided learning
framework on examples of different scales and using various neural network
verifiers.",0
Lumen: Unleashing Versatile Vision-Centric Capabilities of Large Multimodal Models,2403.07304v1,http://arxiv.org/abs/2403.07304v1,2024-03-12 04:13:45+00:00,"Large Multimodal Model (LMM) is a hot research topic in the computer vision
area and has also demonstrated remarkable potential across multiple
disciplinary fields. A recent trend is to further extend and enhance the
perception capabilities of LMMs. The current methods follow the paradigm of
adapting the visual task outputs to the format of the language model, which is
the main component of a LMM. This adaptation leads to convenient development of
such LMMs with minimal modifications, however, it overlooks the intrinsic
characteristics of diverse visual tasks and hinders the learning of perception
capabilities. To address this issue, we propose a novel LMM architecture named
Lumen, a Large multimodal model with versatile vision-centric capability
enhancement. We decouple the LMM's learning of perception capabilities into
task-agnostic and task-specific stages. Lumen first promotes fine-grained
vision-language concept alignment, which is the fundamental capability for
various visual tasks. Thus the output of the task-agnostic stage is a shared
representation for all the tasks we address in this paper. Then the
task-specific decoding is carried out by flexibly routing the shared
representation to lightweight task decoders with negligible training efforts.
Benefiting from such a decoupled design, our Lumen surpasses existing LMM-based
approaches on the COCO detection benchmark with a clear margin and exhibits
seamless scalability to additional visual tasks. Furthermore, we also conduct
comprehensive ablation studies and generalization evaluations for deeper
insights. The code will be released at https://github.com/SxJyJay/Lumen.",0
Taming Pre-trained LLMs for Generalised Time Series Forecasting via Cross-modal Knowledge Distillation,2403.07300v1,http://arxiv.org/abs/2403.07300v1,2024-03-12 04:04:38+00:00,"Multivariate time series forecasting has recently gained great success with
the rapid growth of deep learning models. However, existing approaches usually
train models from scratch using limited temporal data, preventing their
generalization. Recently, with the surge of the Large Language Models (LLMs),
several works have attempted to introduce LLMs into time series forecasting.
Despite promising results, these methods directly take time series as the input
to LLMs, ignoring the inherent modality gap between temporal and text data. In
this work, we propose a novel Large Language Models and time series alignment
framework, dubbed LLaTA, to fully unleash the potentials of LLMs in the time
series forecasting challenge. Based on cross-modal knowledge distillation, the
proposed method exploits both input-agnostic static knowledge and
input-dependent dynamic knowledge in pre-trained LLMs. In this way, it empowers
the forecasting model with favorable performance as well as strong
generalization abilities. Extensive experiments demonstrate the proposed method
establishes a new state of the art for both long- and short-term forecasting.
Code is available at \url{https://github.com/Hank0626/LLaTA}.",0
Advancements in Continuous Glucose Monitoring: Integrating Deep Learning and ECG Signal,2403.07296v1,http://arxiv.org/abs/2403.07296v1,2024-03-12 03:57:25+00:00,"This paper presents a novel approach to noninvasive hyperglycemia monitoring
utilizing electrocardiograms (ECG) from an extensive database comprising 1119
subjects. Previous research on hyperglycemia or glucose detection using ECG has
been constrained by challenges related to generalization and scalability,
primarily due to using all subjects' ECG in training without considering unseen
subjects as a critical factor for developing methods with effective
generalization. We designed a deep neural network model capable of identifying
significant features across various spatial locations and examining the
interdependencies among different features within each convolutional layer. To
expedite processing speed, we segment the ECG of each user to isolate one
heartbeat or one cycle of the ECG. Our model was trained using data from 727
subjects, while 168 were used for validation. The testing phase involved 224
unseen subjects, with a dataset consisting of 9,000 segments. The result
indicates that the proposed algorithm effectively detects hyperglycemia with a
91.60% area under the curve (AUC), 81.05% sensitivity, and 85.54% specificity.",0
Graph Data Condensation via Self-expressive Graph Structure Reconstruction,2403.07294v1,http://arxiv.org/abs/2403.07294v1,2024-03-12 03:54:25+00:00,"With the increasing demands of training graph neural networks (GNNs) on
large-scale graphs, graph data condensation has emerged as a critical technique
to relieve the storage and time costs during the training phase. It aims to
condense the original large-scale graph to a much smaller synthetic graph while
preserving the essential information necessary for efficiently training a
downstream GNN. However, existing methods concentrate either on optimizing node
features exclusively or endeavor to independently learn node features and the
graph structure generator. They could not explicitly leverage the information
of the original graph structure and failed to construct an interpretable graph
structure for the synthetic dataset. To address these issues, we introduce a
novel framework named \textbf{G}raph Data \textbf{C}ondensation via
\textbf{S}elf-expressive Graph Structure \textbf{R}econstruction
(\textbf{GCSR}). Our method stands out by (1) explicitly incorporating the
original graph structure into the condensing process and (2) capturing the
nuanced interdependencies between the condensed nodes by reconstructing an
interpretable self-expressive graph structure. Extensive experiments and
comprehensive analysis validate the efficacy of the proposed method across
diverse GNN models and datasets. Our code is available at
https://www.dropbox.com/scl/fi/2aonyp5ln5gisdqtjimu8/GCSR.zip?rlkey=11cuwfpsf54wxiiktu0klud0x&dl=0",0
Continual All-in-One Adverse Weather Removal with Knowledge Replay on a Unified Network Structure,2403.07292v1,http://arxiv.org/abs/2403.07292v1,2024-03-12 03:50:57+00:00,"In real-world applications, image degeneration caused by adverse weather is
always complex and changes with different weather conditions from days and
seasons. Systems in real-world environments constantly encounter adverse
weather conditions that are not previously observed. Therefore, it practically
requires adverse weather removal models to continually learn from incrementally
collected data reflecting various degeneration types. Existing adverse weather
removal approaches, for either single or multiple adverse weathers, are mainly
designed for a static learning paradigm, which assumes that the data of all
types of degenerations to handle can be finely collected at one time before a
single-phase learning process. They thus cannot directly handle the incremental
learning requirements. To address this issue, we made the earliest effort to
investigate the continual all-in-one adverse weather removal task, in a setting
closer to real-world applications. Specifically, we develop a novel continual
learning framework with effective knowledge replay (KR) on a unified network
structure. Equipped with a principal component projection and an effective
knowledge distillation mechanism, the proposed KR techniques are tailored for
the all-in-one weather removal task. It considers the characteristics of the
image restoration task with multiple degenerations in continual learning, and
the knowledge for different degenerations can be shared and accumulated in the
unified network structure. Extensive experimental results demonstrate the
effectiveness of the proposed method to deal with this challenging task, which
performs competitively to existing dedicated or joint training image
restoration methods. Our code is available at
https://github.com/xiaojihh/CL_all-in-one.",0
Learning Hierarchical Color Guidance for Depth Map Super-Resolution,2403.07290v1,http://arxiv.org/abs/2403.07290v1,2024-03-12 03:44:46+00:00,"Color information is the most commonly used prior knowledge for depth map
super-resolution (DSR), which can provide high-frequency boundary guidance for
detail restoration. However, its role and functionality in DSR have not been
fully developed. In this paper, we rethink the utilization of color information
and propose a hierarchical color guidance network to achieve DSR. On the one
hand, the low-level detail embedding module is designed to supplement
high-frequency color information of depth features in a residual mask manner at
the low-level stages. On the other hand, the high-level abstract guidance
module is proposed to maintain semantic consistency in the reconstruction
process by using a semantic mask that encodes the global guidance information.
The color information of these two dimensions plays a role in the front and
back ends of the attention-based feature projection (AFP) module in a more
comprehensive form. Simultaneously, the AFP module integrates the multi-scale
content enhancement block and adaptive attention projection block to make full
use of multi-scale information and adaptively project critical restoration
information in an attention manner for DSR. Compared with the state-of-the-art
methods on four benchmark datasets, our method achieves more competitive
performance both qualitatively and quantitatively.",0
Rediscovering BCE Loss for Uniform Classification,2403.07289v1,http://arxiv.org/abs/2403.07289v1,2024-03-12 03:44:40+00:00,"This paper introduces the concept of uniform classification, which employs a
unified threshold to classify all samples rather than adaptive threshold
classifying each individual sample. We also propose the uniform classification
accuracy as a metric to measure the model's performance in uniform
classification. Furthermore, begin with a naive loss, we mathematically derive
a loss function suitable for the uniform classification, which is the BCE
function integrated with a unified bias. We demonstrate the unified threshold
could be learned via the bias. The extensive experiments on six classification
datasets and three feature extraction models show that, compared to the SoftMax
loss, the models trained with the BCE loss not only exhibit higher uniform
classification accuracy but also higher sample-wise classification accuracy. In
addition, the learned bias from BCE loss is very close to the unified threshold
used in the uniform classification. The features extracted by the models
trained with BCE loss not only possess uniformity but also demonstrate better
intra-class compactness and inter-class distinctiveness, yielding superior
performance on open-set tasks such as face recognition.",0
MENTOR: Multilingual tExt detectioN TOward leaRning by analogy,2403.07286v1,http://arxiv.org/abs/2403.07286v1,2024-03-12 03:35:17+00:00,"Text detection is frequently used in vision-based mobile robots when they
need to interpret texts in their surroundings to perform a given task. For
instance, delivery robots in multilingual cities need to be capable of doing
multilingual text detection so that the robots can read traffic signs and road
markings. Moreover, the target languages change from region to region, implying
the need of efficiently re-training the models to recognize the novel/new
languages. However, collecting and labeling training data for novel languages
are cumbersome, and the efforts to re-train an existing/trained text detector
are considerable. Even worse, such a routine would repeat whenever a novel
language appears. This motivates us to propose a new problem setting for
tackling the aforementioned challenges in a more efficient way: ""We ask for a
generalizable multilingual text detection framework to detect and identify both
seen and unseen language regions inside scene images without the requirement of
collecting supervised training data for unseen languages as well as model
re-training"". To this end, we propose ""MENTOR"", the first work to realize a
learning strategy between zero-shot learning and few-shot learning for
multilingual scene text detection.",0
A Framework for Cost-Effective and Self-Adaptive LLM Shaking and Recovery Mechanism,2403.07283v1,http://arxiv.org/abs/2403.07283v1,2024-03-12 03:30:04+00:00,"As Large Language Models (LLMs) gain great success in real-world
applications, an increasing number of users are seeking to develop and deploy
their customized LLMs through cloud services. Nonetheless, in some specific
domains, there are still concerns regarding cost and trade-offs between privacy
issues and accuracy. In this study, we introduce a cost-effective and
self-adaptive LLM shaking tuning and recovery mechanism, named CypherTalk. With
carefully designed horizontal and vertical shaking operators, we can achieve
comparable accuracy results with SOTA privacy-preserving LLM schemes using
Cryptography-based or Differential Privacy-based methods. Experiments also show
that with the CypherTalk framework, users can achieve reliable accuracy when
using optimized shaking operator settings. To our best knowledge, this is the
first work that considers cost, and trade-off between model utility and privacy
in LLM scenarios.",0
Enhancing Transfer Learning with Flexible Nonparametric Posterior Sampling,2403.07282v1,http://arxiv.org/abs/2403.07282v1,2024-03-12 03:26:58+00:00,"Transfer learning has recently shown significant performance across various
tasks involving deep neural networks. In these transfer learning scenarios, the
prior distribution for downstream data becomes crucial in Bayesian model
averaging (BMA). While previous works proposed the prior over the neural
network parameters centered around the pre-trained solution, such strategies
have limitations when dealing with distribution shifts between upstream and
downstream data. This paper introduces nonparametric transfer learning (NPTL),
a flexible posterior sampling method to address the distribution shift issue
within the context of nonparametric learning. The nonparametric learning (NPL)
method is a recent approach that employs a nonparametric prior for posterior
sampling, efficiently accounting for model misspecification scenarios, which is
suitable for transfer learning scenarios that may involve the distribution
shift between upstream and downstream tasks. Through extensive empirical
validations, we demonstrate that our approach surpasses other baselines in BMA
performance.",0
A Survey of Explainable Knowledge Tracing,2403.07279v1,http://arxiv.org/abs/2403.07279v1,2024-03-12 03:17:59+00:00,"With the long term accumulation of high quality educational data, artificial
intelligence has shown excellent performance in knowledge tracing. However, due
to the lack of interpretability and transparency of some algorithms, this
approach will result in reduced stakeholder trust and a decreased acceptance of
intelligent decisions. Therefore, algorithms need to achieve high accuracy, and
users need to understand the internal operating mechanism and provide reliable
explanations for decisions. This paper thoroughly analyzes the interpretability
of KT algorithms. First, the concepts and common methods of explainable
artificial intelligence and knowledge tracing are introduced. Next, explainable
knowledge tracing models are classified into two categories: transparent models
and black box models. Then, the interpretable methods used are reviewed from
three stages: ante hoc interpretable methods, post hoc interpretable methods,
and other dimensions. It is worth noting that current evaluation methods for
explainable knowledge tracing are lacking. Hence, contrast and deletion
experiments are conducted to explain the prediction results of the deep
knowledge tracing model on the ASSISTment2009 by using three XAI methods.
Moreover, this paper offers some insights into evaluation methods from the
perspective of educational stakeholders. This paper provides a detailed and
comprehensive review of the research on explainable knowledge tracing, aiming
to offer some basis and inspiration for researchers interested in the
interpretability of knowledge tracing.",0
A Bayesian Approach to OOD Robustness in Image Classification,2403.07277v1,http://arxiv.org/abs/2403.07277v1,2024-03-12 03:15:08+00:00,"An important and unsolved problem in computer vision is to ensure that the
algorithms are robust to changes in image domains. We address this problem in
the scenario where we have access to images from the target domains but no
annotations. Motivated by the challenges of the OOD-CV benchmark where we
encounter real world Out-of-Domain (OOD) nuisances and occlusion, we introduce
a novel Bayesian approach to OOD robustness for object classification. Our work
extends Compositional Neural Networks (CompNets), which have been shown to be
robust to occlusion but degrade badly when tested on OOD data. We exploit the
fact that CompNets contain a generative head defined over feature vectors
represented by von Mises-Fisher (vMF) kernels, which correspond roughly to
object parts, and can be learned without supervision. We obverse that some vMF
kernels are similar between different domains, while others are not. This
enables us to learn a transitional dictionary of vMF kernels that are
intermediate between the source and target domains and train the generative
model on this dictionary using the annotations on the source domain, followed
by iterative refinement. This approach, termed Unsupervised Generative
Transition (UGT), performs very well in OOD scenarios even when occlusion is
present. UGT is evaluated on different OOD benchmarks including the OOD-CV
dataset, several popular datasets (e.g., ImageNet-C [9]), artificial image
corruptions (including adding occluders), and synthetic-to-real domain
transfer, and does well in all scenarios outperforming SOTA alternatives (e.g.
up to 10% top-1 accuracy on Occluded OOD-CV dataset).",0
Anderson acceleration for iteratively reweighted $\ell_1$ algorithm,2403.07271v1,http://arxiv.org/abs/2403.07271v1,2024-03-12 03:00:15+00:00,"Iteratively reweighted L1 (IRL1) algorithm is a common algorithm for solving
sparse optimization problems with nonconvex and nonsmooth regularization. The
development of its acceleration algorithm, often employing Nesterov
acceleration, has sparked significant interest. Nevertheless, the convergence
and complexity analysis of these acceleration algorithms consistently poses
substantial challenges. Recently, Anderson acceleration has gained prominence
owing to its exceptional performance for speeding up fixed-point iteration,
with numerous recent studies applying it to gradient-based algorithms.
Motivated by the powerful impact of Anderson acceleration, we propose an
Anderson-accelerated IRL1 algorithm and establish its local linear convergence
rate. We extend this convergence result, typically observed in smooth settings,
to a nonsmooth scenario. Importantly, our theoretical results do not depend on
the Kurdyka-Lojasiewicz condition, a necessary condition in existing Nesterov
acceleration-based algorithms. Furthermore, to ensure global convergence, we
introduce a globally convergent Anderson accelerated IRL1 algorithm by
incorporating a classical nonmonotone line search condition. Experimental
results indicate that our algorithm outperforms existing Nesterov
acceleration-based algorithms.",0
Self-supervised Contrastive Learning for Implicit Collaborative Filtering,2403.07265v1,http://arxiv.org/abs/2403.07265v1,2024-03-12 02:52:17+00:00,"Contrastive learning-based recommendation algorithms have significantly
advanced the field of self-supervised recommendation, particularly with BPR as
a representative ranking prediction task that dominates implicit collaborative
filtering. However, the presence of false-positive and false-negative examples
in recommendation systems hampers accurate preference learning. In this study,
we propose a simple self-supervised contrastive learning framework that
leverages positive feature augmentation and negative label augmentation to
improve the self-supervisory signal. Theoretical analysis demonstrates that our
learning method is equivalent to maximizing the likelihood estimation with
latent variables representing user interest centers. Additionally, we establish
an efficient negative label augmentation technique that samples unlabeled
examples with a probability linearly dependent on their relative ranking
positions, enabling efficient augmentation in constant time complexity. Through
validation on multiple datasets, we illustrate the significant improvements our
method achieves over the widely used BPR optimization objective while
maintaining comparable runtime.",0
Near-Interpolators: Rapid Norm Growth and the Trade-Off between Interpolation and Generalization,2403.07264v1,http://arxiv.org/abs/2403.07264v1,2024-03-12 02:47:00+00:00,"We study the generalization capability of nearly-interpolating linear
regressors: $\boldsymbol{\beta}$'s whose training error $\tau$ is positive but
small, i.e., below the noise floor. Under a random matrix theoretic assumption
on the data distribution and an eigendecay assumption on the data covariance
matrix $\boldsymbol{\Sigma}$, we demonstrate that any near-interpolator
exhibits rapid norm growth: for $\tau$ fixed, $\boldsymbol{\beta}$ has squared
$\ell_2$-norm $\mathbb{E}[\|{\boldsymbol{\beta}}\|_{2}^{2}] =
\Omega(n^{\alpha})$ where $n$ is the number of samples and $\alpha >1$ is the
exponent of the eigendecay, i.e., $\lambda_i(\boldsymbol{\Sigma}) \sim
i^{-\alpha}$. This implies that existing data-independent norm-based bounds are
necessarily loose. On the other hand, in the same regime we precisely
characterize the asymptotic trade-off between interpolation and generalization.
Our characterization reveals that larger norm scaling exponents $\alpha$
correspond to worse trade-offs between interpolation and generalization. We
verify empirically that a similar phenomenon holds for nearly-interpolating
shallow neural networks.",0
Adaptive Bounding Box Uncertainties via Two-Step Conformal Prediction,2403.07263v1,http://arxiv.org/abs/2403.07263v1,2024-03-12 02:45:24+00:00,"Quantifying a model's predictive uncertainty is essential for safety-critical
applications such as autonomous driving. We consider quantifying such
uncertainty for multi-object detection. In particular, we leverage conformal
prediction to obtain uncertainty intervals with guaranteed coverage for object
bounding boxes. One challenge in doing so is that bounding box predictions are
conditioned on the object's class label. Thus, we develop a novel two-step
conformal approach that propagates uncertainty in predicted class labels into
the uncertainty intervals for the bounding boxes. This broadens the validity of
our conformal coverage guarantees to include incorrectly classified objects,
ensuring their usefulness when maximal safety assurances are required.
Moreover, we investigate novel ensemble and quantile regression formulations to
ensure the bounding box intervals are adaptive to object size, leading to a
more balanced coverage across sizes. Validating our two-step approach on
real-world datasets for 2D bounding box localization, we find that desired
coverage levels are satisfied with actionably tight predictive uncertainty
intervals.",0
Advantage-Aware Policy Optimization for Offline Reinforcement Learning,2403.07262v1,http://arxiv.org/abs/2403.07262v1,2024-03-12 02:43:41+00:00,"Offline Reinforcement Learning (RL) endeavors to leverage offline datasets to
craft effective agent policy without online interaction, which imposes proper
conservative constraints with the support of behavior policies to tackle the
Out-Of-Distribution (OOD) problem. However, existing works often suffer from
the constraint conflict issue when offline datasets are collected from multiple
behavior policies, i.e., different behavior policies may exhibit inconsistent
actions with distinct returns across the state space. To remedy this issue,
recent Advantage-Weighted (AW) methods prioritize samples with high advantage
values for agent training while inevitably leading to overfitting on these
samples. In this paper, we introduce a novel Advantage-Aware Policy
Optimization (A2PO) method to explicitly construct advantage-aware policy
constraints for offline learning under mixed-quality datasets. Specifically,
A2PO employs a Conditional Variational Auto-Encoder (CVAE) to disentangle the
action distributions of intertwined behavior policies by modeling the advantage
values of all training data as conditional variables. Then the agent can follow
such disentangled action distribution constraints to optimize the
advantage-aware policy towards high advantage values. Extensive experiments
conducted on both the single-quality and mixed-quality datasets of the D4RL
benchmark demonstrate that A2PO yields results superior to state-of-the-art
counterparts. Our code will be made publicly available.",0
Disentangling Policy from Offline Task Representation Learning via Adversarial Data Augmentation,2403.07261v1,http://arxiv.org/abs/2403.07261v1,2024-03-12 02:38:36+00:00,"Offline meta-reinforcement learning (OMRL) proficiently allows an agent to
tackle novel tasks while solely relying on a static dataset. For precise and
efficient task identification, existing OMRL research suggests learning
separate task representations that be incorporated with policy input, thus
forming a context-based meta-policy. A major approach to train task
representations is to adopt contrastive learning using multi-task offline data.
The dataset typically encompasses interactions from various policies (i.e., the
behavior policies), thus providing a plethora of contextual information
regarding different tasks. Nonetheless, amassing data from a substantial number
of policies is not only impractical but also often unattainable in realistic
settings. Instead, we resort to a more constrained yet practical scenario,
where multi-task data collection occurs with a limited number of policies. We
observed that learned task representations from previous OMRL methods tend to
correlate spuriously with the behavior policy instead of reflecting the
essential characteristics of the task, resulting in unfavorable
out-of-distribution generalization. To alleviate this issue, we introduce a
novel algorithm to disentangle the impact of behavior policy from task
representation learning through a process called adversarial data augmentation.
Specifically, the objective of adversarial data augmentation is not merely to
generate data analogous to offline data distribution; instead, it aims to
create adversarial examples designed to confound learned task representations
and lead to incorrect task identification. Our experiments show that learning
from such adversarial samples significantly enhances the robustness and
effectiveness of the task identification process and realizes satisfactory
out-of-distribution generalization.",0
Advancing Hyperspectral Targeted Alpha Therapy with Adversarial Machine Learning,2403.07149v1,http://arxiv.org/abs/2403.07149v1,2024-03-11 20:36:35+00:00,"Targeted Alpha Therapy (TAT) has emerged as a promising modality for the
treatment of various malignancies, leveraging the high linear energy transfer
(LET) and short range of alpha particles to selectively irradiate cancer cells
while sparing healthy tissue. Monitoring and optimizing TAT delivery is crucial
for its clinical success. Hyper-spectral Single Photon Imaging (HSPI) presents
a novel and versatile approach for the real-time assessment of TAT in vivo.
This study introduces a comprehensive framework for HSPI in TAT, encompassing
spectral unmixing, quantitative dosimetry, and spatiotemporal visualization. We
report the development of a dedicated HSPI system tailored to alpha-emitting
radionuclides, enabling the simultaneous acquisition of high-resolution
spectral data and single-photon localization. Utilizing advanced spectral
unmixing algorithms, we demonstrate the discrimination of alpha-induced
scintillation from background fluorescence, facilitating precise alpha particle
tracking with adversarial machine learning.",0
"New Perspectives in Online Contract Design: Heterogeneous, Homogeneous, Non-myopic Agents and Team Production",2403.07143v1,http://arxiv.org/abs/2403.07143v1,2024-03-11 20:28:23+00:00,"This work studies the repeated principal-agent problem from an online
learning perspective. The principal's goal is to learn the optimal contract
that maximizes her utility through repeated interactions, without prior
knowledge of the agent's type (i.e., the agent's cost and production
functions).
  I study three different settings when the principal contracts with a
$\textit{single}$ agent each round: 1. The agents are heterogeneous; 2. the
agents are homogenous; 3. the principal interacts with the same agent and the
agent is non-myopic. I present different approaches and techniques for
designing learning algorithms in each setting. For heterogeneous agent types, I
identify a condition that allows the problem to be reduced to Lipschitz bandits
directly. For identical agents, I give a polynomial sample complexity scheme to
learn the optimal contract based on inverse game theory. For strategic
non-myopic agents, I design a low strategic-regret mechanism. Also, I identify
a connection between linear contracts and posted-price auctions, showing the
two can be reduced to one another, and give a regret lower bound on learning
the optimal linear contract based on this observation.
  I also study a $\textit{team production}$ model. I identify a condition under
which the principal's learning problem can be reformulated as solving a family
of convex programs, thereby showing the optimal contract can be found
efficiently.",0
GaussianGrasper: 3D Language Gaussian Splatting for Open-vocabulary Robotic Grasping,2403.09637v1,http://arxiv.org/abs/2403.09637v1,2024-03-14 17:59:46+00:00,"Constructing a 3D scene capable of accommodating open-ended language queries,
is a pivotal pursuit, particularly within the domain of robotics. Such
technology facilitates robots in executing object manipulations based on human
language directives. To tackle this challenge, some research efforts have been
dedicated to the development of language-embedded implicit fields. However,
implicit fields (e.g. NeRF) encounter limitations due to the necessity of
processing a large number of input views for reconstruction, coupled with their
inherent inefficiencies in inference. Thus, we present the GaussianGrasper,
which utilizes 3D Gaussian Splatting to explicitly represent the scene as a
collection of Gaussian primitives. Our approach takes a limited set of RGB-D
views and employs a tile-based splatting technique to create a feature field.
In particular, we propose an Efficient Feature Distillation (EFD) module that
employs contrastive learning to efficiently and accurately distill language
embeddings derived from foundational models. With the reconstructed geometry of
the Gaussian field, our method enables the pre-trained grasping model to
generate collision-free grasp pose candidates. Furthermore, we propose a
normal-guided grasp module to select the best grasp pose. Through comprehensive
real-world experiments, we demonstrate that GaussianGrasper enables robots to
accurately query and grasp objects with language instructions, providing a new
solution for language-guided manipulation tasks. Data and codes can be
available at https://github.com/MrSecant/GaussianGrasper.",0
FreGS: 3D Gaussian Splatting with Progressive Frequency Regularization,2403.06908v1,http://arxiv.org/abs/2403.06908v1,2024-03-11 17:00:27+00:00,"3D Gaussian splatting has achieved very impressive performance in real-time
novel view synthesis. However, it often suffers from over-reconstruction during
Gaussian densification where high-variance image regions are covered by a few
large Gaussians only, leading to blur and artifacts in the rendered images. We
design a progressive frequency regularization (FreGS) technique to tackle the
over-reconstruction issue within the frequency space. Specifically, FreGS
performs coarse-to-fine Gaussian densification by exploiting low-to-high
frequency components that can be easily extracted with low-pass and high-pass
filters in the Fourier space. By minimizing the discrepancy between the
frequency spectrum of the rendered image and the corresponding ground truth, it
achieves high-quality Gaussian densification and alleviates the
over-reconstruction of Gaussian splatting effectively. Experiments over
multiple widely adopted benchmarks (e.g., Mip-NeRF360, Tanks-and-Temples and
Deep Blending) show that FreGS achieves superior novel view synthesis and
outperforms the state-of-the-art consistently.",0
Accurate Crystal Structure Prediction of New 2D Hybrid Organic Inorganic Perovskites,2403.06955v1,http://arxiv.org/abs/2403.06955v1,2024-03-11 17:39:08+00:00,"Low dimensional hybrid organic-inorganic perovskites (HOIPs) represent a
promising class of electronically active materials for both light absorption
and emission. The design space of HOIPs is extremely large, since a diverse
space of organic cations can be combined with different inorganic frameworks.
This immense design space allows for tunable electronic and mechanical
properties, but also necessitates the development of new tools for in silico
high throughput analysis of candidate structures. In this work, we present an
accurate, efficient, transferable and widely applicable machine learning
interatomic potential (MLIP) for predicting the structure of new 2D HOIPs.
Using the MACE architecture, an MLIP is trained on 86 diverse experimentally
reported HOIP structures. The model is tested on 73 unseen perovskite
compositions, and achieves chemical accuracy with respect to the reference
electronic structure method. Our model is then combined with a simple random
structure search algorithm to predict the structure of hypothetical HOIPs given
only the proposed composition. Success is demonstrated by correctly and
reliably recovering the crystal structure of a set of experimentally known 2D
perovskites. Such a random structure search is impossible with ab initio
methods due to the associated computational cost, but is relatively inexpensive
with the MACE potential. Finally, the procedure is used to predict the
structure formed by a new organic cation with no previously known corresponding
perovskite. Laboratory synthesis of the new hybrid perovskite confirms the
accuracy of our prediction. This capability, applied at scale, enables
efficient screening of thousands of combinations of organic cations and
inorganic layers.",0
Quadruped-Frog: Rapid Online Optimization of Continuous Quadruped Jumping,2403.06954v1,http://arxiv.org/abs/2403.06954v1,2024-03-11 17:36:44+00:00,"Legged robots are becoming increasingly agile in exhibiting dynamic behaviors
such as running and jumping. Usually, such behaviors are either optimized and
engineered offline (i.e. the behavior is designed for before it is needed),
either through model-based trajectory optimization, or through deep
learning-based methods involving millions of timesteps of simulation
interactions. Notably, such offline-designed locomotion controllers cannot
perfectly model the true dynamics of the system, such as the motor dynamics. In
contrast, in this paper, we consider a quadruped jumping task that we rapidly
optimize online. We design foot force profiles parameterized by only a few
parameters which we optimize for directly on hardware with Bayesian
Optimization. The force profiles are tracked at the joint level, and added to
Cartesian PD impedance control and Virtual Model Control to stabilize the
jumping motions. After optimization, which takes only a handful of jumps, we
show that this control architecture is capable of diverse and omnidirectional
jumps including forward, lateral, and twist (turning) jumps, even on uneven
terrain, enabling the Unitree Go1 quadruped to jump 0.5 m high, 0.5 m forward,
and jump-turn over 2 rad. Video results can be found at
https://youtu.be/SvfVNQ90k_w.",0
Optimizing Latent Graph Representations of Surgical Scenes for Zero-Shot Domain Transfer,2403.06953v1,http://arxiv.org/abs/2403.06953v1,2024-03-11 17:36:11+00:00,"Purpose: Advances in deep learning have resulted in effective models for
surgical video analysis; however, these models often fail to generalize across
medical centers due to domain shift caused by variations in surgical workflow,
camera setups, and patient demographics. Recently, object-centric learning has
emerged as a promising approach for improved surgical scene understanding,
capturing and disentangling visual and semantic properties of surgical tools
and anatomy to improve downstream task performance. In this work, we conduct a
multi-centric performance benchmark of object-centric approaches, focusing on
Critical View of Safety assessment in laparoscopic cholecystectomy, then
propose an improved approach for unseen domain generalization.
  Methods: We evaluate four object-centric approaches for domain
generalization, establishing baseline performance. Next, leveraging the
disentangled nature of object-centric representations, we dissect one of these
methods through a series of ablations (e.g. ignoring either visual or semantic
features for downstream classification). Finally, based on the results of these
ablations, we develop an optimized method specifically tailored for domain
generalization, LG-DG, that includes a novel disentanglement loss function.
  Results: Our optimized approach, LG-DG, achieves an improvement of 9.28% over
the best baseline approach. More broadly, we show that object-centric
approaches are highly effective for domain generalization thanks to their
modular approach to representation learning.
  Conclusion: We investigate the use of object-centric methods for unseen
domain generalization, identify method-agnostic factors critical for
performance, and present an optimized approach that substantially outperforms
existing methods.",0
SELMA: Learning and Merging Skill-Specific Text-to-Image Experts with Auto-Generated Data,2403.06952v1,http://arxiv.org/abs/2403.06952v1,2024-03-11 17:35:33+00:00,"Recent text-to-image (T2I) generation models have demonstrated impressive
capabilities in creating images from text descriptions. However, these T2I
generation models often fall short of generating images that precisely match
the details of the text inputs, such as incorrect spatial relationship or
missing objects. In this paper, we introduce SELMA: Skill-Specific Expert
Learning and Merging with Auto-Generated Data, a novel paradigm to improve the
faithfulness of T2I models by fine-tuning models on automatically generated,
multi-skill image-text datasets, with skill-specific expert learning and
merging. First, SELMA leverages an LLM's in-context learning capability to
generate multiple datasets of text prompts that can teach different skills, and
then generates the images with a T2I model based on the prompts. Next, SELMA
adapts the T2I model to the new skills by learning multiple single-skill LoRA
(low-rank adaptation) experts followed by expert merging. Our independent
expert fine-tuning specializes multiple models for different skills, and expert
merging helps build a joint multi-skill T2I model that can generate faithful
images given diverse text prompts, while mitigating the knowledge conflict from
different datasets. We empirically demonstrate that SELMA significantly
improves the semantic alignment and text faithfulness of state-of-the-art T2I
diffusion models on multiple benchmarks (+2.1% on TIFA and +6.9% on DSG), human
preference metrics (PickScore, ImageReward, and HPS), as well as human
evaluation. Moreover, fine-tuning with image-text pairs auto-collected via
SELMA shows comparable performance to fine-tuning with ground truth data.
Lastly, we show that fine-tuning with images from a weaker T2I model can help
improve the generation quality of a stronger T2I model, suggesting promising
weak-to-strong generalization in T2I models.",0
DEADiff: An Efficient Stylization Diffusion Model with Disentangled Representations,2403.06951v2,http://arxiv.org/abs/2403.06951v2,2024-03-11 17:35:23+00:00,"The diffusion-based text-to-image model harbors immense potential in
transferring reference style. However, current encoder-based approaches
significantly impair the text controllability of text-to-image models while
transferring styles. In this paper, we introduce DEADiff to address this issue
using the following two strategies: 1) a mechanism to decouple the style and
semantics of reference images. The decoupled feature representations are first
extracted by Q-Formers which are instructed by different text descriptions.
Then they are injected into mutually exclusive subsets of cross-attention
layers for better disentanglement. 2) A non-reconstructive learning method. The
Q-Formers are trained using paired images rather than the identical target, in
which the reference image and the ground-truth image are with the same style or
semantics. We show that DEADiff attains the best visual stylization results and
optimal balance between the text controllability inherent in the text-to-image
model and style similarity to the reference image, as demonstrated both
quantitatively and qualitatively. Our project page is
https://tianhao-qi.github.io/DEADiff/.",0
Split to Merge: Unifying Separated Modalities for Unsupervised Domain Adaptation,2403.06946v1,http://arxiv.org/abs/2403.06946v1,2024-03-11 17:33:12+00:00,"Large vision-language models (VLMs) like CLIP have demonstrated good
zero-shot learning performance in the unsupervised domain adaptation task. Yet,
most transfer approaches for VLMs focus on either the language or visual
branches, overlooking the nuanced interplay between both modalities. In this
work, we introduce a Unified Modality Separation (UniMoS) framework for
unsupervised domain adaptation. Leveraging insights from modality gap studies,
we craft a nimble modality separation network that distinctly disentangles
CLIP's features into language-associated and vision-associated components. Our
proposed Modality-Ensemble Training (MET) method fosters the exchange of
modality-agnostic information while maintaining modality-specific nuances. We
align features across domains using a modality discriminator. Comprehensive
evaluations on three benchmarks reveal our approach sets a new state-of-the-art
with minimal computational costs. Code: https://github.com/TL-UESTC/UniMoS",0
Grid Monitoring and Protection with Continuous Point-on-Wave Measurements and Generative AI,2403.06942v1,http://arxiv.org/abs/2403.06942v1,2024-03-11 17:28:46+00:00,"Purpose This article presents a case for a next-generation grid monitoring
and control system, leveraging recent advances in generative artificial
intelligence (AI), machine learning, and statistical inference. Advancing
beyond earlier generations of wide-area monitoring systems built upon
supervisory control and data acquisition (SCADA) and synchrophasor
technologies, we argue for a monitoring and control framework based on the
streaming of continuous point-on-wave (CPOW) measurements with AI-powered data
compression and fault detection.
  Methods and Results: The architecture of the proposed design originates from
the Wiener-Kallianpur innovation representation of a random process that
transforms causally a stationary random process into an innovation sequence
with independent and identically distributed random variables. This work
presents a generative AI approach that (i) learns an innovation autoencoder
that extracts innovation sequence from CPOW time series, (ii) compresses the
CPOW streaming data with innovation autoencoder and subband coding, and (iii)
detects unknown faults and novel trends via nonparametric sequential hypothesis
testing.
  Conclusion: This work argues that conventional monitoring using SCADA and
phasor measurement unit (PMU) technologies is ill-suited for a future grid with
deep penetration of inverter-based renewable generations and distributed energy
resources. A monitoring system based on CPOW data streaming and AI data
analytics should be the basic building blocks for situational awareness of a
highly dynamic future grid.",0
Conditional Score-Based Diffusion Model for Cortical Thickness Trajectory Prediction,2403.06940v1,http://arxiv.org/abs/2403.06940v1,2024-03-11 17:26:18+00:00,"Alzheimer's Disease (AD) is a neurodegenerative condition characterized by
diverse progression rates among individuals, with changes in cortical thickness
(CTh) closely linked to its progression. Accurately forecasting CTh
trajectories can significantly enhance early diagnosis and intervention
strategies, providing timely care. However, the longitudinal data essential for
these studies often suffer from temporal sparsity and incompleteness,
presenting substantial challenges in modeling the disease's progression
accurately. Existing methods are limited, focusing primarily on datasets
without missing entries or requiring predefined assumptions about CTh
progression. To overcome these obstacles, we propose a conditional score-based
diffusion model specifically designed to generate CTh trajectories with the
given baseline information, such as age, sex, and initial diagnosis. Our
conditional diffusion model utilizes all available data during the training
phase to make predictions based solely on baseline information during inference
without needing prior history about CTh progression. The prediction accuracy of
the proposed CTh prediction pipeline using a conditional score-based model was
compared for sub-groups consisting of cognitively normal, mild cognitive
impairment, and AD subjects. The Bland-Altman analysis shows our
diffusion-based prediction model has a near-zero bias with narrow 95%
confidential interval compared to the ground-truth CTh in 6-36 months. In
addition, our conditional diffusion model has a stochastic generative nature,
therefore, we demonstrated an uncertainty analysis of patient-specific CTh
prediction through multiple realizations.",0
Counterfactual Reasoning with Knowledge Graph Embeddings,2403.06936v1,http://arxiv.org/abs/2403.06936v1,2024-03-11 17:21:39+00:00,"Knowledge graph embeddings (KGEs) were originally developed to infer true but
missing facts in incomplete knowledge repositories. In this paper, we link
knowledge graph completion and counterfactual reasoning via our new task CFKGR.
We model the original world state as a knowledge graph, hypothetical scenarios
as edges added to the graph, and plausible changes to the graph as inferences
from logical rules. We create corresponding benchmark datasets, which contain
diverse hypothetical scenarios with plausible changes to the original knowledge
graph and facts that should be retained. We develop COULDD, a general method
for adapting existing knowledge graph embeddings given a hypothetical premise,
and evaluate it on our benchmark. Our results indicate that KGEs learn patterns
in the graph without explicit training. We further observe that KGEs adapted
with COULDD solidly detect plausible counterfactual changes to the graph that
follow these patterns. An evaluation on human-annotated data reveals that KGEs
adapted with COULDD are mostly unable to recognize changes to the graph that do
not follow learned inference rules. In contrast, ChatGPT mostly outperforms
KGEs in detecting plausible changes to the graph but has poor knowledge
retention. In summary, CFKGR connects two previously distinct areas, namely KG
completion and counterfactual reasoning.",0
Simplicity Bias of Transformers to Learn Low Sensitivity Functions,2403.06925v1,http://arxiv.org/abs/2403.06925v1,2024-03-11 17:12:09+00:00,"Transformers achieve state-of-the-art accuracy and robustness across many
tasks, but an understanding of the inductive biases that they have and how
those biases are different from other neural network architectures remains
elusive. Various neural network architectures such as fully connected networks
have been found to have a simplicity bias towards simple functions of the data;
one version of this simplicity bias is a spectral bias to learn simple
functions in the Fourier space. In this work, we identify the notion of
sensitivity of the model to random changes in the input as a notion of
simplicity bias which provides a unified metric to explain the simplicity and
spectral bias of transformers across different data modalities. We show that
transformers have lower sensitivity than alternative architectures, such as
LSTMs, MLPs and CNNs, across both vision and language tasks. We also show that
low-sensitivity bias correlates with improved robustness; furthermore, it can
also be used as an efficient intervention to further improve the robustness of
transformers.",0
Distributed Average Consensus via Noisy and Non-Coherent Over-the-Air Aggregation,2403.06920v1,http://arxiv.org/abs/2403.06920v1,2024-03-11 17:08:15+00:00,"Over-the-air aggregation has attracted widespread attention for its potential
advantages in task-oriented applications, such as distributed sensing,
learning, and consensus. In this paper, we develop a communication-efficient
distributed average consensus protocol by utilizing over-the-air aggregation,
which exploits the superposition property of wireless channels rather than
combat it. Noisy channels and non-coherent transmission are taken into account,
and only half-duplex transceivers are required. We prove that the system can
achieve average consensus in mean square and even almost surely under the
proposed protocol. Furthermore, we extend the analysis to the scenarios with
time-varying topology. Numerical simulation shows the effectiveness of the
proposed protocol.",0
MEND: Meta dEmonstratioN Distillation for Efficient and Effective In-Context Learning,2403.06914v2,http://arxiv.org/abs/2403.06914v2,2024-03-11 17:03:04+00:00,"Large Language models (LLMs) have demonstrated impressive in-context learning
(ICL) capabilities, where a LLM makes predictions for a given test input
together with a few input-output pairs (demonstrations). Nevertheless, the
inclusion of demonstrations leads to a quadratic increase in the computational
overhead of the self-attention mechanism. Existing solutions attempt to distill
lengthy demonstrations into compact vectors. However, they often require
task-specific retraining or compromise LLM's in-context learning performance.
To mitigate these challenges, we present Meta dEmonstratioN Distillation
(MEND), where a language model learns to distill any lengthy demonstrations
into vectors without retraining for a new downstream task. We exploit the
knowledge distillation to enhance alignment between MEND and LLM, achieving
both efficiency and effectiveness simultaneously. MEND is endowed with the
meta-knowledge of distilling demonstrations through a two-stage training
process, which includes meta-distillation pretraining and fine-tuning.
Comprehensive evaluations across seven diverse ICL task partitions using
decoder-only (GPT-2) and encoder-decoder (T5) attest to MEND's prowess. It not
only matches but often outperforms the Vanilla ICL as well as other
state-of-the-art distillation models, while significantly reducing the
computational demands. This innovation promises enhanced scalability and
efficiency for the practical deployment of large language models",0
Responsible Artificial Intelligence: A Structured Literature Review,2403.06910v1,http://arxiv.org/abs/2403.06910v1,2024-03-11 17:01:13+00:00,"Our research endeavors to advance the concept of responsible artificial
intelligence (AI), a topic of increasing importance within EU policy
discussions. The EU has recently issued several publications emphasizing the
necessity of trust in AI, underscoring the dual nature of AI as both a
beneficial tool and a potential weapon. This dichotomy highlights the urgent
need for international regulation. Concurrently, there is a need for frameworks
that guide companies in AI development, ensuring compliance with such
regulations. Our research aims to assist lawmakers and machine learning
practitioners in navigating the evolving landscape of AI regulation,
identifying focal areas for future attention. This paper introduces a
comprehensive and, to our knowledge, the first unified definition of
responsible AI. Through a structured literature review, we elucidate the
current understanding of responsible AI. Drawing from this analysis, we propose
an approach for developing a future framework centered around this concept. Our
findings advocate for a human-centric approach to Responsible AI. This approach
encompasses the implementation of AI methods with a strong emphasis on ethics,
model explainability, and the pillars of privacy, security, and trust.",0
Cost-Sensitive Learning to Defer to Multiple Experts with Workload Constraints,2403.06906v1,http://arxiv.org/abs/2403.06906v1,2024-03-11 16:57:20+00:00,"Learning to defer (L2D) aims to improve human-AI collaboration systems by
learning how to defer decisions to humans when they are more likely to be
correct than an ML classifier. Existing research in L2D overlooks key aspects
of real-world systems that impede its practical adoption, namely: i) neglecting
cost-sensitive scenarios, where type 1 and type 2 errors have different costs;
ii) requiring concurrent human predictions for every instance of the training
dataset and iii) not dealing with human work capacity constraints. To address
these issues, we propose the deferral under cost and capacity constraints
framework (DeCCaF). DeCCaF is a novel L2D approach, employing supervised
learning to model the probability of human error under less restrictive data
requirements (only one expert prediction per instance) and using constraint
programming to globally minimize the error cost subject to workload
limitations. We test DeCCaF in a series of cost-sensitive fraud detection
scenarios with different teams of 9 synthetic fraud analysts, with individual
work capacity constraints. The results demonstrate that our approach performs
significantly better than the baselines in a wide array of scenarios, achieving
an average 8.4% reduction in the misclassification cost.",0
One Category One Prompt: Dataset Distillation using Diffusion Models,2403.07142v1,http://arxiv.org/abs/2403.07142v1,2024-03-11 20:23:59+00:00,"The extensive amounts of data required for training deep neural networks pose
significant challenges on storage and transmission fronts. Dataset distillation
has emerged as a promising technique to condense the information of massive
datasets into a much smaller yet representative set of synthetic samples.
However, traditional dataset distillation approaches often struggle to scale
effectively with high-resolution images and more complex architectures due to
the limitations in bi-level optimization. Recently, several works have proposed
exploiting knowledge distillation with decoupled optimization schemes to scale
up dataset distillation. Although these methods effectively address the
scalability issue, they rely on extensive image augmentations requiring the
storage of soft labels for augmented images. In this paper, we introduce
Dataset Distillation using Diffusion Models (D3M) as a novel paradigm for
dataset distillation, leveraging recent advancements in generative
text-to-image foundation models. Our approach utilizes textual inversion, a
technique for fine-tuning text-to-image generative models, to create concise
and informative representations for large datasets. By employing these learned
text prompts, we can efficiently store and infer new samples for introducing
data variability within a fixed memory budget. We show the effectiveness of our
method through extensive experiments across various computer vision benchmark
datasets with different memory budgets.",0
FocusCLIP: Multimodal Subject-Level Guidance for Zero-Shot Transfer in Human-Centric Tasks,2403.06904v1,http://arxiv.org/abs/2403.06904v1,2024-03-11 16:56:37+00:00,"We propose FocusCLIP, integrating subject-level guidance--a specialized
mechanism for target-specific supervision--into the CLIP framework for improved
zero-shot transfer on human-centric tasks. Our novel contributions enhance CLIP
on both the vision and text sides. On the vision side, we incorporate ROI
heatmaps emulating human visual attention mechanisms to emphasize
subject-relevant image regions. On the text side, we introduce human pose
descriptions to provide rich contextual information. For human-centric tasks,
FocusCLIP is trained with images from the MPII Human Pose dataset. The proposed
approach surpassed CLIP by an average of 8.61% across five previously unseen
datasets covering three human-centric tasks. FocusCLIP achieved an average
accuracy of 33.65% compared to 25.04% by CLIP. We observed a 3.98% improvement
in activity recognition, a 14.78% improvement in age classification, and a
7.06% improvement in emotion recognition. Moreover, using our proposed
single-shot LLM prompting strategy, we release a high-quality MPII Pose
Descriptions dataset to encourage further research in multimodal learning for
human-centric tasks. Furthermore, we also demonstrate the effectiveness of our
subject-level supervision on non-human-centric tasks. FocusCLIP shows a 2.47%
improvement over CLIP in zero-shot bird classification using the CUB dataset.
Our findings emphasize the potential of integrating subject-level guidance with
general pretraining methods for enhanced downstream performance.",0
Benign overfitting in leaky ReLU networks with moderate input dimension,2403.06903v1,http://arxiv.org/abs/2403.06903v1,2024-03-11 16:56:01+00:00,"The problem of benign overfitting asks whether it is possible for a model to
perfectly fit noisy training data and still generalize well. We study benign
overfitting in two-layer leaky ReLU networks trained with the hinge loss on a
binary classification task. We consider input data which can be decomposed into
the sum of a common signal and a random noise component, which lie on subspaces
orthogonal to one another. We characterize conditions on the signal to noise
ratio (SNR) of the model parameters giving rise to benign versus non-benign, or
harmful, overfitting: in particular, if the SNR is high then benign overfitting
occurs, conversely if the SNR is low then harmful overfitting occurs. We
attribute both benign and non-benign overfitting to an approximate margin
maximization property and show that leaky ReLU networks trained on hinge loss
with Gradient Descent (GD) satisfy this property. In contrast to prior work we
do not require near orthogonality conditions on the training data: notably, for
input dimension $d$ and training sample size $n$, while prior work shows
asymptotically optimal error when $d = \Omega(n^2 \log n)$, here we require
only $d = \Omega\left(n \log \frac{1}{\epsilon}\right)$ to obtain error within
$\epsilon$ of optimal.",0
Deep adaptative spectral zoom for improved remote heart rate estimation,2403.06902v1,http://arxiv.org/abs/2403.06902v1,2024-03-11 16:55:19+00:00,"Recent advances in remote heart rate measurement, motivated by data-driven
approaches, have notably enhanced accuracy. However, these improvements
primarily focus on recovering the rPPG signal, overlooking the implicit
challenges of estimating the heart rate (HR) from the derived signal. While
many methods employ the Fast Fourier Transform (FFT) for HR estimation, the
performance of the FFT is inherently affected by a limited frequency
resolution. In contrast, the Chirp-Z Transform (CZT), a generalization form of
FFT, can refine the spectrum to the narrow-band range of interest for heart
rate, providing improved frequential resolution and, consequently, more
accurate estimation. This paper presents the advantages of employing the CZT
for remote HR estimation and introduces a novel data-driven adaptive CZT
estimator. The objective of our proposed model is to tailor the CZT to match
the characteristics of each specific dataset sensor, facilitating a more
optimal and accurate estimation of HR from the rPPG signal without compromising
generalization across diverse datasets. This is achieved through a Sparse
Matrix Optimization (SMO). We validate the effectiveness of our model through
exhaustive evaluations on three publicly available datasets UCLA-rPPG, PURE,
and UBFC-rPPG employing both intra- and cross-database performance metrics. The
results reveal outstanding heart rate estimation capabilities, establishing the
proposed approach as a robust and versatile estimator for any rPPG method.",0
LIBR+: Improving Intraoperative Liver Registration by Learning the Residual of Biomechanics-Based Deformable Registration,2403.06901v1,http://arxiv.org/abs/2403.06901v1,2024-03-11 16:54:44+00:00,"The surgical environment imposes unique challenges to the intraoperative
registration of organ shapes to their preoperatively-imaged geometry.
Biomechanical model-based registration remains popular, while deep learning
solutions remain limited due to the sparsity and variability of intraoperative
measurements and the limited ground-truth deformation of an organ that can be
obtained during the surgery. In this paper, we propose a novel \textit{hybrid}
registration approach that leverage a linearized iterative boundary
reconstruction (LIBR) method based on linear elastic biomechanics, and use deep
neural networks to learn its residual to the ground-truth deformation (LIBR+).
We further formulate a dual-branch spline-residual graph convolutional neural
network (SR-GCN) to assimilate information from sparse and variable
intraoperative measurements and effectively propagate it through the geometry
of the 3D organ. Experiments on a large intraoperative liver registration
dataset demonstrated the consistent improvements achieved by LIBR+ in
comparison to existing rigid, biomechnical model-based non-rigid, and
deep-learning based non-rigid approaches to intraoperative liver registration.",0
"Dynamic Client Clustering, Bandwidth Allocation, and Workload Optimization for Semi-synchronous Federated Learning",2403.06900v1,http://arxiv.org/abs/2403.06900v1,2024-03-11 16:54:23+00:00,"Federated Learning (FL) revolutionizes collaborative machine learning among
Internet of Things (IoT) devices by enabling them to train models collectively
while preserving data privacy. FL algorithms fall into two primary categories:
synchronous and asynchronous. While synchronous FL efficiently handles
straggler devices, it can compromise convergence speed and model accuracy. In
contrast, asynchronous FL allows all devices to participate but incurs high
communication overhead and potential model staleness. To overcome these
limitations, the semi-synchronous FL framework introduces client tiering based
on computing and communication latencies. Clients in different tiers upload
their local models at distinct frequencies, striking a balance between
straggler mitigation and communication costs. Enter the DecantFed algorithm
(Dynamic client clustering, bandwidth allocation, and local training for
semi-synchronous Federated learning), a dynamic solution that optimizes client
clustering, bandwidth allocation, and local training workloads to maximize data
sample processing rates. Additionally, DecantFed adapts client learning rates
according to their tiers, addressing the model staleness problem. The
algorithm's performance shines in extensive simulations using benchmark
datasets, including MNIST and CIFAR-10, under independent and identically
distributed (IID) and non-IID scenarios. DecantFed outpaces FedAvg and FedProx
in terms of convergence speed and delivers a remarkable minimum 28% boost in
model accuracy compared to FedProx.",0
Application of Quantum Tensor Networks for Protein Classification,2403.06890v1,http://arxiv.org/abs/2403.06890v1,2024-03-11 16:47:09+00:00,"We show that protein sequences can be thought of as sentences in natural
language processing and can be parsed using the existing Quantum Natural
Language framework into parameterized quantum circuits of reasonable qubits,
which can be trained to solve various protein-related machine-learning
problems. We classify proteins based on their subcellular locations, a pivotal
task in bioinformatics that is key to understanding biological processes and
disease mechanisms. Leveraging the quantum-enhanced processing capabilities, we
demonstrate that Quantum Tensor Networks (QTN) can effectively handle the
complexity and diversity of protein sequences. We present a detailed
methodology that adapts QTN architectures to the nuanced requirements of
protein data, supported by comprehensive experimental results. We demonstrate
two distinct QTNs, inspired by classical recurrent neural networks (RNN) and
convolutional neural networks (CNN), to solve the binary classification task
mentioned above. Our top-performing quantum model has achieved a 94% accuracy
rate, which is comparable to the performance of a classical model that uses the
ESM2 protein language model embeddings. It's noteworthy that the ESM2 model is
extremely large, containing 8 million parameters in its smallest configuration,
whereas our best quantum model requires only around 800 parameters. We
demonstrate that these hybrid models exhibit promising performance, showcasing
their potential to compete with classical models of similar complexity.",0
Process signature-driven high spatio-temporal resolution alignment of multimodal data,2403.06888v2,http://arxiv.org/abs/2403.06888v2,2024-03-11 16:45:19+00:00,"We present HiRA-Pro, a novel procedure to align, at high spatio-temporal
resolutions, multimodal signals from real-world processes and systems that
exhibit diverse transient, nonlinear stochastic dynamics, such as manufacturing
machines. It is based on discerning and synchronizing the process signatures of
salient kinematic and dynamic events in these disparate signals. HiRA-Pro
addresses the challenge of aligning data with sub-millisecond phenomena, where
traditional timestamp, external trigger, or clock-based alignment methods fall
short. The effectiveness of HiRA-Pro is demonstrated in a smart manufacturing
context, where it aligns data from 13+ channels acquired during 3D-printing and
milling operations on an Optomec-LENS MTS 500 hybrid machine. The aligned data
is then voxelized to generate 0.25 second aligned data chunks that correspond
to physical voxels on the produced part. The superiority of HiRA-Pro is further
showcased through case studies in additive manufacturing, demonstrating
improved machine learning-based predictive performance due to precise
multimodal data alignment. Specifically, testing classification accuracies
improved by almost 35% with the application of HiRA-Pro, even with limited
data, allowing for precise localization of artifacts. The paper also provides a
comprehensive discussion on the proposed method, its applications, and
comparative qualitative analysis with a few other alignment methods. HiRA-Pro
achieves temporal-spatial resolutions of 10-1000 us and 100 um in order to
generate datasets that register with physical voxels on the 3D-printed and
milled part. These resolutions are at least an order of magnitude finer than
the existing alignment methods that employ individual timestamps, statistical
correlations, or common clocks, which achieve precision of hundreds of
milliseconds.",0
A Holistic Framework Towards Vision-based Traffic Signal Control with Microscopic Simulation,2403.06884v1,http://arxiv.org/abs/2403.06884v1,2024-03-11 16:42:29+00:00,"Traffic signal control (TSC) is crucial for reducing traffic congestion that
leads to smoother traffic flow, reduced idling time, and mitigated CO2
emissions. In this study, we explore the computer vision approach for TSC that
modulates on-road traffic flows through visual observation. Unlike traditional
feature-based approaches, vision-based methods depend much less on heuristics
and predefined features, bringing promising potentials for end-to-end learning
and optimization of traffic signals. Thus, we introduce a holistic traffic
simulation framework called TrafficDojo towards vision-based TSC and its
benchmarking by integrating the microscopic traffic flow provided in SUMO into
the driving simulator MetaDrive. This proposed framework offers a versatile
traffic environment for in-depth analysis and comprehensive evaluation of
traffic signal controllers across diverse traffic conditions and scenarios. We
establish and compare baseline algorithms including both traditional and
Reinforecment Learning (RL) approaches. This work sheds insights into the
design and development of vision-based TSC approaches and open up new research
opportunities. All the code and baselines will be made publicly available.",0
Unveiling the Significance of Toddler-Inspired Reward Transition in Goal-Oriented Reinforcement Learning,2403.06880v1,http://arxiv.org/abs/2403.06880v1,2024-03-11 16:34:23+00:00,"Toddlers evolve from free exploration with sparse feedback to exploiting
prior experiences for goal-directed learning with denser rewards. Drawing
inspiration from this Toddler-Inspired Reward Transition, we set out to explore
the implications of varying reward transitions when incorporated into
Reinforcement Learning (RL) tasks. Central to our inquiry is the transition
from sparse to potential-based dense rewards, which share optimal strategies
regardless of reward changes. Through various experiments, including those in
egocentric navigation and robotic arm manipulation tasks, we found that proper
reward transitions significantly influence sample efficiency and success rates.
Of particular note is the efficacy of the toddler-inspired Sparse-to-Dense
(S2D) transition. Beyond these performance metrics, using Cross-Density
Visualizer technique, we observed that transitions, especially the S2D, smooth
the policy loss landscape, promoting wide minima that enhance generalization in
RL models.",0
COOD: Combined out-of-distribution detection using multiple measures for anomaly & novel class detection in large-scale hierarchical classification,2403.06874v1,http://arxiv.org/abs/2403.06874v1,2024-03-11 16:26:35+00:00,"High-performing out-of-distribution (OOD) detection, both anomaly and novel
class, is an important prerequisite for the practical use of classification
models. In this paper, we focus on the species recognition task in images
concerned with large databases, a large number of fine-grained hierarchical
classes, severe class imbalance, and varying image quality. We propose a
framework for combining individual OOD measures into one combined OOD (COOD)
measure using a supervised model. The individual measures are several existing
state-of-the-art measures and several novel OOD measures developed with novel
class detection and hierarchical class structure in mind. COOD was extensively
evaluated on three large-scale (500k+ images) biodiversity datasets in the
context of anomaly and novel class detection. We show that COOD outperforms
individual, including state-of-the-art, OOD measures by a large margin in terms
of TPR@1% FPR in the majority of experiments, e.g., improving detecting
ImageNet images (OOD) from 54.3% to 85.4% for the iNaturalist 2018 dataset.
SHAP (feature contribution) analysis shows that different individual OOD
measures are essential for various tasks, indicating that multiple OOD measures
and combinations are needed to generalize. Additionally, we show that
explicitly considering ID images that are incorrectly classified for the
original (species) recognition task is important for constructing
high-performing OOD detection methods and for practical applicability. The
framework can easily be extended or adapted to other tasks and media
modalities.",0
Ant Colony Sampling with GFlowNets for Combinatorial Optimization,2403.07041v1,http://arxiv.org/abs/2403.07041v1,2024-03-11 16:26:06+00:00,"This paper introduces the Generative Flow Ant Colony Sampler (GFACS), a novel
neural-guided meta-heuristic algorithm for combinatorial optimization. GFACS
integrates generative flow networks (GFlowNets) with the ant colony
optimization (ACO) methodology. GFlowNets, a generative model that learns a
constructive policy in combinatorial spaces, enhance ACO by providing an
informed prior distribution of decision variables conditioned on input graph
instances. Furthermore, we introduce a novel combination of training tricks,
including search-guided local exploration, energy normalization, and energy
shaping to improve GFACS. Our experimental results demonstrate that GFACS
outperforms baseline ACO algorithms in seven CO tasks and is competitive with
problem-specific heuristics for vehicle routing problems. The source code is
available at \url{https://github.com/ai4co/gfacs}.",0
Last Iterate Convergence of Incremental Methods and Applications in Continual Learning,2403.06873v1,http://arxiv.org/abs/2403.06873v1,2024-03-11 16:24:26+00:00,"Incremental gradient methods and incremental proximal methods are a
fundamental class of optimization algorithms used for solving finite sum
problems, broadly studied in the literature. Yet, when it comes to their
convergence guarantees, nonasymptotic (first-order or proximal) oracle
complexity bounds have been obtained fairly recently, almost exclusively
applying to the average iterate. Motivated by applications in continual
learning, we obtain the first convergence guarantees for the last iterate of
both incremental gradient and incremental proximal methods, in general convex
smooth (for both) and convex Lipschitz (for the proximal variants) settings.
Our oracle complexity bounds for the last iterate nearly match (i.e., match up
to a square-root-log or a log factor) the best known oracle complexity bounds
for the average iterate, for both classes of methods. We further obtain
generalizations of our results to weighted averaging of the iterates with
increasing weights, which can be seen as interpolating between the last iterate
and the average iterate guarantees. Additionally, we discuss how our results
can be generalized to variants of studied incremental methods with permuted
ordering of updates. Our results generalize last iterate guarantees for
incremental methods compared to state of the art, as such results were
previously known only for overparameterized linear models, which correspond to
convex quadratic problems with infinitely many solutions.",0
Exploring Large Language Models and Hierarchical Frameworks for Classification of Large Unstructured Legal Documents,2403.06872v1,http://arxiv.org/abs/2403.06872v1,2024-03-11 16:24:08+00:00,"Legal judgment prediction suffers from the problem of long case documents
exceeding tens of thousands of words, in general, and having a non-uniform
structure. Predicting judgments from such documents becomes a challenging task,
more so on documents with no structural annotation. We explore the
classification of these large legal documents and their lack of structural
information with a deep-learning-based hierarchical framework which we call
MESc; ""Multi-stage Encoder-based Supervised with-clustering""; for judgment
prediction. Specifically, we divide a document into parts to extract their
embeddings from the last four layers of a custom fine-tuned Large Language
Model, and try to approximate their structure through unsupervised clustering.
Which we use in another set of transformer encoder layers to learn the
inter-chunk representations. We analyze the adaptability of Large Language
Models (LLMs) with multi-billion parameters (GPT-Neo, and GPT-J) with the
hierarchical framework of MESc and compare them with their standalone
performance on legal texts. We also study their intra-domain(legal) transfer
learning capability and the impact of combining embeddings from their last
layers in MESc. We test these methods and their effectiveness with extensive
experiments and ablation studies on legal documents from India, the European
Union, and the United States with the ILDC dataset and a subset of the LexGLUE
dataset. Our approach achieves a minimum total performance gain of
approximately 2 points over previous state-of-the-art methods.",0
Gluon Double-Spin Asymmetry in the Longitudinally Polarized $p+p$ Collisions,2403.06959v1,http://arxiv.org/abs/2403.06959v1,2024-03-11 17:45:18+00:00,"We derive the first-ever small-$x$ expression for the inclusive gluon
production cross section in the central rapidity region of the longitudinally
polarized proton-proton collisions. The cross section depends on the
polarizations of both protons, therefore comprising the numerator of the
longitudinal double-spin asymmetry $A_{LL}$ for the produced gluons. The cross
section is calculated in the shock wave formalism and is expressed in terms of
the polarized dipole scattering amplitudes on the projectile and target
protons. We show that the small-$x$ evolution corrections are included into our
cross section expression if one evolves these polarized dipole amplitudes using
the double-logarithmic helicity evolution derived in \cite{Kovchegov:2015pbl,
Kovchegov:2016zex, Kovchegov:2018znm, Cougoulic:2022gbk}. Our calculation is
performed for the gluon sector only, with the quark contribution left for
future work. When that work is complete, the resulting formula will be
applicable to longitudinally polarized proton-proton and proton-nucleus
collisions, as well as to polarized semi-inclusive deep inelastic scattering
(SIDIS) on a proton or a nucleus. Our results should allow one to extend the
small-$x$ helicity phenomenology analysis of \cite{Adamiak:2023yhz} to the
jet/hadron production data reported for the longitudinally polarized
proton-proton collisions at RHIC and to polarized SIDIS measurements at central
rapidities to be performed at the EIC.",0
Explainable Transformer Prototypes for Medical Diagnoses,2403.06961v1,http://arxiv.org/abs/2403.06961v1,2024-03-11 17:46:21+00:00,"Deployments of artificial intelligence in medical diagnostics mandate not
just accuracy and efficacy but also trust, emphasizing the need for
explainability in machine decisions. The recent trend in automated medical
image diagnostics leans towards the deployment of Transformer-based
architectures, credited to their impressive capabilities. Since the
self-attention feature of transformers contributes towards identifying crucial
regions during the classification process, they enhance the trustability of the
methods. However, the complex intricacies of these attention mechanisms may
fall short of effectively pinpointing the regions of interest directly
influencing AI decisions. Our research endeavors to innovate a unique attention
block that underscores the correlation between 'regions' rather than 'pixels'.
To address this challenge, we introduce an innovative system grounded in
prototype learning, featuring an advanced self-attention mechanism that goes
beyond conventional ad-hoc visual explanation techniques by offering
comprehensible visual insights. A combined quantitative and qualitative
methodological approach was used to demonstrate the effectiveness of the
proposed method on the large-scale NIH chest X-ray dataset. Experimental
results showed that our proposed method offers a promising direction for
explainability, which can lead to the development of more trustable systems,
which can facilitate easier and rapid adoption of such technology into routine
clinics. The code is available at www.github.com/NUBagcilab/r2r_proto.",0
The pitfalls of next-token prediction,2403.06963v1,http://arxiv.org/abs/2403.06963v1,2024-03-11 17:47:30+00:00,"Can a mere next-token predictor faithfully model human intelligence? We
crystallize this intuitive concern, which is fragmented in the literature. As a
starting point, we argue that the two often-conflated phases of next-token
prediction -- autoregressive inference and teacher-forced training -- must be
treated distinctly. The popular criticism that errors can compound during
autoregressive inference, crucially assumes that teacher-forcing has learned an
accurate next-token predictor. This assumption sidesteps a more deep-rooted
problem we expose: in certain classes of tasks, teacher-forcing can simply fail
to learn an accurate next-token predictor in the first place. We describe a
general mechanism of how teacher-forcing can fail, and design a minimal
planning task where both the Transformer and the Mamba architecture empirically
fail in that manner -- remarkably, despite the task being straightforward to
learn. We provide preliminary evidence that this failure can be resolved when
training to predict multiple tokens in advance. We hope this finding can ground
future debates and inspire explorations beyond the next-token prediction
paradigm. We make our code available under
https://github.com/gregorbachmann/Next-Token-Failures",0
Acquiring Diverse Skills using Curriculum Reinforcement Learning with Mixture of Experts,2403.06966v1,http://arxiv.org/abs/2403.06966v1,2024-03-11 17:49:18+00:00,"Reinforcement learning (RL) is a powerful approach for acquiring a
good-performing policy. However, learning diverse skills is challenging in RL
due to the commonly used Gaussian policy parameterization. We propose
\textbf{Di}verse \textbf{Skil}l \textbf{L}earning (Di-SkilL), an RL method for
learning diverse skills using Mixture of Experts, where each expert formalizes
a skill as a contextual motion primitive. Di-SkilL optimizes each expert and
its associate context distribution to a maximum entropy objective that
incentivizes learning diverse skills in similar contexts. The per-expert
context distribution enables automatic curricula learning, allowing each expert
to focus on its best-performing sub-region of the context space. To overcome
hard discontinuities and multi-modalities without any prior knowledge of the
environment's unknown context probability space, we leverage energy-based
models to represent the per-expert context distributions and demonstrate how we
can efficiently train them using the standard policy gradient objective. We
show on challenging robot simulation tasks that Di-SkilL can learn diverse and
performant skills.",0
Exploring Cluster Analysis in Nelore Cattle Visual Score Attribution,2403.07137v1,http://arxiv.org/abs/2403.07137v1,2024-03-11 20:07:05+00:00,"Assessing the biotype of cattle through human visual inspection is a very
common and important practice in precision cattle breeding. This paper presents
the results of a correlation analysis between scores produced by humans for
Nelore cattle and a variety of measurements that can be derived from images or
other instruments. It also presents a study using the k-means algorithm to
generate new ways of clustering a batch of cattle using the measurements that
most correlate with the animal's body weight and visual scores.",0
On the Limited Representational Power of Value Functions and its Links to Statistical (In)Efficiency,2403.07136v1,http://arxiv.org/abs/2403.07136v1,2024-03-11 20:05:48+00:00,"Identifying the trade-offs between model-based and model-free methods is a
central question in reinforcement learning. Value-based methods offer
substantial computational advantages and are sometimes just as statistically
efficient as model-based methods. However, focusing on the core problem of
policy evaluation, we show information about the transition dynamics may be
impossible to represent in the space of value functions. We explore this
through a series of case studies focused on structures that arises in many
important problems. In several, there is no information loss and value-based
methods are as statistically efficient as model based ones. In other
closely-related examples, information loss is severe and value-based methods
are severely outperformed. A deeper investigation points to the limitations of
the representational power as the driver of the inefficiency, as opposed to
failure in algorithm design.",0
COMQ: A Backpropagation-Free Algorithm for Post-Training Quantization,2403.07134v1,http://arxiv.org/abs/2403.07134v1,2024-03-11 20:04:03+00:00,"Post-training quantization (PTQ) has emerged as a practical approach to
compress large neural networks, making them highly efficient for deployment.
However, effectively reducing these models to their low-bit counterparts
without compromising the original accuracy remains a key challenge. In this
paper, we propose an innovative PTQ algorithm termed COMQ, which sequentially
conducts coordinate-wise minimization of the layer-wise reconstruction errors.
We consider the widely used integer quantization, where every quantized weight
can be decomposed into a shared floating-point scalar and an integer bit-code.
Within a fixed layer, COMQ treats all the scaling factor(s) and bit-codes as
the variables of the reconstruction error. Every iteration improves this error
along a single coordinate while keeping all other variables constant. COMQ is
easy to use and requires no hyper-parameter tuning. It instead involves only
dot products and rounding operations. We update these variables in a carefully
designed greedy order, significantly enhancing the accuracy. COMQ achieves
remarkable results in quantizing 4-bit Vision Transformers, with a negligible
loss of less than 1% in Top-1 accuracy. In 4-bit INT quantization of
convolutional neural networks, COMQ maintains near-lossless accuracy with a
minimal drop of merely 0.3% in Top-1 accuracy.",0
A New Machine Learning Dataset of Bulldog Nostril Images for Stenosis Degree Classification,2403.07132v1,http://arxiv.org/abs/2403.07132v1,2024-03-11 20:02:17+00:00,"Brachycephaly, a conformation trait in some dog breeds, causes BOAS, a
respiratory disorder that affects the health and welfare of the dogs with
various symptoms. In this paper, a new annotated dataset composed of 190 images
of bulldogs' nostrils is presented. Three degrees of stenosis are approximately
equally represented in the dataset: mild, moderate and severe stenosis. The
dataset also comprises a small quantity of non stenotic nostril images. To the
best of our knowledge, this is the first image dataset addressing this problem.
Furthermore, deep learning is investigated as an alternative to automatically
infer stenosis degree using nostril images. In this work, several neural
networks were tested: ResNet50, MobileNetV3, DenseNet201, SwinV2 and MaxViT.
For this evaluation, the problem was modeled in two different ways: first, as a
three-class classification problem (mild or open, moderate, and severe);
second, as a binary classification problem, with severe stenosis as target. For
the multiclass classification, a maximum median f-score of 53.77\% was achieved
by the MobileNetV3. For binary classification, a maximum median f-score of
72.08\% has been reached by ResNet50, indicating that the problem is
challenging but possibly tractable.",0
Bigraph Matching Weighted with Learnt Incentive Function for Multi-Robot Task Allocation,2403.07131v1,http://arxiv.org/abs/2403.07131v1,2024-03-11 19:55:08+00:00,"Most real-world Multi-Robot Task Allocation (MRTA) problems require fast and
efficient decision-making, which is often achieved using heuristics-aided
methods such as genetic algorithms, auction-based methods, and bipartite graph
matching methods. These methods often assume a form that lends better
explainability compared to an end-to-end (learnt) neural network based policy
for MRTA. However, deriving suitable heuristics can be tedious, risky and in
some cases impractical if problems are too complex. This raises the question:
can these heuristics be learned? To this end, this paper particularly develops
a Graph Reinforcement Learning (GRL) framework to learn the heuristics or
incentives for a bipartite graph matching approach to MRTA. Specifically a
Capsule Attention policy model is used to learn how to weight task/robot
pairings (edges) in the bipartite graph that connects the set of tasks to the
set of robots. The original capsule attention network architecture is
fundamentally modified by adding encoding of robots' state graph, and two
Multihead Attention based decoders whose output are used to construct a
LogNormal distribution matrix from which positive bigraph weights can be drawn.
The performance of this new bigraph matching approach augmented with a
GRL-derived incentive is found to be at par with the original bigraph matching
approach that used expert-specified heuristics, with the former offering
notable robustness benefits. During training, the learned incentive policy is
found to get initially closer to the expert-specified incentive and then
slightly deviate from its trend.",0
RaceMOP: Mapless Online Path Planning for Multi-Agent Autonomous Racing using Residual Policy Learning,2403.07129v1,http://arxiv.org/abs/2403.07129v1,2024-03-11 19:52:00+00:00,"The interactive decision-making in multi-agent autonomous racing offers
insights valuable beyond the domain of self-driving cars. Mapless online path
planning is particularly of practical appeal but poses a challenge for safely
overtaking opponents due to the limited planning horizon. Accordingly, this
paper introduces RaceMOP, a novel method for mapless online path planning
designed for multi-agent racing of F1TENTH cars. Unlike classical planners that
depend on predefined racing lines, RaceMOP operates without a map, relying
solely on local observations to overtake other race cars at high speed. Our
approach combines an artificial potential field method as a base policy with
residual policy learning to introduce long-horizon planning capabilities. We
advance the field by introducing a novel approach for policy fusion with the
residual policy directly in probability space. Our experiments for twelve
simulated racetracks validate that RaceMOP is capable of long-horizon
decision-making with robust collision avoidance during overtaking maneuvers.
RaceMOP demonstrates superior handling over existing mapless planners while
generalizing to unknown racetracks, paving the way for further use of our
method in robotics. We make the open-source code for RaceMOP available at
http://github.com/raphajaner/racemop.",0
FAX: Scalable and Differentiable Federated Primitives in JAX,2403.07128v1,http://arxiv.org/abs/2403.07128v1,2024-03-11 19:51:01+00:00,"We present FAX, a JAX-based library designed to support large-scale
distributed and federated computations in both data center and cross-device
applications. FAX leverages JAX's sharding mechanisms to enable native
targeting of TPUs and state-of-the-art JAX runtimes, including Pathways. FAX
embeds building blocks for federated computations as primitives in JAX. This
enables three key benefits. First, FAX computations can be translated to XLA
HLO. Second, FAX provides a full implementation of federated automatic
differentiation, greatly simplifying the expression of federated computations.
Last, FAX computations can be interpreted out to existing production
cross-device federated compute systems. We show that FAX provides an easily
programmable, performant, and scalable framework for federated computations in
the data center. FAX is available at
https://github.com/google-research/google-research/tree/master/fax .",0
Learning-Aided Control of Robotic Tether-Net with Maneuverable Nodes to Capture Large Space Debris,2403.07125v1,http://arxiv.org/abs/2403.07125v1,2024-03-11 19:41:40+00:00,"Maneuverable tether-net systems launched from an unmanned spacecraft offer a
promising solution for the active removal of large space debris. Guaranteeing
the successful capture of such space debris is dependent on the ability to
reliably maneuver the tether-net system -- a flexible, many-DoF (thus complex)
system -- for a wide range of launch scenarios. Here, scenarios are defined by
the relative location of the debris with respect to the chaser spacecraft. This
paper represents and solves this problem as a hierarchically decentralized
implementation of robotic trajectory planning and control and demonstrates the
effectiveness of the approach when applied to two different tether-net systems,
with 4 and 8 maneuverable units (MUs), respectively. Reinforcement learning
(policy gradient) is used to design the centralized trajectory planner that,
based on the relative location of the target debris at the launch of the net,
computes the final aiming positions of each MU, from which their trajectory can
be derived. Each MU then seeks to follow its assigned trajectory by using a
decentralized PID controller that outputs the MU's thrust vector and is
informed by noisy sensor feedback (for realism) of its relative location.
System performance is assessed in terms of capture success and overall fuel
consumption by the MUs. Reward shaping and surrogate models are used to
respectively guide and speed up the RL process. Simulation-based experiments
show that this approach allows the successful capture of debris at fuel costs
that are notably lower than nominal baselines, including in scenarios where the
debris is significantly off-centered compared to the approaching chaser
spacecraft.",0
Simulation-Based Segmentation of Blood Vessels in Cerebral 3D OCTA Images,2403.07116v1,http://arxiv.org/abs/2403.07116v1,2024-03-11 19:14:51+00:00,"Segmentation of blood vessels in murine cerebral 3D OCTA images is
foundational for in vivo quantitative analysis of the effects of neurovascular
disorders, such as stroke or Alzheimer's, on the vascular network. However, to
accurately segment blood vessels with state-of-the-art deep learning methods, a
vast amount of voxel-level annotations is required. Since cerebral 3D OCTA
images are typically plagued by artifacts and generally have a low
signal-to-noise ratio, acquiring manual annotations poses an especially
cumbersome and time-consuming task. To alleviate the need for manual
annotations, we propose utilizing synthetic data to supervise segmentation
algorithms. To this end, we extract patches from vessel graphs and transform
them into synthetic cerebral 3D OCTA images paired with their matching ground
truth labels by simulating the most dominant 3D OCTA artifacts. In extensive
experiments, we demonstrate that our approach achieves competitive results,
enabling annotation-free blood vessel segmentation in cerebral 3D OCTA images.",0
Class Imbalance in Object Detection: An Experimental Diagnosis and Study of Mitigation Strategies,2403.07113v1,http://arxiv.org/abs/2403.07113v1,2024-03-11 19:06:04+00:00,"Object detection, a pivotal task in computer vision, is frequently hindered
by dataset imbalances, particularly the under-explored issue of
foreground-foreground class imbalance. This lack of attention to
foreground-foreground class imbalance becomes even more pronounced in the
context of single-stage detectors. This study introduces a benchmarking
framework utilizing the YOLOv5 single-stage detector to address the problem of
foreground-foreground class imbalance. We crafted a novel 10-class long-tailed
dataset from the COCO dataset, termed COCO-ZIPF, tailored to reflect common
real-world detection scenarios with a limited number of object classes. Against
this backdrop, we scrutinized three established techniques: sampling, loss
weighing, and data augmentation. Our comparative analysis reveals that sampling
and loss reweighing methods, while shown to be beneficial in two-stage detector
settings, do not translate as effectively in improving YOLOv5's performance on
the COCO-ZIPF dataset. On the other hand, data augmentation methods,
specifically mosaic and mixup, significantly enhance the model's mean Average
Precision (mAP), by introducing more variability and complexity into the
training data. (Code available:
https://github.com/craston/object_detection_cib)",0
A slice classification neural network for automated classification of axial PET/CT slices from a multi-centric lymphoma dataset,2403.07105v1,http://arxiv.org/abs/2403.07105v1,2024-03-11 18:57:45+00:00,"Automated slice classification is clinically relevant since it can be
incorporated into medical image segmentation workflows as a preprocessing step
that would flag slices with a higher probability of containing tumors, thereby
directing physicians attention to the important slices. In this work, we train
a ResNet-18 network to classify axial slices of lymphoma PET/CT images
(collected from two institutions) depending on whether the slice intercepted a
tumor (positive slice) in the 3D image or if the slice did not (negative
slice). Various instances of the network were trained on 2D axial datasets
created in different ways: (i) slice-level split and (ii) patient-level split;
inputs of different types were used: (i) only PET slices and (ii) concatenated
PET and CT slices; and different training strategies were employed: (i)
center-aware (CAW) and (ii) center-agnostic (CAG). Model performances were
compared using the area under the receiver operating characteristic curve
(AUROC) and the area under the precision-recall curve (AUPRC), and various
binary classification metrics. We observe and describe a performance
overestimation in the case of slice-level split as compared to the
patient-level split training. The model trained using patient-level split data
with the network input containing only PET slices in the CAG training regime
was the best performing/generalizing model on a majority of metrics. Our models
were additionally more closely compared using the sensitivity metric on the
positive slices from their respective test sets.",0
Overcoming the Paradox of Certified Training with Gaussian Smoothing,2403.07095v1,http://arxiv.org/abs/2403.07095v1,2024-03-11 18:44:36+00:00,"Training neural networks with high certified accuracy against adversarial
examples remains an open problem despite significant efforts. While
certification methods can effectively leverage tight convex relaxations for
bound computation, in training, these methods perform worse than looser
relaxations. Prior work hypothesized that this is caused by the discontinuity
and perturbation sensitivity of the loss surface induced by these tighter
relaxations. In this work, we show theoretically that Gaussian Loss Smoothing
can alleviate both of these issues. We confirm this empirically by proposing a
certified training method combining PGPE, an algorithm computing gradients of a
smoothed loss, with different convex relaxations. When using this training
method, we observe that tighter bounds indeed lead to strictly better networks
that can outperform state-of-the-art methods on the same network. While scaling
PGPE-based training remains challenging due to high computational cost, our
results clearly demonstrate the promise of Gaussian Loss Smoothing for training
certifiably robust neural networks.",0
FALCON: FLOP-Aware Combinatorial Optimization for Neural Network Pruning,2403.07094v1,http://arxiv.org/abs/2403.07094v1,2024-03-11 18:40:47+00:00,"The increasing computational demands of modern neural networks present
deployment challenges on resource-constrained devices. Network pruning offers a
solution to reduce model size and computational cost while maintaining
performance. However, most current pruning methods focus primarily on improving
sparsity by reducing the number of nonzero parameters, often neglecting other
deployment costs such as inference time, which are closely related to the
number of floating-point operations (FLOPs). In this paper, we propose FALCON,
a novel combinatorial-optimization-based framework for network pruning that
jointly takes into account model accuracy (fidelity), FLOPs, and sparsity
constraints. A main building block of our approach is an integer linear program
(ILP) that simultaneously handles FLOP and sparsity constraints. We present a
novel algorithm to approximately solve the ILP. We propose a novel first-order
method for our optimization framework which makes use of our ILP solver. Using
problem structure (e.g., the low-rank structure of approx. Hessian), we can
address instances with millions of parameters. Our experiments demonstrate that
FALCON achieves superior accuracy compared to other pruning approaches within a
fixed FLOP budget. For instance, for ResNet50 with 20% of the total FLOPs
retained, our approach improves the accuracy by 48% relative to
state-of-the-art. Furthermore, in gradual pruning settings with re-training
between pruning steps, our framework outperforms existing pruning methods,
emphasizing the significance of incorporating both FLOP and sparsity
constraints for effective network pruning.",0
A cascaded deep network for automated tumor detection and segmentation in clinical PET imaging of diffuse large B-cell lymphoma,2403.07092v1,http://arxiv.org/abs/2403.07092v1,2024-03-11 18:36:55+00:00,"Accurate detection and segmentation of diffuse large B-cell lymphoma (DLBCL)
from PET images has important implications for estimation of total metabolic
tumor volume, radiomics analysis, surgical intervention and radiotherapy.
Manual segmentation of tumors in whole-body PET images is time-consuming,
labor-intensive and operator-dependent. In this work, we develop and validate a
fast and efficient three-step cascaded deep learning model for automated
detection and segmentation of DLBCL tumors from PET images. As compared to a
single end-to-end network for segmentation of tumors in whole-body PET images,
our three-step model is more effective (improves 3D Dice score from 58.9% to
78.1%) since each of its specialized modules, namely the slice classifier, the
tumor detector and the tumor segmentor, can be trained independently to a high
degree of skill to carry out a specific task, rather than a single network with
suboptimal performance on overall segmentation.",0
Sim-to-Real gap in RL: Use Case with TIAGo and Isaac Sim/Gym,2403.07091v1,http://arxiv.org/abs/2403.07091v1,2024-03-11 18:35:32+00:00,"This paper explores policy-learning approaches in the context of sim-to-real
transfer for robotic manipulation using a TIAGo mobile manipulator, focusing on
two state-of-art simulators, Isaac Gym and Isaac Sim, both developed by Nvidia.
Control architectures are discussed, with a particular emphasis on achieving
collision-less movement in both simulation and the real environment. Presented
results demonstrate successful sim-to-real transfer, showcasing similar
movements executed by an RL-trained model in both simulated and real setups.",0
Graph learning methods to extract empathy supporting regions in a naturalistic stimuli fMRI,2403.07089v1,http://arxiv.org/abs/2403.07089v1,2024-03-11 18:33:21+00:00,"Functional MRI (fMRI) research, employing naturalistic stimuli like movies,
explores brain network interactions in complex cognitive processes such as
empathy. The empathy network encompasses multiple brain areas, including the
Insula, PFC, ACC, and parietal regions. Our novel processing pipeline applies
graph learning methods to whole-brain timeseries signals, incorporating
high-pass filtering, voxel-level clustering, and windowed graph learning with a
sparsity-based approach. The study involves two short movies shown to 14
healthy volunteers, considering 54 regions extracted from the AAL Atlas. The
sparsity-based graph learning consistently outperforms, achieving over 88%
accuracy in capturing emotion contagion variations. Temporal analysis reveals a
gradual induction of empathy, supported by the method's effectiveness in
capturing dynamic connectomes through graph clustering. Edge-weight dynamics
analysis underscores sparsity-based learning's superiority, while
connectome-network analysis highlights the pivotal role of the Insula,
Amygdala, and Thalamus in empathy. Spectral filtering analysis emphasizes the
band-pass filter's significance in isolating regions linked to emotional and
empathetic processing during empathy HIGH states. Key regions like Amygdala,
Insula, and Angular Gyrus consistently activate, supporting their critical role
in immediate emotional responses. Strong similarities across movies in graph
cluster labels, connectome-network analysis, and spectral filtering-based
analyses reveal robust neural correlates of empathy. These findings advance our
understanding of empathy-related neural dynamics and identify specific regions
in empathetic responses, offering insights for targeted interventions and
treatments associated with empathetic processing.",0
Exploring the Impact of ChatGPT on Student Interactions in Computer-Supported Collaborative Learning,2403.07082v1,http://arxiv.org/abs/2403.07082v1,2024-03-11 18:18:18+00:00,"The growing popularity of generative AI, particularly ChatGPT, has sparked
both enthusiasm and caution among practitioners and researchers in education.
To effectively harness the full potential of ChatGPT in educational contexts,
it is crucial to analyze its impact and suitability for different educational
purposes. This paper takes an initial step in exploring the applicability of
ChatGPT in a computer-supported collaborative learning (CSCL) environment.
Using statistical analysis, we validate the shifts in student interactions
during an asynchronous group brainstorming session by introducing ChatGPT as an
instantaneous question-answering agent.",0
"Improving deep learning with prior knowledge and cognitive models: A survey on enhancing explainability, adversarial robustness and zero-shot learning",2403.07078v1,http://arxiv.org/abs/2403.07078v1,2024-03-11 18:11:00+00:00,"We review current and emerging knowledge-informed and brain-inspired
cognitive systems for realizing adversarial defenses, eXplainable Artificial
Intelligence (XAI), and zero-shot or few-short learning. Data-driven deep
learning models have achieved remarkable performance and demonstrated
capabilities surpassing human experts in many applications. Yet, their
inability to exploit domain knowledge leads to serious performance limitations
in practical applications. In particular, deep learning systems are exposed to
adversarial attacks, which can trick them into making glaringly incorrect
decisions. Moreover, complex data-driven models typically lack interpretability
or explainability, i.e., their decisions cannot be understood by human
subjects. Furthermore, models are usually trained on standard datasets with a
closed-world assumption. Hence, they struggle to generalize to unseen cases
during inference in practical open-world environments, thus, raising the zero-
or few-shot generalization problem. Although many conventional solutions exist,
explicit domain knowledge, brain-inspired neural network and cognitive
architectures offer powerful new dimensions towards alleviating these problems.
Prior knowledge is represented in appropriate forms and incorporated in deep
learning frameworks to improve performance. Brain-inspired cognition methods
use computational models that mimic the human mind to enhance intelligent
behavior in artificial agents and autonomous robots. Ultimately, these models
achieve better explainability, higher adversarial robustness and data-efficient
learning, and can, in turn, provide insights for cognitive science and
neuroscience-that is, to deepen human understanding on how the brain works in
general, and how it handles these problems.",0
Explainable Learning with Gaussian Processes,2403.07072v1,http://arxiv.org/abs/2403.07072v1,2024-03-11 18:03:02+00:00,"The field of explainable artificial intelligence (XAI) attempts to develop
methods that provide insight into how complicated machine learning methods make
predictions. Many methods of explanation have focused on the concept of feature
attribution, a decomposition of the model's prediction into individual
contributions corresponding to each input feature. In this work, we explore the
problem of feature attribution in the context of Gaussian process regression
(GPR). We take a principled approach to defining attributions under model
uncertainty, extending the existing literature. We show that although GPR is a
highly flexible and non-parametric approach, we can derive interpretable,
closed-form expressions for the feature attributions. When using integrated
gradients as an attribution method, we show that the attributions of a GPR
model also follow a Gaussian process distribution, which quantifies the
uncertainty in attribution arising from uncertainty in the model. We
demonstrate, both through theory and experimentation, the versatility and
robustness of this approach. We also show that, when applicable, the exact
expressions for GPR attributions are both more accurate and less
computationally expensive than the approximations currently used in practice.
The source code for this project is freely available under MIT license at
https://github.com/KurtButler/2024_attributions_paper.",0
Re-Simulation-based Self-Supervised Learning for Pre-Training Foundation Models,2403.07066v1,http://arxiv.org/abs/2403.07066v1,2024-03-11 18:00:47+00:00,"Self-Supervised Learning (SSL) is at the core of training modern large
machine learning models, providing a scheme for learning powerful
representations that can be used in a variety of downstream tasks. However, SSL
strategies must be adapted to the type of training data and downstream tasks
required. We propose RS3L, a novel simulation-based SSL strategy that employs a
method of re-simulation to drive data augmentation for contrastive learning. By
intervening in the middle of the simulation process and re-running simulation
components downstream of the intervention, we generate multiple realizations of
an event, thus producing a set of augmentations covering all physics-driven
variations available in the simulator. Using experiments from high-energy
physics, we explore how this strategy may enable the development of a
foundation model; we show how R3SL pre-training enables powerful performance in
downstream tasks such as discrimination of a variety of objects and uncertainty
mitigation. In addition to our results, we make the RS3L dataset publicly
available for further studies on how to improve SSL strategies.",0
CANUCS: An Updated Mass and Magnification Model of Abell 370 with JWST,2403.07062v1,http://arxiv.org/abs/2403.07062v1,2024-03-11 18:00:12+00:00,"We report an updated mass and magnification model of galaxy cluster Abell 370
using new NIRCam and NIRISS data from the CAnadian NIRISS Unbiased Cluster
Survey (CANUCS). Using Lenstool and a combination of archival HST and MUSE data
with new JWST data as constraints, we derive an improved gravitational lensing
model and extract magnifications of background galaxies with uncertainties.
Using our best fit model, we perform a search for new multiply imaged systems
via predicted positions. We report no new multiply imaged systems with
identifiable redshifts, likely due to already very deep HST and Spitzer data,
but confirm a $z\sim8$ multiply imaged system by measuring its redshift with
NIRISS and NIRSpec spectra. We find that the overall shape of the critical
curve for a source at $z = 9.0$ is similar to previous models of Abell 370,
with small changes. We investigate the $z\sim8$ galaxy with two images
observable with an apparent magnitude in the F125W band of $26.0\pm0.2$ and
$25.6\pm0.1$. After correcting for the magnifications of the images,
7.2$^{+0.2}_{-1.2}$ and 8.7$^{+0.4}_{-0.4}$, we use SED fitting to find an
intrinsic stellar mass of log($M^*/M_{\odot})$ = 7.35$^{+0.04}_{-0.05}$,
intrinsic SFR of 3.5$^{+2.2}_{-1.4}$ M$_{\odot}$/yr, and $M_{UV}$ of
-21.3$^{+0.2}_{-0.2}$, which is close to the knee of the luminosity function at
that redshift. Our model, and corresponding magnification, shear, and
convergence maps are available on request and will be made publicly available
on MAST in a CANUCS data release (DOI: 10.17909/ph4n-6n76).",0
Better than classical? The subtle art of benchmarking quantum machine learning models,2403.07059v2,http://arxiv.org/abs/2403.07059v2,2024-03-11 18:00:06+00:00,"Benchmarking models via classical simulations is one of the main ways to
judge ideas in quantum machine learning before noise-free hardware is
available. However, the huge impact of the experimental design on the results,
the small scales within reach today, as well as narratives influenced by the
commercialisation of quantum technologies make it difficult to gain robust
insights. To facilitate better decision-making we develop an open-source
package based on the PennyLane software framework and use it to conduct a
large-scale study that systematically tests 12 popular quantum machine learning
models on 6 binary classification tasks used to create 160 individual datasets.
We find that overall, out-of-the-box classical machine learning models
outperform the quantum classifiers. Moreover, removing entanglement from a
quantum model often results in as good or better performance, suggesting that
""quantumness"" may not be the crucial ingredient for the small learning tasks
considered here. Our benchmarks also unlock investigations beyond simplistic
leaderboard comparisons, and we identify five important questions for quantum
model design that follow from our results.",0
Dark Matter-induced electron excitations in silicon and germanium with Deep Learning,2403.07053v1,http://arxiv.org/abs/2403.07053v1,2024-03-11 18:00:01+00:00,"We train a deep neural network (DNN) to output rates of dark matter (DM)
induced electron excitations in silicon and germanium detectors. Our DNN
provides a massive speedup of around $5$ orders of magnitude relative to
existing methods (i.e. QEdark-EFT), allowing for extensive parameter scans in
the event of an observed DM signal. The network is also lighter and simpler to
use than alternative computational frameworks based on a direct calculation of
the DM-induced excitation rate. The DNN can be downloaded
$\href{https://github.com/urdshals/DEDD}{\text{here}}$.",0
BrushNet: A Plug-and-Play Image Inpainting Model with Decomposed Dual-Branch Diffusion,2403.06976v1,http://arxiv.org/abs/2403.06976v1,2024-03-11 17:59:31+00:00,"Image inpainting, the process of restoring corrupted images, has seen
significant advancements with the advent of diffusion models (DMs). Despite
these advancements, current DM adaptations for inpainting, which involve
modifications to the sampling strategy or the development of
inpainting-specific DMs, frequently suffer from semantic inconsistencies and
reduced image quality. Addressing these challenges, our work introduces a novel
paradigm: the division of masked image features and noisy latent into separate
branches. This division dramatically diminishes the model's learning load,
facilitating a nuanced incorporation of essential masked image information in a
hierarchical fashion. Herein, we present BrushNet, a novel plug-and-play
dual-branch model engineered to embed pixel-level masked image features into
any pre-trained DM, guaranteeing coherent and enhanced image inpainting
outcomes. Additionally, we introduce BrushData and BrushBench to facilitate
segmentation-based inpainting training and performance assessment. Our
extensive experimental analysis demonstrates BrushNet's superior performance
over existing models across seven key metrics, including image quality, mask
region preservation, and textual coherence.",0
Memory-based Adapters for Online 3D Scene Perception,2403.06974v1,http://arxiv.org/abs/2403.06974v1,2024-03-11 17:57:41+00:00,"In this paper, we propose a new framework for online 3D scene perception.
Conventional 3D scene perception methods are offline, i.e., take an already
reconstructed 3D scene geometry as input, which is not applicable in robotic
applications where the input data is streaming RGB-D videos rather than a
complete 3D scene reconstructed from pre-collected RGB-D videos. To deal with
online 3D scene perception tasks where data collection and perception should be
performed simultaneously, the model should be able to process 3D scenes frame
by frame and make use of the temporal information. To this end, we propose an
adapter-based plug-and-play module for the backbone of 3D scene perception
model, which constructs memory to cache and aggregate the extracted RGB-D
features to empower offline models with temporal learning ability.
Specifically, we propose a queued memory mechanism to cache the supporting
point cloud and image features. Then we devise aggregation modules which
directly perform on the memory and pass temporal information to current frame.
We further propose 3D-to-2D adapter to enhance image features with strong
global context. Our adapters can be easily inserted into mainstream offline
architectures of different tasks and significantly boost their performance on
online tasks. Extensive experiments on ScanNet and SceneNN datasets demonstrate
our approach achieves leading performance on three 3D scene perception tasks
compared with state-of-the-art online methods by simply finetuning existing
offline models, without any model and task-specific designs.
\href{https://xuxw98.github.io/Online3D/}{Project page}.",0
Bayesian Diffusion Models for 3D Shape Reconstruction,2403.06973v1,http://arxiv.org/abs/2403.06973v1,2024-03-11 17:55:53+00:00,"We present Bayesian Diffusion Models (BDM), a prediction algorithm that
performs effective Bayesian inference by tightly coupling the top-down (prior)
information with the bottom-up (data-driven) procedure via joint diffusion
processes. We show the effectiveness of BDM on the 3D shape reconstruction
task. Compared to prototypical deep learning data-driven approaches trained on
paired (supervised) data-labels (e.g. image-point clouds) datasets, our BDM
brings in rich prior information from standalone labels (e.g. point clouds) to
improve the bottom-up 3D reconstruction. As opposed to the standard Bayesian
frameworks where explicit prior and likelihood are required for the inference,
BDM performs seamless information fusion via coupled diffusion processes with
learned gradient computation networks. The specialty of our BDM lies in its
capability to engage the active and effective information exchange and fusion
of the top-down and bottom-up processes where each itself is a diffusion
process. We demonstrate state-of-the-art results on both synthetic and
real-world benchmarks for 3D shape reconstruction.",0
A representation-learning game for classes of prediction tasks,2403.06971v1,http://arxiv.org/abs/2403.06971v1,2024-03-11 17:54:42+00:00,"We propose a game-based formulation for learning dimensionality-reducing
representations of feature vectors, when only a prior knowledge on future
prediction tasks is available. In this game, the first player chooses a
representation, and then the second player adversarially chooses a prediction
task from a given class, representing the prior knowledge. The first player
aims is to minimize, and the second player to maximize, the regret: The minimal
prediction loss using the representation, compared to the same loss using the
original features. For the canonical setting in which the representation, the
response to predict and the predictors are all linear functions, and under the
mean squared error loss function, we derive the theoretically optimal
representation in pure strategies, which shows the effectiveness of the prior
knowledge, and the optimal regret in mixed strategies, which shows the
usefulness of randomizing the representation. For general representations and
loss functions, we propose an efficient algorithm to optimize a randomized
representation. The algorithm only requires the gradients of the loss function,
and is based on incrementally adding a representation rule to a mixture of such
rules.",0
D$^2$-JSCC: Digital Deep Joint Source-channel Coding for Semantic Communications,2403.07338v3,http://arxiv.org/abs/2403.07338v3,2024-03-12 05:43:16+00:00,"Semantic communications (SemCom) have emerged as a new paradigm for
supporting sixth-generation applications, where semantic features of data are
transmitted using artificial intelligence algorithms to attain high
communication efficiencies. Most existing SemCom techniques utilize deep neural
networks (DNNs) to implement analog source-channel mappings, which are
incompatible with existing digital communication architectures. To address this
issue, this paper proposes a novel framework of digital deep joint
source-channel coding (D$^2$-JSCC) targeting image transmission in SemCom. The
framework features digital source and channel codings that are jointly
optimized to reduce the end-to-end (E2E) distortion. First, deep source coding
with an adaptive density model is designed to encode semantic features
according to their distributions. Second, digital channel coding is employed to
protect encoded features against channel distortion. To facilitate their joint
design, the E2E distortion is characterized as a function of the source and
channel rates via the analysis of the Bayesian model and Lipschitz assumption
on the DNNs. Then to minimize the E2E distortion, a two-step algorithm is
proposed to control the source-channel rates for a given channel
signal-to-noise ratio. Simulation results reveal that the proposed framework
outperforms classic deep JSCC and mitigates the cliff and leveling-off effects,
which commonly exist for separation-based approaches.",0
IM-Unpack: Training and Inference with Arbitrarily Low Precision Integers,2403.07339v1,http://arxiv.org/abs/2403.07339v1,2024-03-12 05:44:27+00:00,"GEneral Matrix Multiply (GEMM) is a central operation in deep learning and
corresponds to the largest chunk of the compute footprint. Therefore, improving
its efficiency is an active topic of ongoing research. A popular strategy is
the use of low bit-width integers to approximate the original entries in a
matrix. This allows efficiency gains, but often requires sophisticated
techniques to control the rounding error incurred. In this work, we first
verify/check that when the low bit-width restriction is removed, for a variety
of Transformer-based models, whether integers are sufficient for all GEMMs need
-- for {\em both} training and inference stages, and can achieve parity with
floating point counterparts. No sophisticated techniques are needed. We find
that while a large majority of entries in matrices (encountered in such models)
can be easily represented by {\em low} bit-width integers, the existence of a
few heavy hitter entries make it difficult to achieve efficiency gains via the
exclusive use of low bit-width GEMMs alone. To address this issue, we develop a
simple algorithm, Integer Matrix Unpacking (IM-Unpack), to {\em unpack} a
matrix with large integer entries into a larger matrix whose entries all lie
within the representable range of arbitrarily low bit-width integers. This
allows {\em equivalence} with the original GEMM, i.e., the exact result can be
obtained using purely low bit-width integer GEMMs. This comes at the cost of
additional operations -- we show that for many popular models, this overhead is
quite small.",0
Rethinking ASTE: A Minimalist Tagging Scheme Alongside Contrastive Learning,2403.07342v1,http://arxiv.org/abs/2403.07342v1,2024-03-12 06:01:04+00:00,"Aspect Sentiment Triplet Extraction (ASTE) is a burgeoning subtask of
fine-grained sentiment analysis, aiming to extract structured sentiment
triplets from unstructured textual data. Existing approaches to ASTE often
complicate the task with additional structures or external data. In this
research, we propose a novel tagging scheme and employ a contrastive learning
approach to mitigate these challenges. The proposed approach demonstrates
comparable or superior performance in comparison to state-of-the-art
techniques, while featuring a more compact design and reduced computational
overhead. Notably, even in the era of Large Language Models (LLMs), our method
exhibits superior efficacy compared to GPT 3.5 and GPT 4 in a few-shot learning
scenarios. This study also provides valuable insights for the advancement of
ASTE techniques within the paradigm of large language models.",0
Conditional computation in neural networks: principles and research trends,2403.07965v1,http://arxiv.org/abs/2403.07965v1,2024-03-12 11:56:38+00:00,"This article summarizes principles and ideas from the emerging area of
applying \textit{conditional computation} methods to the design of neural
networks. In particular, we focus on neural networks that can dynamically
activate or de-activate parts of their computational graph conditionally on
their input. Examples include the dynamic selection of, e.g., input tokens,
layers (or sets of layers), and sub-modules inside each layer (e.g., channels
in a convolutional filter). We first provide a general formalism to describe
these techniques in an uniform way. Then, we introduce three notable
implementations of these principles: mixture-of-experts (MoEs) networks, token
selection mechanisms, and early-exit neural networks. The paper aims to provide
a tutorial-like introduction to this growing field. To this end, we analyze the
benefits of these modular designs in terms of efficiency, explainability, and
transfer learning, with a focus on emerging applicative areas ranging from
automated scientific discovery to semantic communication.",0
Unified Source-Free Domain Adaptation,2403.07601v1,http://arxiv.org/abs/2403.07601v1,2024-03-12 12:40:08+00:00,"In the pursuit of transferring a source model to a target domain without
access to the source training data, Source-Free Domain Adaptation (SFDA) has
been extensively explored across various scenarios, including closed-set,
open-set, partial-set, and generalized settings. Existing methods, focusing on
specific scenarios, not only address only a subset of challenges but also
necessitate prior knowledge of the target domain, significantly limiting their
practical utility and deployability. In light of these considerations, we
introduce a more practical yet challenging problem, termed unified SFDA, which
comprehensively incorporates all specific scenarios in a unified manner. To
tackle this unified SFDA problem, we propose a novel approach called Latent
Causal Factors Discovery (LCFD). In contrast to previous alternatives that
emphasize learning the statistical description of reality, we formulate LCFD
from a causality perspective. The objective is to uncover the causal
relationships between latent variables and model decisions, enhancing the
reliability and robustness of the learned model against domain shifts. To
integrate extensive world knowledge, we leverage a pre-trained vision-language
model such as CLIP. This aids in the formation and discovery of latent causal
factors in the absence of supervision in the variation of distribution and
semantics, coupled with a newly designed information bottleneck with
theoretical guarantees. Extensive experiments demonstrate that LCFD can achieve
new state-of-the-art results in distinct SFDA settings, as well as source-free
out-of-distribution generalization.Our code and data are available at
https://github.com/tntek/source-free-domain-adaptation.",0
Scale-free identity: The emergence of social network science,2403.07595v1,http://arxiv.org/abs/2403.07595v1,2024-03-12 12:28:34+00:00,"Social Network Analysis is a way of studying agents embedded in contexts. In
about 1998, physicists discovered social networks as representations of complex
systems. Small-world and scale-free networks are the paradigmatic models of
this Network Science. Relying on various models and mechanisms of
socio-cultural processes, an identity model is developed and calibrated in a
case study of Social Network Science. This research domain results from the
union of Social Network Analysis and Network Science. A unique dataset of
25,760 scholarly articles from one century of research (1916-2012) is created.
Clustering this set of publications, five subdomains are detected and analyzed
in terms of authorship, citation, and word usage structures and dynamics. The
scaling hypothesis of percolation theory is formulated for socio-cultural
systems, namely that power-law size distributions like Lotka's, Bradford's, and
Zipf's Law mean that the described identity resides at the phase transition
between the stability and change of meaning. In this case, it can be diagnosed
using bivariate scaling laws and Abbott's heuristic of fractal distinctions.
Identities are not dichotomies but dualities of social network and cultural
domain, micro and macro phenomena, as well as stability and change. Story sets
that give direction to research fluctuate less, are less distinctive, and more
inert than the individuals doing the research. Identities are scale-free. Six
senses are diagnostic of different aspects of identity, and when they come
together as process, a complex socio-cultural system comes into existence. A
mutual benefit that results from mating Relational Sociology and Network
Science is identified. The latter can learn from the former that social systems
are dualities of transactions and meaning. For the social sciences, the
importance of Paretian thinking (scale invariance) is pointed out.",0
Accurate Spatial Gene Expression Prediction by integrating Multi-resolution features,2403.07592v1,http://arxiv.org/abs/2403.07592v1,2024-03-12 12:25:38+00:00,"Recent advancements in Spatial Transcriptomics (ST) technology have
facilitated detailed gene expression analysis within tissue contexts. However,
the high costs and methodological limitations of ST necessitate a more robust
predictive model. In response, this paper introduces TRIPLEX, a novel deep
learning framework designed to predict spatial gene expression from Whole Slide
Images (WSIs). TRIPLEX uniquely harnesses multi-resolution features, capturing
cellular morphology at individual spots, the local context around these spots,
and the global tissue organization. By integrating these features through an
effective fusion strategy, TRIPLEX achieves accurate gene expression
prediction. Our comprehensive benchmark study, conducted on three public ST
datasets and supplemented with Visium data from 10X Genomics, demonstrates that
TRIPLEX outperforms current state-of-the-art models in Mean Squared Error
(MSE), Mean Absolute Error (MAE), and Pearson Correlation Coefficient (PCC).
The model's predictions align closely with ground truth gene expression
profiles and tumor annotations, underscoring TRIPLEX's potential in advancing
cancer diagnosis and treatment.",0
Robustifying and Boosting Training-Free Neural Architecture Search,2403.07591v1,http://arxiv.org/abs/2403.07591v1,2024-03-12 12:24:11+00:00,"Neural architecture search (NAS) has become a key component of AutoML and a
standard tool to automate the design of deep neural networks. Recently,
training-free NAS as an emerging paradigm has successfully reduced the search
costs of standard training-based NAS by estimating the true architecture
performance with only training-free metrics. Nevertheless, the estimation
ability of these metrics typically varies across different tasks, making it
challenging to achieve robust and consistently good search performance on
diverse tasks with only a single training-free metric. Meanwhile, the
estimation gap between training-free metrics and the true architecture
performances limits training-free NAS to achieve superior performance. To
address these challenges, we propose the robustifying and boosting
training-free NAS (RoBoT) algorithm which (a) employs the optimized combination
of existing training-free metrics explored from Bayesian optimization to
develop a robust and consistently better-performing metric on diverse tasks,
and (b) applies greedy search, i.e., the exploitation, on the newly developed
metric to bridge the aforementioned gap and consequently to boost the search
performance of standard training-free NAS further. Remarkably, the expected
performance of our RoBoT can be theoretically guaranteed, which improves over
the existing training-free NAS under mild conditions with additional
interesting insights. Our extensive experiments on various NAS benchmark tasks
yield substantial empirical evidence to support our theoretical results.",0
Visual Privacy Auditing with Diffusion Models,2403.07588v1,http://arxiv.org/abs/2403.07588v1,2024-03-12 12:18:55+00:00,"Image reconstruction attacks on machine learning models pose a significant
risk to privacy by potentially leaking sensitive information. Although
defending against such attacks using differential privacy (DP) has proven
effective, determining appropriate DP parameters remains challenging. Current
formal guarantees on data reconstruction success suffer from overly theoretical
assumptions regarding adversary knowledge about the target data, particularly
in the image domain. In this work, we empirically investigate this discrepancy
and find that the practicality of these assumptions strongly depends on the
domain shift between the data prior and the reconstruction target. We propose a
reconstruction attack based on diffusion models (DMs) that assumes adversary
access to real-world image priors and assess its implications on privacy
leakage under DP-SGD. We show that (1) real-world data priors significantly
influence reconstruction success, (2) current reconstruction bounds do not
model the risk posed by data priors well, and (3) DMs can serve as effective
auditing tools for visualizing privacy leakage.",0
Federated Learning of Socially Appropriate Agent Behaviours in Simulated Home Environments,2403.07586v1,http://arxiv.org/abs/2403.07586v1,2024-03-12 12:16:40+00:00,"As social robots become increasingly integrated into daily life, ensuring
their behaviours align with social norms is crucial. For their widespread
open-world application, it is important to explore Federated Learning (FL)
settings where individual robots can learn about their unique environments
while also learning from each others' experiences. In this paper, we present a
novel FL benchmark that evaluates different strategies, using multi-label
regression objectives, where each client individually learns to predict the
social appropriateness of different robot actions while also sharing their
learning with others. Furthermore, splitting the training data by different
contexts such that each client incrementally learns across contexts, we present
a novel Federated Continual Learning (FCL) benchmark that adapts FL-based
methods to use state-of-the-art Continual Learning (CL) methods to continually
learn socially appropriate agent behaviours under different contextual
settings. Federated Averaging (FedAvg) of weights emerges as a robust FL
strategy while rehearsal-based FCL enables incrementally learning the social
appropriateness of robot actions, across contextual splits.",0
"Communication Optimization for Distributed Training: Architecture, Advances, and Opportunities",2403.07585v1,http://arxiv.org/abs/2403.07585v1,2024-03-12 12:15:57+00:00,"The past few years have witnessed the flourishing of large-scale deep neural
network models with ever-growing parameter numbers. Training such large-scale
models typically requires massive memory and computing resources that exceed
those of a single GPU, necessitating distributed training. As GPU performance
has rapidly evolved in recent years, computation time has shrunk, thereby
increasing the proportion of communication in the overall training time.
Therefore, optimizing communication for distributed training has become an
urgent issue. In this article, we briefly introduce the general architecture of
distributed deep neural network training and analyze relationships among
Parallelization Strategy, Collective Communication Library, and Network from
the perspective of communication optimization, which forms a three-layer
paradigm. We then review current representative research advances with this
three-layer paradigm. We find that layers in the current three-layer paradigm
are relatively independent, but there is a rich design space for cross-layer
collaborative optimization in distributed training scenarios. Therefore, we
further advocate a communication-efficient five-layer paradigm underlining
opportunities for collaboration designs and look forward to the perspectives of
""Vertical"", ""Horizontal"", ""Intra-Inter"" and ""Host-Net"" collaboration designs.
We hope this article can shed some light on future research on communication
optimization for distributed training.",0
Molecularity: a fast and efficient criterion for probing superconductivity,2403.07584v1,http://arxiv.org/abs/2403.07584v1,2024-03-12 12:15:45+00:00,"We present an efficient criterion for probing the critical temperature of
hydrogen based superconductors. We start by expanding the applicability of 3D
descriptors of electron localization to superconducting states within the
framework of superconducting DFT. We first apply this descriptor to a model
system, the hydrogen chain, which allows to prove two main concepts: i) that
the electron localization changes very little when the transition from the
normal to the superconducting state takes place, i.e. that it can be described
at the DFT level from the normal state; and ii) that the formation of molecules
can be characterized within this theoretical framework, enabling to filter out
systems with marked molecular character and hence with low potential to be good
superconductors. These two ideas, are then exploited in real binary and ternary
systems, showing i) that the bonding type can be characterized automatically;
and ii) that this provides a new index which enables to feed machine learning
algorithms for a better prediction of critical temperatures. Overall, this sets
a grounded theoretical scenario for an automatic and efficient high-throughput
of potential hydrogen based superconductors.",0
LLMvsSmall Model? Large Language Model Based Text Augmentation Enhanced Personality Detection Model,2403.07581v1,http://arxiv.org/abs/2403.07581v1,2024-03-12 12:10:18+00:00,"Personality detection aims to detect one's personality traits underlying in
social media posts. One challenge of this task is the scarcity of ground-truth
personality traits which are collected from self-report questionnaires. Most
existing methods learn post features directly by fine-tuning the pre-trained
language models under the supervision of limited personality labels. This leads
to inferior quality of post features and consequently affects the performance.
In addition, they treat personality traits as one-hot classification labels,
overlooking the semantic information within them. In this paper, we propose a
large language model (LLM) based text augmentation enhanced personality
detection model, which distills the LLM's knowledge to enhance the small model
for personality detection, even when the LLM fails in this task. Specifically,
we enable LLM to generate post analyses (augmentations) from the aspects of
semantic, sentiment, and linguistic, which are critical for personality
detection. By using contrastive learning to pull them together in the embedding
space, the post encoder can better capture the psycho-linguistic information
within the post representations, thus improving personality detection.
Furthermore, we utilize the LLM to enrich the information of personality labels
for enhancing the detection performance. Experimental results on the benchmark
datasets demonstrate that our model outperforms the state-of-the-art methods on
personality detection.",0
AACP: Aesthetics assessment of children's paintings based on self-supervised learning,2403.07578v1,http://arxiv.org/abs/2403.07578v1,2024-03-12 12:07:00+00:00,"The Aesthetics Assessment of Children's Paintings (AACP) is an important
branch of the image aesthetics assessment (IAA), playing a significant role in
children's education. This task presents unique challenges, such as limited
available data and the requirement for evaluation metrics from multiple
perspectives. However, previous approaches have relied on training large
datasets and subsequently providing an aesthetics score to the image, which is
not applicable to AACP. To solve this problem, we construct an aesthetics
assessment dataset of children's paintings and a model based on self-supervised
learning. 1) We build a novel dataset composed of two parts: the first part
contains more than 20k unlabeled images of children's paintings; the second
part contains 1.2k images of children's paintings, and each image contains
eight attributes labeled by multiple design experts. 2) We design a pipeline
that includes a feature extraction module, perception modules and a
disentangled evaluation module. 3) We conduct both qualitative and quantitative
experiments to compare our model's performance with five other methods using
the AACP dataset. Our experiments reveal that our method can accurately capture
aesthetic features and achieve state-of-the-art performance.",0
Towards a Dynamic Future with Adaptable Computing and Network Convergence (ACNC),2403.07573v1,http://arxiv.org/abs/2403.07573v1,2024-03-12 12:03:16+00:00,"In the context of advancing 6G, a substantial paradigm shift is anticipated,
highlighting comprehensive everything-to-everything interactions characterized
by numerous connections and stringent adherence to Quality of
Service/Experience (QoS/E) prerequisites. The imminent challenge stems from
resource scarcity, prompting a deliberate transition to Computing-Network
Convergence (CNC) as an auspicious approach for joint resource orchestration.
While CNC-based mechanisms have garnered attention, their effectiveness in
realizing future services, particularly in use cases like the Metaverse, may
encounter limitations due to the continually changing nature of users,
services, and resources. Hence, this paper presents the concept of Adaptable
CNC (ACNC) as an autonomous Machine Learning (ML)-aided mechanism crafted for
the joint orchestration of computing and network resources, catering to dynamic
and voluminous user requests with stringent requirements. ACNC encompasses two
primary functionalities: state recognition and context detection. Given the
intricate nature of the user-service-computing-network space, the paper employs
dimension reduction to generate live, holistic, abstract system states in a
hierarchical structure. To address the challenges posed by dynamic changes,
Continual Learning (CL) is employed, classifying the system state into contexts
controlled by dedicated ML agents, enabling them to operate efficiently. These
two functionalities are intricately linked within a closed loop overseen by the
End-to-End (E2E) orchestrator to allocate resources. The paper introduces the
components of ACNC, proposes a Metaverse scenario to exemplify ACNC's role in
resource provisioning with Segment Routing v6 (SRv6), outlines ACNC's workflow,
details a numerical analysis for efficiency assessment, and concludes with
discussions on relevant challenges and potential avenues for future research.",0
Proactive Recommendation with Iterative Preference Guidance,2403.07571v1,http://arxiv.org/abs/2403.07571v1,2024-03-12 11:58:50+00:00,"Recommender systems mainly tailor personalized recommendations according to
user interests learned from user feedback. However, such recommender systems
passively cater to user interests and even reinforce existing interests in the
feedback loop, leading to problems like filter bubbles and opinion
polarization. To counteract this, proactive recommendation actively steers
users towards developing new interests in a target item or topic by
strategically modulating recommendation sequences. Existing work for proactive
recommendation faces significant hurdles: 1) overlooking the user feedback in
the guidance process; 2) lacking explicit modeling of the guiding objective;
and 3) insufficient flexibility for integration into existing industrial
recommender systems. To address these issues, we introduce an Iterative
Preference Guidance (IPG) framework. IPG performs proactive recommendation in a
flexible post-processing manner by ranking items according to their IPG scores
that consider both interaction probability and guiding value. These scores are
explicitly estimated with iteratively updated user representation that
considers the most recent user interactions. Extensive experiments validate
that IPG can effectively guide user interests toward target interests with a
reasonable trade-off in recommender accuracy. The code is available at
https://github.com/GabyUSTC/IPG-Rec.",0
Exploring Challenges in Deep Learning of Single-Station Ground Motion Records,2403.07569v1,http://arxiv.org/abs/2403.07569v1,2024-03-12 11:56:50+00:00,"Contemporary deep learning models have demonstrated promising results across
various applications within seismology and earthquake engineering. These models
rely primarily on utilizing ground motion records for tasks such as earthquake
event classification, localization, earthquake early warning systems, and
structural health monitoring. However, the extent to which these models
effectively learn from these complex time-series signals has not been
thoroughly analyzed. In this study, our objective is to evaluate the degree to
which auxiliary information, such as seismic phase arrival times or seismic
station distribution within a network, dominates the process of deep learning
from ground motion records, potentially hindering its effectiveness. We perform
a hyperparameter search on two deep learning models to assess their
effectiveness in deep learning from ground motion records while also examining
the impact of auxiliary information on model performance. Experimental results
reveal a strong reliance on the highly correlated P and S phase arrival
information. Our observations highlight a potential gap in the field,
indicating an absence of robust methodologies for deep learning of
single-station ground motion recordings independent of any auxiliary
information.",0
Triples-to-isiXhosa (T2X): Addressing the Challenges of Low-Resource Agglutinative Data-to-Text Generation,2403.07567v1,http://arxiv.org/abs/2403.07567v1,2024-03-12 11:53:27+00:00,"Most data-to-text datasets are for English, so the difficulties of modelling
data-to-text for low-resource languages are largely unexplored. In this paper
we tackle data-to-text for isiXhosa, which is low-resource and agglutinative.
We introduce Triples-to-isiXhosa (T2X), a new dataset based on a subset of
WebNLG, which presents a new linguistic context that shifts modelling demands
to subword-driven techniques. We also develop an evaluation framework for T2X
that measures how accurately generated text describes the data. This enables
future users of T2X to go beyond surface-level metrics in evaluation. On the
modelling side we explore two classes of methods - dedicated data-to-text
models trained from scratch and pretrained language models (PLMs). We propose a
new dedicated architecture aimed at agglutinative data-to-text, the Subword
Segmental Pointer Generator (SSPG). It jointly learns to segment words and copy
entities, and outperforms existing dedicated models for 2 agglutinative
languages (isiXhosa and Finnish). We investigate pretrained solutions for T2X,
which reveals that standard PLMs come up short. Fine-tuning machine translation
models emerges as the best method overall. These findings underscore the
distinct challenge presented by T2X: neither well-established data-to-text
architectures nor customary pretrained methodologies prove optimal. We conclude
with a qualitative analysis of generation errors and an ablation study.",0
Frequency Decoupling for Motion Magnification via Multi-Level Isomorphic Architecture,2403.07347v1,http://arxiv.org/abs/2403.07347v1,2024-03-12 06:07:29+00:00,"Video Motion Magnification (VMM) aims to reveal subtle and imperceptible
motion information of objects in the macroscopic world. Prior methods directly
model the motion field from the Eulerian perspective by Representation Learning
that separates shape and texture or Multi-domain Learning from phase
fluctuations. Inspired by the frequency spectrum, we observe that the
low-frequency components with stable energy always possess spatial structure
and less noise, making them suitable for modeling the subtle motion field. To
this end, we present FD4MM, a new paradigm of Frequency Decoupling for Motion
Magnification with a Multi-level Isomorphic Architecture to capture multi-level
high-frequency details and a stable low-frequency structure (motion field) in
video space. Since high-frequency details and subtle motions are susceptible to
information degradation due to their inherent subtlety and unavoidable external
interference from noise, we carefully design Sparse High/Low-pass Filters to
enhance the integrity of details and motion structures, and a Sparse Frequency
Mixer to promote seamless recoupling. Besides, we innovatively design a
contrastive regularization for this task to strengthen the model's ability to
discriminate irrelevant features, reducing undesired motion magnification.
Extensive experiments on both Real-world and Synthetic Datasets show that our
FD4MM outperforms SOTA methods. Meanwhile, FD4MM reduces FLOPs by 1.63$\times$
and boosts inference speed by 1.68$\times$ than the latest method. Our code is
available at https://github.com/Jiafei127/FD4MM.",0
An Improved Strategy for Blood Glucose Control Using Multi-Step Deep Reinforcement Learning,2403.07566v1,http://arxiv.org/abs/2403.07566v1,2024-03-12 11:53:00+00:00,"Blood Glucose (BG) control involves keeping an individual's BG within a
healthy range through extracorporeal insulin injections is an important task
for people with type 1 diabetes. However,traditional patient self-management is
cumbersome and risky. Recent research has been devoted to exploring
individualized and automated BG control approaches, among which Deep
Reinforcement Learning (DRL) shows potential as an emerging approach. In this
paper, we use an exponential decay model of drug concentration to convert the
formalization of the BG control problem, which takes into account the delay and
prolongedness of drug effects, from a PAE-POMDP (Prolonged Action
Effect-Partially Observable Markov Decision Process) to a MDP, and we propose a
novel multi-step DRL-based algorithm to solve the problem. The Prioritized
Experience Replay (PER) sampling method is also used in it. Compared to
single-step bootstrapped updates, multi-step learning is more efficient and
reduces the influence from biasing targets. Our proposed method converges
faster and achieves higher cumulative rewards compared to the benchmark in the
same training environment, and improves the time-in-range (TIR), the percentage
of time the patient's BG is within the target range, in the evaluation phase.
Our work validates the effectiveness of multi-step reinforcement learning in BG
control, which may help to explore the optimal glycemic control measure and
improve the survival of diabetic patients.",0
Learning Generalizable Feature Fields for Mobile Manipulation,2403.07563v1,http://arxiv.org/abs/2403.07563v1,2024-03-12 11:51:55+00:00,"An open problem in mobile manipulation is how to represent objects and scenes
in a unified manner, so that robots can use it both for navigating in the
environment and manipulating objects. The latter requires capturing intricate
geometry while understanding fine-grained semantics, whereas the former
involves capturing the complexity inherit to an expansive physical scale. In
this work, we present GeFF (Generalizable Feature Fields), a scene-level
generalizable neural feature field that acts as a unified representation for
both navigation and manipulation that performs in real-time. To do so, we treat
generative novel view synthesis as a pre-training task, and then align the
resulting rich scene priors with natural language via CLIP feature
distillation. We demonstrate the effectiveness of this approach by deploying
GeFF on a quadrupedal robot equipped with a manipulator. We evaluate GeFF's
ability to generalize to open-set objects as well as running time, when
performing open-vocabulary mobile manipulation in dynamic scenes.",0
A Flexible Cell Classification for ML Projects in Jupyter Notebooks,2403.07562v1,http://arxiv.org/abs/2403.07562v1,2024-03-12 11:50:47+00:00,"Jupyter Notebook is an interactive development environment commonly used for
rapid experimentation of machine learning (ML) solutions. Describing the ML
activities performed along code cells improves the readability and
understanding of Notebooks. Manual annotation of code cells is time-consuming
and error-prone. Therefore, tools have been developed that classify the cells
of a notebook concerning the ML activity performed in them. However, the
current tools are not flexible, as they work based on look-up tables that have
been created, which map function calls of commonly used ML libraries to ML
activities. These tables must be manually adjusted to account for new or
changed libraries.
  This paper presents a more flexible approach to cell classification based on
a hybrid classification approach that combines a rule-based and a decision tree
classifier. We discuss the design rationales and describe the developed
classifiers in detail. We implemented the new flexible cell classification
approach in a tool called JupyLabel. Its evaluation and the obtained metric
scores regarding precision, recall, and F1-score are discussed. Additionally,
we compared JupyLabel with HeaderGen, an existing cell classification tool. We
were able to show that the presented flexible cell classification approach
outperforms this tool significantly.",0
Unleashing Network Potentials for Semantic Scene Completion,2403.07560v2,http://arxiv.org/abs/2403.07560v2,2024-03-12 11:48:49+00:00,"Semantic scene completion (SSC) aims to predict complete 3D voxel occupancy
and semantics from a single-view RGB-D image, and recent SSC methods commonly
adopt multi-modal inputs. However, our investigation reveals two limitations:
ineffective feature learning from single modalities and overfitting to limited
datasets. To address these issues, this paper proposes a novel SSC framework -
Adversarial Modality Modulation Network (AMMNet) - with a fresh perspective of
optimizing gradient updates. The proposed AMMNet introduces two core modules: a
cross-modal modulation enabling the interdependence of gradient flows between
modalities, and a customized adversarial training scheme leveraging dynamic
gradient competition. Specifically, the cross-modal modulation adaptively
re-calibrates the features to better excite representation potentials from each
single modality. The adversarial training employs a minimax game of evolving
gradients, with customized guidance to strengthen the generator's perception of
visual fidelity from both geometric completeness and semantic correctness.
Extensive experimental results demonstrate that AMMNet outperforms
state-of-the-art SSC methods by a large margin, providing a promising direction
for improving the effectiveness and generalization of SSC methods.",0
Ensembling Prioritized Hybrid Policies for Multi-agent Pathfinding,2403.07559v1,http://arxiv.org/abs/2403.07559v1,2024-03-12 11:47:12+00:00,"Multi-Agent Reinforcement Learning (MARL) based Multi-Agent Path Finding
(MAPF) has recently gained attention due to its efficiency and scalability.
Several MARL-MAPF methods choose to use communication to enrich the information
one agent can perceive. However, existing works still struggle in structured
environments with high obstacle density and a high number of agents. To further
improve the performance of the communication-based MARL-MAPF solvers, we
propose a new method, Ensembling Prioritized Hybrid Policies (EPH). We first
propose a selective communication block to gather richer information for better
agent coordination within multi-agent environments and train the model with a
Q-learning-based algorithm. We further introduce three advanced inference
strategies aimed at bolstering performance during the execution phase. First,
we hybridize the neural policy with single-agent expert guidance for navigating
conflict-free zones. Secondly, we propose Q value-based methods for prioritized
resolution of conflicts as well as deadlock situations. Finally, we introduce a
robust ensemble method that can efficiently collect the best out of multiple
possible solutions. We empirically evaluate EPH in complex multi-agent
environments and demonstrate competitive performance against state-of-the-art
neural methods for MAPF.",0
SIFiD: Reassess Summary Factual Inconsistency Detection with LLM,2403.07557v1,http://arxiv.org/abs/2403.07557v1,2024-03-12 11:41:51+00:00,"Ensuring factual consistency between the summary and the original document is
paramount in summarization tasks. Consequently, considerable effort has been
dedicated to detecting inconsistencies. With the advent of Large Language
Models (LLMs), recent studies have begun to leverage their advanced language
understanding capabilities for inconsistency detection. However, early attempts
have shown that LLMs underperform traditional models due to their limited
ability to follow instructions and the absence of an effective detection
methodology. In this study, we reassess summary inconsistency detection with
LLMs, comparing the performances of GPT-3.5 and GPT-4. To advance research in
LLM-based inconsistency detection, we propose SIFiD (Summary Inconsistency
Detection with Filtered Document) that identify key sentences within documents
by either employing natural language inference or measuring semantic similarity
between summaries and documents.",0
An Adaptive Learning Approach to Multivariate Time Forecasting in Industrial Processes,2403.07554v1,http://arxiv.org/abs/2403.07554v1,2024-03-12 11:39:46+00:00,"Industrial processes generate a massive amount of monitoring data that can be
exploited to uncover hidden time losses in the system, leading to enhanced
accuracy of maintenance policies and, consequently, increasing the
effectiveness of the equipment. In this work, we propose a method for one-step
probabilistic multivariate forecasting of time variables based on a Hidden
Markov Model with covariates (IO-HMM). These covariates account for the
correlation of the predicted variables with their past values and additional
process measurements by means of a discrete model and a continuous model. The
probabilities of the former are updated using Bayesian principles, while the
parameter estimates for the latter are recursively computed through an adaptive
algorithm that also admits a Bayesian interpretation. This approach permits the
integration of new samples into the estimation of unknown parameters,
computationally improving the efficiency of the process. We evaluate the
performance of the method using a real data set obtained from a company of a
particular sector; however, it is a versatile technique applicable to any other
data set. The results show a consistent improvement over a persistence model,
which assumes that future values are the same as current values, and more
importantly, over univariate versions of our model.",0
Online Continual Learning For Interactive Instruction Following Agents,2403.07548v2,http://arxiv.org/abs/2403.07548v2,2024-03-12 11:33:48+00:00,"In learning an embodied agent executing daily tasks via language directives,
the literature largely assumes that the agent learns all training data at the
beginning. We argue that such a learning scenario is less realistic since a
robotic agent is supposed to learn the world continuously as it explores and
perceives it. To take a step towards a more realistic embodied agent learning
scenario, we propose two continual learning setups for embodied agents;
learning new behaviors (Behavior Incremental Learning, Behavior-IL) and new
environments (Environment Incremental Learning, Environment-IL) For the tasks,
previous 'data prior' based continual learning methods maintain logits for the
past tasks. However, the stored information is often insufficiently learned
information and requires task boundary information, which might not always be
available. Here, we propose to update them based on confidence scores without
task boundary information during training (i.e., task-free) in a moving average
fashion, named Confidence-Aware Moving Average (CAMA). In the proposed
Behavior-IL and Environment-IL setups, our simple CAMA outperforms prior state
of the art in our empirical validations by noticeable margins. The project page
including codes is https://github.com/snumprlab/cl-alfred.",0
A Survey of Vision Transformers in Autonomous Driving: Current Trends and Future Directions,2403.07542v1,http://arxiv.org/abs/2403.07542v1,2024-03-12 11:29:40+00:00,"This survey explores the adaptation of visual transformer models in
Autonomous Driving, a transition inspired by their success in Natural Language
Processing. Surpassing traditional Recurrent Neural Networks in tasks like
sequential image processing and outperforming Convolutional Neural Networks in
global context capture, as evidenced in complex scene recognition, Transformers
are gaining traction in computer vision. These capabilities are crucial in
Autonomous Driving for real-time, dynamic visual scene processing. Our survey
provides a comprehensive overview of Vision Transformer applications in
Autonomous Driving, focusing on foundational concepts such as self-attention,
multi-head attention, and encoder-decoder architecture. We cover applications
in object detection, segmentation, pedestrian detection, lane detection, and
more, comparing their architectural merits and limitations. The survey
concludes with future research directions, highlighting the growing role of
Vision Transformers in Autonomous Driving.",0
WannaLaugh: A Configurable Ransomware Emulator -- Learning to Mimic Malicious Storage Traces,2403.07540v1,http://arxiv.org/abs/2403.07540v1,2024-03-12 11:26:58+00:00,"Ransomware, a fearsome and rapidly evolving cybersecurity threat, continues
to inflict severe consequences on individuals and organizations worldwide.
Traditional detection methods, reliant on static signatures and application
behavioral patterns, are challenged by the dynamic nature of these threats.
This paper introduces three primary contributions to address this challenge.
First, we introduce a ransomware emulator. This tool is designed to safely
mimic ransomware attacks without causing actual harm or spreading malware,
making it a unique solution for studying ransomware behavior. Second, we
demonstrate how we use this emulator to create storage I/O traces. These traces
are then utilized to train machine-learning models. Our results show that these
models are effective in detecting ransomware, highlighting the practical
application of our emulator in developing responsible cybersecurity tools.
Third, we show how our emulator can be used to mimic the I/O behavior of
existing ransomware thereby enabling safe trace collection. Both the emulator
and its application represent significant steps forward in ransomware detection
in the era of machine-learning-driven cybersecurity.",0
LaB-GATr: geometric algebra transformers for large biomedical surface and volume meshes,2403.07536v1,http://arxiv.org/abs/2403.07536v1,2024-03-12 11:19:46+00:00,"Many anatomical structures can be described by surface or volume meshes.
Machine learning is a promising tool to extract information from these 3D
models. However, high-fidelity meshes often contain hundreds of thousands of
vertices, which creates unique challenges in building deep neural network
architectures. Furthermore, patient-specific meshes may not be canonically
aligned which limits the generalisation of machine learning algorithms. We
propose LaB-GATr, a transfomer neural network with geometric tokenisation that
can effectively learn with large-scale (bio-)medical surface and volume meshes
through sequence compression and interpolation. Our method extends the recently
proposed geometric algebra transformer (GATr) and thus respects all Euclidean
symmetries, i.e. rotation, translation and reflection, effectively mitigating
the problem of canonical alignment between patients. LaB-GATr achieves
state-of-the-art results on three tasks in cardiovascular hemodynamics
modelling and neurodevelopmental phenotype prediction, featuring meshes of up
to 200,000 vertices. Our results demonstrate that LaB-GATr is a powerful
architecture for learning with high-fidelity meshes which has the potential to
enable interesting downstream applications. Our implementation is publicly
available.",0
Physics-Transfer Learning for Material Strength Screening,2403.07526v1,http://arxiv.org/abs/2403.07526v1,2024-03-12 11:05:05+00:00,"The strength of materials, like many problems in the natural sciences, spans
multiple length and time scales, and the solution has to balance accuracy and
performance. Peierls stress is one of the central concepts in crystal
plasticity that measures the strength through the resistance of a dislocation
to plastic flow. The determination of Peierls stress involves a multiscale
nature depending on both elastic lattice responses and the energy landscape of
crystal slips. Material screening by strength via the Peierls stress from
first-principles calculations is computationally intractable for the nonlocal
characteristics of dislocations, and not included in the state-of-the-art
computational material databases. In this work, we propose a physics-transfer
framework to learn the physics of crystal plasticity from empirical atomistic
simulations and then predict the Peierls stress from chemically accurate
density functional theory-based calculations of material parameters. Notably,
the strengths of single-crystalline metals can be predicted from a few
single-point calculations for the deformed lattice and on the {\gamma} surface,
allowing efficient, high-throughput screening for material discovery.
Uncertainty quantification is carried out to assess the accuracy of models and
sources of errors, showing reduced physical and system uncertainties in the
predictions by elevating the fidelity of training models. This physics-transfer
framework can be generalized to other problems facing the accuracy-performance
dilemma, by harnessing the hierarchy of physics in the multiscale models of
materials science.",0
Universal Chemical Formula Dependence of $Ab$ $Initio$ Low-Energy Effective Hamiltonian in Single-Layer Carrier Doped Cuprate Superconductors -- Study by Hierarchical Dependence Extraction Algorithm,2403.07525v1,http://arxiv.org/abs/2403.07525v1,2024-03-12 11:03:25+00:00,"We explore the possibility to control the superconducting (SC) transition
temperature at optimal hole doping $T_{c}^{\rm opt}$ in cuprates by tuning the
chemical formula (CF). $T_{c}^{\rm opt}$ can be theoretically predicted from
the parameters of the \textit{ab initio} low-energy effective Hamiltonian (LEH)
with one antibonding (AB) Cu$3d_{x^2-y^2}$/O$2p_{\sigma}$ orbital per Cu atom
in the CuO$_2$ plane, notably the nearest neighbor hopping amplitude $|t_1|$
and the ratio $u=U/|t_1|$, where $U$ is the onsite effective Coulomb repulsion.
However, the CF dependence of $|t_1|$ and $u$ is a highly nontrivial question.
In this paper, we propose the universal dependence of $|t_1|$ and $u$ on the CF
and structural features in hole doped cuprates with a single CuO$_2$ layer
sandwiched between block layers. To do so, we perform extensive \textit{ab
initio} calculations of $|t_1|$ and $u$ and analyze the results by employing a
machine learning method called Hierarchical Dependence Extraction (HDE). The
main results are the following: (a) $|t_1|$ has a main-order dependence on the
radii $R_{\rm X}$ and $R_{\rm A}$ of the apical anion X and cation A in the
block layer. ($|t_1|$ increases when $R_{\rm X}$ or $R_{\rm A}$ decreases.) (b)
$u$ has a main-order dependence on the negative ionic charge $Z_{\rm X}$ of X
and the hole doping $\delta$ of the AB orbital. ($u$ decreases when $|Z_{\rm
X}|$ increases or $\delta$ increases.) We elucidate and discuss the microscopic
mechanism of (a,b). We demonstrate the predictive power of the HDE by showing
the consistency between (a,b) and results from previous works. The present
results provide a basis for optimizing SC properties in cuprates and possibly
akin materials. Also, the HDE method offers a general platform to identify
dependencies between physical quantities.",0
ProPML: Probability Partial Multi-label Learning,2403.07603v1,http://arxiv.org/abs/2403.07603v1,2024-03-12 12:40:23+00:00,"Partial Multi-label Learning (PML) is a type of weakly supervised learning
where each training instance corresponds to a set of candidate labels, among
which only some are true. In this paper, we introduce \our{}, a novel
probabilistic approach to this problem that extends the binary cross entropy to
the PML setup. In contrast to existing methods, it does not require suboptimal
disambiguation and, as such, can be applied to any deep architecture.
Furthermore, experiments conducted on artificial and real-world datasets
indicate that \our{} outperforms existing approaches, especially for high noise
in a candidate set.",0
Optimizing Negative Prompts for Enhanced Aesthetics and Fidelity in Text-To-Image Generation,2403.07605v1,http://arxiv.org/abs/2403.07605v1,2024-03-12 12:44:34+00:00,"In text-to-image generation, using negative prompts, which describe
undesirable image characteristics, can significantly boost image quality.
However, producing good negative prompts is manual and tedious. To address
this, we propose NegOpt, a novel method for optimizing negative prompt
generation toward enhanced image generation, using supervised fine-tuning and
reinforcement learning. Our combined approach results in a substantial increase
of 25% in Inception Score compared to other approaches and surpasses
ground-truth negative prompts from the test set. Furthermore, with NegOpt we
can preferentially optimize the metrics most important to us. Finally, we
construct Negative Prompts DB, a dataset of negative prompts.",0
Couler: Unified Machine Learning Workflow Optimization in Cloud,2403.07608v1,http://arxiv.org/abs/2403.07608v1,2024-03-12 12:47:32+00:00,"Machine Learning (ML) has become ubiquitous, fueling data-driven applications
across various organizations. Contrary to the traditional perception of ML in
research, ML workflows can be complex, resource-intensive, and time-consuming.
Expanding an ML workflow to encompass a wider range of data infrastructure and
data types may lead to larger workloads and increased deployment costs.
Currently, numerous workflow engines are available (with over ten being widely
recognized). This variety poses a challenge for end-users in terms of mastering
different engine APIs. While efforts have primarily focused on optimizing ML
Operations (MLOps) for a specific workflow engine, current methods largely
overlook workflow optimization across different engines.
  In this work, we design and implement Couler, a system designed for unified
ML workflow optimization in the cloud. Our main insight lies in the ability to
generate an ML workflow using natural language (NL) descriptions. We integrate
Large Language Models (LLMs) into workflow generation, and provide a unified
programming interface for various workflow engines. This approach alleviates
the need to understand various workflow engines' APIs. Moreover, Couler
enhances workflow computation efficiency by introducing automated caching at
multiple stages, enabling large workflow auto-parallelization and automatic
hyperparameters tuning. These enhancements minimize redundant computational
costs and improve fault tolerance during deep learning workflow training.
Couler is extensively deployed in real-world production scenarios at Ant Group,
handling approximately 22k workflows daily, and has successfully improved the
CPU/Memory utilization by more than 15% and the workflow completion rate by
around 17%.",0
Efficient Knowledge Deletion from Trained Models through Layer-wise Partial Machine Unlearning,2403.07611v1,http://arxiv.org/abs/2403.07611v1,2024-03-12 12:49:47+00:00,"Machine unlearning has garnered significant attention due to its ability to
selectively erase knowledge obtained from specific training data samples in an
already trained machine learning model. This capability enables data holders to
adhere strictly to data protection regulations. However, existing unlearning
techniques face practical constraints, often causing performance degradation,
demanding brief fine-tuning post unlearning, and requiring significant storage.
In response, this paper introduces a novel class of machine unlearning
algorithms. First method is partial amnesiac unlearning, integration of
layer-wise pruning with amnesiac unlearning. In this method, updates made to
the model during training are pruned and stored, subsequently used to forget
specific data from trained model. The second method assimilates layer-wise
partial-updates into label-flipping and optimization-based unlearning to
mitigate the adverse effects of data deletion on model efficacy. Through a
detailed experimental evaluation, we showcase the effectiveness of proposed
unlearning methods. Experimental results highlight that the partial amnesiac
unlearning not only preserves model efficacy but also eliminates the necessity
for brief post fine-tuning, unlike conventional amnesiac unlearning. Moreover,
employing layer-wise partial updates in label-flipping and optimization-based
unlearning techniques demonstrates superiority in preserving model efficacy
compared to their naive counterparts.",0
Symmetric Q-learning: Reducing Skewness of Bellman Error in Online Reinforcement Learning,2403.07704v1,http://arxiv.org/abs/2403.07704v1,2024-03-12 14:49:19+00:00,"In deep reinforcement learning, estimating the value function to evaluate the
quality of states and actions is essential. The value function is often trained
using the least squares method, which implicitly assumes a Gaussian error
distribution. However, a recent study suggested that the error distribution for
training the value function is often skewed because of the properties of the
Bellman operator, and violates the implicit assumption of normal error
distribution in the least squares method. To address this, we proposed a method
called Symmetric Q-learning, in which the synthetic noise generated from a
zero-mean distribution is added to the target values to generate a Gaussian
error distribution. We evaluated the proposed method on continuous control
benchmark tasks in MuJoCo. It improved the sample efficiency of a
state-of-the-art reinforcement learning method by reducing the skewness of the
error distribution.",0
CuVLER: Enhanced Unsupervised Object Discoveries through Exhaustive Self-Supervised Transformers,2403.07700v1,http://arxiv.org/abs/2403.07700v1,2024-03-12 14:46:03+00:00,"In this paper, we introduce VoteCut, an innovative method for unsupervised
object discovery that leverages feature representations from multiple
self-supervised models. VoteCut employs normalized-cut based graph
partitioning, clustering and a pixel voting approach. Additionally, We present
CuVLER (Cut-Vote-and-LEaRn), a zero-shot model, trained using pseudo-labels,
generated by VoteCut, and a novel soft target loss to refine segmentation
accuracy. Through rigorous evaluations across multiple datasets and several
unsupervised setups, our methods demonstrate significant improvements in
comparison to previous state-of-the-art models. Our ablation studies further
highlight the contributions of each component, revealing the robustness and
efficacy of our approach. Collectively, VoteCut and CuVLER pave the way for
future advancements in image segmentation.",0
Masked AutoDecoder is Effective Multi-Task Vision Generalist,2403.07692v1,http://arxiv.org/abs/2403.07692v1,2024-03-12 14:36:52+00:00,"Inspired by the success of general-purpose models in NLP, recent studies
attempt to unify different vision tasks in the same sequence format and employ
autoregressive Transformers for sequence prediction. They apply uni-directional
attention to capture sequential dependencies and generate task sequences
recursively. However, such autoregressive Transformers may not fit vision tasks
well, as vision task sequences usually lack the sequential dependencies
typically observed in natural languages. In this work, we design Masked
AutoDecoder~(MAD), an effective multi-task vision generalist. MAD consists of
two core designs. First, we develop a parallel decoding framework that
introduces bi-directional attention to capture contextual dependencies
comprehensively and decode vision task sequences in parallel. Second, we design
a masked sequence modeling approach that learns rich task contexts by masking
and reconstructing task sequences. In this way, MAD handles all the tasks by a
single network branch and a simple cross-entropy loss with minimal
task-specific designs. Extensive experiments demonstrate the great potential of
MAD as a new paradigm for unifying various vision tasks. MAD achieves superior
performance and inference efficiency compared to autoregressive counterparts
while obtaining competitive accuracy with task-specific models. Code will be
released.",0
SATDAUG -- A Balanced and Augmented Dataset for Detecting Self-Admitted Technical Debt,2403.07690v1,http://arxiv.org/abs/2403.07690v1,2024-03-12 14:33:53+00:00,"Self-admitted technical debt (SATD) refers to a form of technical debt in
which developers explicitly acknowledge and document the existence of technical
shortcuts, workarounds, or temporary solutions within the codebase. Over recent
years, researchers have manually labeled datasets derived from various software
development artifacts: source code comments, messages from the issue tracker
and pull request sections, and commit messages. These datasets are designed for
training, evaluation, performance validation, and improvement of machine
learning and deep learning models to accurately identify SATD instances.
However, class imbalance poses a serious challenge across all the existing
datasets, particularly when researchers are interested in categorizing the
specific types of SATD. In order to address the scarcity of labeled data for
SATD \textit{identification} (i.e., whether an instance is SATD or not) and
\textit{categorization} (i.e., which type of SATD is being classified) in
existing datasets, we share the \textit{SATDAUG} dataset, an augmented version
of existing SATD datasets, including source code comments, issue tracker, pull
requests, and commit messages. These augmented datasets have been balanced in
relation to the available artifacts and provide a much richer source of labeled
data for training machine learning or deep learning models.",0
Maxwell's Demon at Work: Efficient Pruning by Leveraging Saturation of Neurons,2403.07688v1,http://arxiv.org/abs/2403.07688v1,2024-03-12 14:28:06+00:00,"When training deep neural networks, the phenomenon of $\textit{dying
neurons}$ $\unicode{x2013}$units that become inactive or saturated, output zero
during training$\unicode{x2013}$ has traditionally been viewed as undesirable,
linked with optimization challenges, and contributing to plasticity loss in
continual learning scenarios. In this paper, we reassess this phenomenon,
focusing on sparsity and pruning. By systematically exploring the impact of
various hyperparameter configurations on dying neurons, we unveil their
potential to facilitate simple yet effective structured pruning algorithms. We
introduce $\textit{Demon Pruning}$ (DemP), a method that controls the
proliferation of dead neurons, dynamically leading to network sparsity.
Achieved through a combination of noise injection on active units and a
one-cycled schedule regularization strategy, DemP stands out for its simplicity
and broad applicability. Experiments on CIFAR10 and ImageNet datasets
demonstrate that DemP surpasses existing structured pruning techniques,
showcasing superior accuracy-sparsity tradeoffs and training speedups. These
findings suggest a novel perspective on dying neurons as a valuable resource
for efficient model compression and optimization.",0
Genuine Knowledge from Practice: Diffusion Test-Time Adaptation for Video Adverse Weather Removal,2403.07684v1,http://arxiv.org/abs/2403.07684v1,2024-03-12 14:21:30+00:00,"Real-world vision tasks frequently suffer from the appearance of unexpected
adverse weather conditions, including rain, haze, snow, and raindrops. In the
last decade, convolutional neural networks and vision transformers have yielded
outstanding results in single-weather video removal. However, due to the
absence of appropriate adaptation, most of them fail to generalize to other
weather conditions. Although ViWS-Net is proposed to remove adverse weather
conditions in videos with a single set of pre-trained weights, it is seriously
blinded by seen weather at train-time and degenerates when coming to unseen
weather during test-time. In this work, we introduce test-time adaptation into
adverse weather removal in videos, and propose the first framework that
integrates test-time adaptation into the iterative diffusion reverse process.
Specifically, we devise a diffusion-based network with a novel temporal noise
model to efficiently explore frame-correlated information in degraded video
clips at training stage. During inference stage, we introduce a proxy task
named Diffusion Tubelet Self-Calibration to learn the primer distribution of
test video stream and optimize the model by approximating the temporal noise
model for online adaptation. Experimental results, on benchmark datasets,
demonstrate that our Test-Time Adaptation method with Diffusion-based
network(Diff-TTA) outperforms state-of-the-art methods in terms of restoring
videos degraded by seen weather conditions. Its generalizable capability is
also validated with unseen weather conditions in both synthesized and
real-world videos.",0
MoralBERT: Detecting Moral Values in Social Discourse,2403.07678v1,http://arxiv.org/abs/2403.07678v1,2024-03-12 14:12:59+00:00,"Morality plays a fundamental role in how we perceive information while
greatly influencing our decisions and judgements. Controversial topics,
including vaccination, abortion, racism, and sexuality, often elicit opinions
and attitudes that are not solely based on evidence but rather reflect moral
worldviews. Recent advances in natural language processing have demonstrated
that moral values can be gauged in human-generated textual content. Here, we
design a range of language representation models fine-tuned to capture exactly
the moral nuances in text, called MoralBERT. We leverage annotated moral data
from three distinct sources: Twitter, Reddit, and Facebook user-generated
content covering various socially relevant topics. This approach broadens
linguistic diversity and potentially enhances the models' ability to comprehend
morality in various contexts. We also explore a domain adaptation technique and
compare it to the standard fine-tuned BERT model, using two different
frameworks for moral prediction: single-label and multi-label. We compare
in-domain approaches with conventional models relying on lexicon-based
techniques, as well as a Machine Learning classifier with Word2Vec
representation. Our results showed that in-domain prediction models
significantly outperformed traditional models. While the single-label setting
reaches a higher accuracy than previously achieved for the task when using BERT
pretrained models. Experiments in an out-of-domain setting, instead, suggest
that further work is needed for existing domain adaptation techniques to
generalise between different social media platforms, especially for the
multi-label task. The investigations and outcomes from this study pave the way
for further exploration, enabling a more profound comprehension of moral
narratives about controversial social issues.",0
Multichannel Long-Term Streaming Neural Speech Enhancement for Static and Moving Speakers,2403.07675v1,http://arxiv.org/abs/2403.07675v1,2024-03-12 14:11:29+00:00,"In this work, we extend our previously proposed offline SpatialNet for
long-term streaming multichannel speech enhancement in both static and moving
speaker scenarios. SpatialNet exploits spatial information, such as the
spatial/steering direction of speech, for discriminating between target speech
and interferences, and achieved outstanding performance. The core of SpatialNet
is a narrow-band self-attention module used for learning the temporal dynamic
of spatial vectors. Towards long-term streaming speech enhancement, we propose
to replace the offline self-attention network with online networks that have
linear inference complexity w.r.t signal length and meanwhile maintain the
capability of learning long-term information. Three variants are developed
based on (i) masked self-attention, (ii) Retention, a self-attention variant
with linear inference complexity, and (iii) Mamba, a
structured-state-space-based RNN-like network. Moreover, we investigate the
length extrapolation ability of different networks, namely test on signals that
are much longer than training signals, and propose a short-signal training plus
long-signal fine-tuning strategy, which largely improves the length
extrapolation ability of the networks within limited training time. Overall,
the proposed online SpatialNet achieves outstanding speech enhancement
performance for long audio streams, and for both static and moving speakers.
The proposed method will be open-sourced in
https://github.com/Audio-WestlakeU/NBSS.",0
Towards Model Extraction Attacks in GAN-Based Image Translation via Domain Shift Mitigation,2403.07673v2,http://arxiv.org/abs/2403.07673v2,2024-03-12 14:06:44+00:00,"Model extraction attacks (MEAs) enable an attacker to replicate the
functionality of a victim deep neural network (DNN) model by only querying its
API service remotely, posing a severe threat to the security and integrity of
pay-per-query DNN-based services. Although the majority of current research on
MEAs has primarily concentrated on neural classifiers, there is a growing
prevalence of image-to-image translation (I2IT) tasks in our everyday
activities. However, techniques developed for MEA of DNN classifiers cannot be
directly transferred to the case of I2IT, rendering the vulnerability of I2IT
models to MEA attacks often underestimated. This paper unveils the threat of
MEA in I2IT tasks from a new perspective. Diverging from the traditional
approach of bridging the distribution gap between attacker queries and victim
training samples, we opt to mitigate the effect caused by the different
distributions, known as the domain shift. This is achieved by introducing a new
regularization term that penalizes high-frequency noise, and seeking a flatter
minimum to avoid overfitting to the shifted distribution. Extensive experiments
on different image translation tasks, including image super-resolution and
style transfer, are performed on different backbone victim models, and the new
design consistently outperforms the baseline by a large margin across all
metrics. A few real-life I2IT APIs are also verified to be extremely vulnerable
to our attack, emphasizing the need for enhanced defenses and potentially
revised API publishing policies.",0
Machine Learning for Soccer Match Result Prediction,2403.07669v1,http://arxiv.org/abs/2403.07669v1,2024-03-12 14:00:50+00:00,"Machine learning has become a common approach to predicting the outcomes of
soccer matches, and the body of literature in this domain has grown
substantially in the past decade and a half. This chapter discusses available
datasets, the types of models and features, and ways of evaluating model
performance in this application domain. The aim of this chapter is to give a
broad overview of the current state and potential future developments in
machine learning for soccer match results prediction, as a resource for those
interested in conducting future studies in the area. Our main findings are that
while gradient-boosted tree models such as CatBoost, applied to soccer-specific
ratings such as pi-ratings, are currently the best-performing models on
datasets containing only goals as the match features, there needs to be a more
thorough comparison of the performance of deep learning models and Random
Forest on a range of datasets with different types of features. Furthermore,
new rating systems using both player- and team-level information and
incorporating additional information from, e.g., spatiotemporal tracking and
event data, could be investigated further. Finally, the interpretability of
match result prediction models needs to be enhanced for them to be more useful
for team management.",0
Do Deep Neural Network Solutions Form a Star Domain?,2403.07968v1,http://arxiv.org/abs/2403.07968v1,2024-03-12 13:59:23+00:00,"Entezari et al. (2022) conjectured that neural network solution sets
reachable via stochastic gradient descent (SGD) are convex, considering
permutation invariances. This means that two independent solutions can be
connected by a linear path with low loss, given one of them is appropriately
permuted. However, current methods to test this theory often fail to eliminate
loss barriers between two independent solutions (Ainsworth et al., 2022;
Benzing et al., 2022). In this work, we conjecture that a more relaxed claim
holds: the SGD solution set is a star domain that contains a star model that is
linearly connected to all the other solutions via paths with low loss values,
modulo permutations. We propose the Starlight algorithm that finds a star model
of a given learning task. We validate our claim by showing that this star model
is linearly connected with other independently found solutions. As an
additional benefit of our study, we demonstrate better uncertainty estimates on
Bayesian Model Averaging over the obtained star domain. Code is available at
https://github.com/aktsonthalia/starlight.",0
Optical computing with supercontinuum generation in photonic crystal fibers,2403.07667v1,http://arxiv.org/abs/2403.07667v1,2024-03-12 13:58:01+00:00,"We introduce a novel photonic neural network using photonic crystal fibers,
leveraging femtosecond pulse supercontinuum generation for optical computing.
Investigating its efficacy across machine learning tasks, we uncover the
crucial impact of nonlinear pulse propagation dynamics on network performance.
Our findings show that octave-spanning supercontinuum generation results in
loss of dataset variety due to many-to-one mapping, and optimal performance
requires balancing optical nonlinearity. This study offers guidance for
designing energy-efficient and high-performance photonic neural network
architectures by explaining the interplay between nonlinear dynamics and
optical computing.",0
Scalable Spatiotemporal Prediction with Bayesian Neural Fields,2403.07657v1,http://arxiv.org/abs/2403.07657v1,2024-03-12 13:47:50+00:00,"Spatiotemporal datasets, which consist of spatially-referenced time series,
are ubiquitous in many scientific and business-intelligence applications, such
as air pollution monitoring, disease tracking, and cloud-demand forecasting. As
modern datasets continue to increase in size and complexity, there is a growing
need for new statistical methods that are flexible enough to capture complex
spatiotemporal dynamics and scalable enough to handle large prediction
problems. This work presents the Bayesian Neural Field (BayesNF), a
domain-general statistical model for inferring rich probability distributions
over a spatiotemporal domain, which can be used for data-analysis tasks
including forecasting, interpolation, and variography. BayesNF integrates a
novel deep neural network architecture for high-capacity function estimation
with hierarchical Bayesian inference for robust uncertainty quantification. By
defining the prior through a sequence of smooth differentiable transforms,
posterior inference is conducted on large-scale data using variationally
learned surrogates trained via stochastic gradient descent. We evaluate BayesNF
against prominent statistical and machine-learning baselines, showing
considerable improvements on diverse prediction problems from climate and
public health datasets that contain tens to hundreds of thousands of
measurements. The paper is accompanied with an open-source software package
(https://github.com/google/bayesnf) that is easy-to-use and compatible with
modern GPU and TPU accelerators on the JAX machine learning platform.",0
Analyzing Adversarial Attacks on Sequence-to-Sequence Relevance Models,2403.07654v1,http://arxiv.org/abs/2403.07654v1,2024-03-12 13:45:20+00:00,"Modern sequence-to-sequence relevance models like monoT5 can effectively
capture complex textual interactions between queries and documents through
cross-encoding. However, the use of natural language tokens in prompts, such as
Query, Document, and Relevant for monoT5, opens an attack vector for malicious
documents to manipulate their relevance score through prompt injection, e.g.,
by adding target words such as true. Since such possibilities have not yet been
considered in retrieval evaluation, we analyze the impact of query-independent
prompt injection via manually constructed templates and LLM-based rewriting of
documents on several existing relevance models. Our experiments on the TREC
Deep Learning track show that adversarial documents can easily manipulate
different sequence-to-sequence relevance models, while BM25 (as a typical
lexical model) is not affected. Remarkably, the attacks also affect
encoder-only relevance models (which do not rely on natural language prompt
tokens), albeit to a lesser extent.",0
OmniMatch: Effective Self-Supervised Any-Join Discovery in Tabular Data Repositories,2403.07653v1,http://arxiv.org/abs/2403.07653v1,2024-03-12 13:42:49+00:00,"How can we discover join relationships among columns of tabular data in a
data repository? Can this be done effectively when metadata is missing?
Traditional column matching works mainly rely on similarity measures based on
exact value overlaps, hence missing important semantics or failing to handle
noise in the data. At the same time, recent dataset discovery methods focusing
on deep table representation learning techniques, do not take into
consideration the rich set of column similarity signals found in prior matching
and discovery methods. Finally, existing methods heavily depend on
user-provided similarity thresholds, hindering their deployability in
real-world settings. In this paper, we propose OmniMatch, a novel join
discovery technique that detects equi-joins and fuzzy-joins betwen columns by
combining column-pair similarity measures with Graph Neural Networks (GNNs).
OmniMatch's GNN can capture column relatedness leveraging graph transitivity,
significantly improving the recall of join discovery tasks. At the same time,
OmniMatch also increases the precision by augmenting its training data with
negative column join examples through an automated negative example generation
process. Most importantly, compared to the state-of-the-art matching and
discovery methods, OmniMatch exhibits up to 14% higher effectiveness in F1
score and AUC without relying on metadata or user-provided thresholds for each
similarity metric.",0
Harder Tasks Need More Experts: Dynamic Routing in MoE Models,2403.07652v1,http://arxiv.org/abs/2403.07652v1,2024-03-12 13:41:15+00:00,"In this paper, we introduce a novel dynamic expert selection framework for
Mixture of Experts (MoE) models, aiming to enhance computational efficiency and
model performance by adjusting the number of activated experts based on input
difficulty. Unlike traditional MoE approaches that rely on fixed Top-K routing,
which activates a predetermined number of experts regardless of the input's
complexity, our method dynamically selects experts based on the confidence
level in expert selection for each input. This allows for a more efficient
utilization of computational resources, activating more experts for complex
tasks requiring advanced reasoning and fewer for simpler tasks. Through
extensive evaluations, our dynamic routing method demonstrates substantial
improvements over conventional Top-2 routing across various benchmarks,
achieving an average improvement of 0.7% with less than 90% activated
parameters. Further analysis shows our model dispatches more experts to tasks
requiring complex reasoning skills, like BBH, confirming its ability to
dynamically allocate computational resources in alignment with the input's
complexity. Our findings also highlight a variation in the number of experts
needed across different layers of the transformer model, offering insights into
the potential for designing heterogeneous MoE frameworks. The code and models
are available at https://github.com/ZhenweiAn/Dynamic_MoE.",0
Characterization of Large Language Model Development in the Datacenter,2403.07648v1,http://arxiv.org/abs/2403.07648v1,2024-03-12 13:31:14+00:00,"Large Language Models (LLMs) have presented impressive performance across
several transformative tasks. However, it is non-trivial to efficiently utilize
large-scale cluster resources to develop LLMs, often riddled with numerous
challenges such as frequent hardware failures, intricate parallelization
strategies, and imbalanced resource utilization. In this paper, we present an
in-depth characterization study of a six-month LLM development workload trace
collected from our GPU datacenter Acme. Specifically, we investigate
discrepancies between LLMs and prior task-specific Deep Learning (DL)
workloads, explore resource utilization patterns, and identify the impact of
various job failures. Our analysis summarizes hurdles we encountered and
uncovers potential opportunities to optimize systems tailored for LLMs.
Furthermore, we introduce our system efforts: (1) fault-tolerant pretraining,
which enhances fault tolerance through LLM-involved failure diagnosis and
automatic recovery. (2) decoupled scheduling for evaluation, which achieves
timely performance feedback via trial decomposition and scheduling
optimization.",0
Feasibility of machine learning-based rice yield prediction in India at the district level using climate reanalysis data,2403.07967v1,http://arxiv.org/abs/2403.07967v1,2024-03-12 13:31:13+00:00,"Yield forecasting, the science of predicting agricultural productivity before
the crop harvest occurs, helps a wide range of stakeholders make better
decisions around agricultural planning. This study aims to investigate whether
machine learning-based yield prediction models can capably predict Kharif
season rice yields at the district level in India several months before the
rice harvest takes place. The methodology involved training 19 machine learning
models such as CatBoost, LightGBM, Orthogonal Matching Pursuit, and Extremely
Randomized Trees on 20 years of climate, satellite, and rice yield data across
247 of Indian rice-producing districts. In addition to model-building, a
dynamic dashboard was built understand how the reliability of rice yield
predictions varies across districts. The results of the proof-of-concept
machine learning pipeline demonstrated that rice yields can be predicted with a
reasonable degree of accuracy, with out-of-sample R2, MAE, and MAPE performance
of up to 0.82, 0.29, and 0.16 respectively. These results outperformed test set
performance reported in related literature on rice yield modeling in other
contexts and countries. In addition, SHAP value analysis was conducted to infer
both the importance and directional impact of the climate and remote sensing
variables included in the model. Important features driving rice yields
included temperature, soil water volume, and leaf area index. In particular,
higher temperatures in August correlate with increased rice yields,
particularly when the leaf area index in August is also high. Building on the
results, a proof-of-concept dashboard was developed to allow users to easily
explore which districts may experience a rise or fall in yield relative to the
previous year.",0
A Framework for Controlling Multiple Industrial Robots using Mobile Applications,2403.07639v1,http://arxiv.org/abs/2403.07639v1,2024-03-12 13:23:40+00:00,"Purpose: Over the last few decades, the development of the hardware and
software has enabled the application of advanced systems. In the robotics
field, the UI design is an intriguing area to be explored due to the creation
of devices with a wide range of functionalities in a reduced size. Moreover,
the idea of using the same UI to control several systems arouses a great
interest considering that this involves less learning effort and time for the
users. Therefore, this paper will present a mobile application to control two
industrial robots with four modes of operation. Design/methodology/approach:
The smartphone was selected to be the interface due to its wide range of
capabilities and the MIT Inventor App was used to create the application, whose
environment is supported by Android smartphones. For the validation, ROS was
used since it is a fundamental framework utilised in industrial robotics and
the Arduino Uno was used to establish the data transmission between the
smartphone and the board NVIDIA Jetson TX2. In MIT Inventor App, the graphical
interface was created to visualize the options available in the app whereas two
scripts in python were programmed to perform the simulations in ROS and carry
out the tests. Findings: The results indicated that the use of the sliders to
control the robots is more favourable than the Orientation Sensor due to the
sensibility of the sensor and human limitations to hold the smartphone
perfectly still. Another important finding was the limitations of the
autonomous mode, in which the robot grabs an object. In this case, the
configuration of the Kinect camera and the controllers has a significant impact
on the success of the simulation. Finally, it was observed that the delay was
appropriate despite the use of the Arduino UNO to transfer the data between the
Smartphone and the Nvidia Jetson TX2.",0
Online Adaptation of Sampling-Based Motion Planning with Inaccurate Models,2403.07638v1,http://arxiv.org/abs/2403.07638v1,2024-03-12 13:23:31+00:00,"Robotic manipulation relies on analytical or learned models to simulate the
system dynamics. These models are often inaccurate and based on offline
information, so that the robot planner is unable to cope with mismatches
between the expected and the actual behavior of the system (e.g., the presence
of an unexpected obstacle). In these situations, the robot should use
information gathered online to correct its planning strategy and adapt to the
actual system response. We propose a sampling-based motion planning approach
that uses an estimate of the model error and online observations to correct the
planning strategy at each new replanning. Our approach adapts the cost function
and the sampling bias of a kinodynamic motion planner when the outcome of the
executed transitions is different from the expected one (e.g., when the robot
unexpectedly collides with an obstacle) so that future trajectories will avoid
unreliable motions. To infer the properties of a new transition, we introduce
the notion of context-awareness, i.e., we store local environment information
for each executed transition and avoid new transitions with context similar to
previous unreliable ones. This is helpful for leveraging online information
even if the simulated transitions are far (in the state-and-action space) from
the executed ones. Simulation and experimental results show that the proposed
approach increases the success rate in execution and reduces the number of
replannings needed to reach the goal.",0
CardioGenAI: A Machine Learning-Based Framework for Re-Engineering Drugs for Reduced hERG Liability,2403.07632v1,http://arxiv.org/abs/2403.07632v1,2024-03-12 13:12:24+00:00,"Drug-induced cardiotoxicity is a major health concern which can lead to
serious adverse effects including life-threatening cardiac arrhythmias via the
blockade of the voltage-gated hERG potassium ion channel. It is therefore of
tremendous interest to develop advanced methods to identify hERG-active
compounds in early stages of drug development, as well as to optimize
commercially available drugs for reduced hERG activity. In this work, we
present CardioGenAI, a machine learning-based framework for re-engineering both
developmental and marketed drugs for reduced hERG activity while preserving
their pharmacological activity. The framework incorporates novel
state-of-the-art discriminative models for predicting hERG channel activity, as
well as activity against the voltage-gated NaV1.5 and CaV1.2 channels due to
their potential implications in modulating the arrhythmogenic potential induced
by hERG channel blockade. These models can also serve independently as
effective components of a virtual screening pipeline. We applied the complete
framework to pimozide, an FDA-approved antipsychotic agent that demonstrates
high affinity to the hERG channel, and generated 100 refined candidates.
Remarkably, among the candidates is fluspirilene, a compound which is of the
same class of drugs (diphenylmethanes) as pimozide and therefore has similar
pharmacological activity, yet exhibits over 700-fold weaker binding to hERG. We
have made all of our software open-source to facilitate integration of the
CardioGenAI framework for molecular hypothesis generation into drug discovery
workflows.",0
Hunting Attributes: Context Prototype-Aware Learning for Weakly Supervised Semantic Segmentation,2403.07630v1,http://arxiv.org/abs/2403.07630v1,2024-03-12 13:11:58+00:00,"Recent weakly supervised semantic segmentation (WSSS) methods strive to
incorporate contextual knowledge to improve the completeness of class
activation maps (CAM). In this work, we argue that the knowledge bias between
instances and contexts affects the capability of the prototype to sufficiently
understand instance semantics. Inspired by prototype learning theory, we
propose leveraging prototype awareness to capture diverse and fine-grained
feature attributes of instances. The hypothesis is that contextual prototypes
might erroneously activate similar and frequently co-occurring object
categories due to this knowledge bias. Therefore, we propose to enhance the
prototype representation ability by mitigating the bias to better capture
spatial coverage in semantic object regions. With this goal, we present a
Context Prototype-Aware Learning (CPAL) strategy, which leverages semantic
context to enrich instance comprehension. The core of this method is to
accurately capture intra-class variations in object features through
context-aware prototypes, facilitating the adaptation to the semantic
attributes of various instances. We design feature distribution alignment to
optimize prototype awareness, aligning instance feature distributions with
dense features. In addition, a unified training framework is proposed to
combine label-guided classification supervision and prototypes-guided
self-supervision. Experimental results on PASCAL VOC 2012 and MS COCO 2014 show
that CPAL significantly improves off-the-shelf methods and achieves
state-of-the-art performance. The project is available at
https://github.com/Barrett-python/CPAL.",0
generAItor: Tree-in-the-Loop Text Generation for Language Model Explainability and Adaptation,2403.07627v1,http://arxiv.org/abs/2403.07627v1,2024-03-12 13:09:15+00:00,"Large language models (LLMs) are widely deployed in various downstream tasks,
e.g., auto-completion, aided writing, or chat-based text generation. However,
the considered output candidates of the underlying search algorithm are
under-explored and under-explained. We tackle this shortcoming by proposing a
tree-in-the-loop approach, where a visual representation of the beam search
tree is the central component for analyzing, explaining, and adapting the
generated outputs. To support these tasks, we present generAItor, a visual
analytics technique, augmenting the central beam search tree with various
task-specific widgets, providing targeted visualizations and interaction
possibilities. Our approach allows interactions on multiple levels and offers
an iterative pipeline that encompasses generating, exploring, and comparing
output candidates, as well as fine-tuning the model based on adapted data. Our
case study shows that our tool generates new insights in gender bias analysis
beyond state-of-the-art template-based methods. Additionally, we demonstrate
the applicability of our approach in a qualitative user study. Finally, we
quantitatively evaluate the adaptability of the model to few samples, as
occurring in text-generation use cases.",0
Empowering Sequential Recommendation from Collaborative Signals and Semantic Relatedness,2403.07623v1,http://arxiv.org/abs/2403.07623v1,2024-03-12 13:06:31+00:00,"Sequential recommender systems (SRS) could capture dynamic user preferences
by modeling historical behaviors ordered in time. Despite effectiveness,
focusing only on the \textit{collaborative signals} from behaviors does not
fully grasp user interests. It is also significant to model the
\textit{semantic relatedness} reflected in content features, e.g., images and
text. Towards that end, in this paper, we aim to enhance the SRS tasks by
effectively unifying collaborative signals and semantic relatedness together.
Notably, we empirically point out that it is nontrivial to achieve such a goal
due to semantic gap issues. Thus, we propose an end-to-end two-stream
architecture for sequential recommendation, named TSSR, to learn user
preferences from ID-based and content-based sequence. Specifically, we first
present novel hierarchical contrasting module, including coarse user-grained
and fine item-grained terms, to align the representations of inter-modality.
Furthermore, we also design a two-stream architecture to learn the dependence
of intra-modality sequence and the complex interactions of inter-modality
sequence, which can yield more expressive capacity in understanding user
interests. We conduct extensive experiments on five public datasets. The
experimental results show that the TSSR could yield superior performance than
competitive baselines. We also make our experimental codes publicly available
at https://anonymous.4open.science/r/TSSR-2A27/.",0
Smartphone region-wise image indoor localization using deep learning for indoor tourist attraction,2403.07621v1,http://arxiv.org/abs/2403.07621v1,2024-03-12 13:04:37+00:00,"Smart indoor tourist attractions, such as smart museums and aquariums,
usually require a significant investment in indoor localization devices. The
smartphone Global Positional Systems use is unsuitable for scenarios where
dense materials such as concrete and metal block weaken the GPS signals, which
is the most common scenario in an indoor tourist attraction. Deep learning
makes it possible to perform region-wise indoor localization using smartphone
images. This approach does not require any investment in infrastructure,
reducing the cost and time to turn museums and aquariums into smart museums or
smart aquariums. This paper proposes using deep learning algorithms to classify
locations using smartphone camera images for indoor tourism attractions. We
evaluate our proposal in a real-world scenario in Brazil. We extensively
collect images from ten different smartphones to classify biome-themed fish
tanks inside the Pantanal Biopark, creating a new dataset of 3654 images. We
tested seven state-of-the-art neural networks, three being transformer-based,
achieving precision around 90% on average and recall and f-score around 89% on
average. The results indicate good feasibility of the proposal in a most indoor
tourist attractions.",0
Applying ranking techniques for estimating influence of Earth variables on temperature forecast error,2403.07966v1,http://arxiv.org/abs/2403.07966v1,2024-03-12 12:59:00+00:00,"This paper describes how to analyze the influence of Earth system variables
on the errors when providing temperature forecasts. The initial framework to
get the data has been based on previous research work, which resulted in a very
interesting discovery. However, the aforementioned study only worked on
individual correlations of the variables with respect to the error. This
research work is going to re-use the main ideas but introduce three main
novelties: (1) applying a data science approach by a few representative
locations; (2) taking advantage of the rankings created by Spearman correlation
but enriching them with other metrics looking for a more robust ranking of the
variables; (3) evaluation of the methodology by learning random forest models
for regression with the distinct experimental variations. The main contribution
is the framework that shows how to convert correlations into rankings and
combine them into an aggregate ranking. We have carried out experiments on five
chosen locations to analyze the behavior of this ranking-based methodology. The
results show that the specific performance is dependent on the location and
season, which is expected, and that this selection technique works properly
with Random Forest models but can also improve simpler regression models such
as Bayesian Ridge. This work also contributes with an extensive analysis of the
results. We can conclude that this selection based on the top-k ranked
variables seems promising for this real problem, and it could also be applied
in other domains.",0
Imagine a dragon made of seaweed: How images enhance learning in Wikipedia,2403.07613v1,http://arxiv.org/abs/2403.07613v1,2024-03-12 12:50:19+00:00,"Though images are ubiquitous across Wikipedia, it is not obvious that the
image choices optimally support learning. When well selected, images can
enhance learning by dual coding, complementing, or supporting articles. When
chosen poorly, images can mislead, distract, and confuse. We developed a large
dataset containing 470 questions & answers to 94 Wikipedia articles with images
on a wide range of topics. Through an online experiment (n=704), we determined
whether the images displayed alongside the text of the article are effective in
helping readers understand and learn. For certain tasks, such as learning to
identify targets visually (e.g., ""which of these pictures is a gujia?""),
article images significantly improve accuracy. Images did not significantly
improve general knowledge questions (e.g., ""where are gujia from?""). Most
interestingly, only some images helped with visual knowledge questions (e.g.,
""what shape is a gujia?""). Using our findings, we reflect on the implications
for editors and tools to support image selection.",0
Online Misogyny Against Female Candidates in the 2022 Brazilian Elections: A Threat to Women's Political Representation?,2403.07523v1,http://arxiv.org/abs/2403.07523v1,2024-03-12 10:59:30+00:00,"Technology-facilitated gender-based violence has become a global threat to
women's political representation and democracy. Understanding how online hate
affects its targets is thus paramount. We analyse 10 million tweets directed at
female candidates in the Brazilian election in 2022 and examine their reactions
to online misogyny. Using a self-trained machine learning classifier to detect
Portuguese misogynistic tweets and a quantitative analysis of the candidates'
tweeting behaviour, we investigate how the number of misogynistic attacks
received alters the online activity of the female candidates. We find that
young and left-wing candidates and candidates with higher visibility online
received significantly more attacks. Furthermore, we find that an increase in
misogynistic attacks in the previous week is associated with a decrease in
female candidates' tweets in the following week. This potentially threatens
their equal participation in public opinion building and silences women's
voices in political discourse.",0
D4D: An RGBD diffusion model to boost monocular depth estimation,2403.07516v1,http://arxiv.org/abs/2403.07516v1,2024-03-12 10:47:53+00:00,"Ground-truth RGBD data are fundamental for a wide range of computer vision
applications; however, those labeled samples are difficult to collect and
time-consuming to produce. A common solution to overcome this lack of data is
to employ graphic engines to produce synthetic proxies; however, those data do
not often reflect real-world images, resulting in poor performance of the
trained models at the inference step. In this paper we propose a novel training
pipeline that incorporates Diffusion4D (D4D), a customized 4-channels diffusion
model able to generate realistic RGBD samples. We show the effectiveness of the
developed solution in improving the performances of deep learning models on the
monocular depth estimation task, where the correspondence between RGB and depth
map is crucial to achieving accurate measurements. Our supervised training
pipeline, enriched by the generated samples, outperforms synthetic and original
data performances achieving an RMSE reduction of (8.2%, 11.9%) and (8.1%, 6.1%)
respectively on the indoor NYU Depth v2 and the outdoor KITTI dataset.",0
Uncertainty-guided Contrastive Learning for Single Source Domain Generalisation,2403.07514v1,http://arxiv.org/abs/2403.07514v1,2024-03-12 10:47:45+00:00,"In the context of single domain generalisation, the objective is for models
that have been exclusively trained on data from a single domain to demonstrate
strong performance when confronted with various unfamiliar domains. In this
paper, we introduce a novel model referred to as Contrastive Uncertainty Domain
Generalisation Network (CUDGNet). The key idea is to augment the source
capacity in both input and label spaces through the fictitious domain generator
and jointly learn the domain invariant representation of each class through
contrastive learning. Extensive experiments on two Single Source Domain
Generalisation (SSDG) datasets demonstrate the effectiveness of our approach,
which surpasses the state-of-the-art single-DG methods by up to $7.08\%$. Our
method also provides efficient uncertainty estimation at inference time from a
single forward pass through the generator subnetwork.",0
Learning-Augmented Algorithms with Explicit Predictors,2403.07413v1,http://arxiv.org/abs/2403.07413v1,2024-03-12 08:40:21+00:00,"Recent advances in algorithmic design show how to utilize predictions
obtained by machine learning models from past and present data. These
approaches have demonstrated an enhancement in performance when the predictions
are accurate, while also ensuring robustness by providing worst-case guarantees
when predictions fail. In this paper we focus on online problems; prior
research in this context was focused on a paradigm where the predictor is
pre-trained on past data and then used as a black box (to get the predictions
it was trained for). In contrast, in this work, we unpack the predictor and
integrate the learning problem it gives rise for within the algorithmic
challenge. In particular we allow the predictor to learn as it receives larger
parts of the input, with the ultimate goal of designing online learning
algorithms specifically tailored for the algorithmic task at hand. Adopting
this perspective, we focus on a number of fundamental problems, including
caching and scheduling, which have been well-studied in the black-box setting.
For each of the problems we consider, we introduce new algorithms that take
advantage of explicit learning algorithms which we carefully design towards
optimizing the overall performance. We demonstrate the potential of our
approach by deriving performance bounds which improve over those established in
previous work.",0
In-context learning enables multimodal large language models to classify cancer pathology images,2403.07407v1,http://arxiv.org/abs/2403.07407v1,2024-03-12 08:34:34+00:00,"Medical image classification requires labeled, task-specific datasets which
are used to train deep learning networks de novo, or to fine-tune foundation
models. However, this process is computationally and technically demanding. In
language processing, in-context learning provides an alternative, where models
learn from within prompts, bypassing the need for parameter updates. Yet,
in-context learning remains underexplored in medical image analysis. Here, we
systematically evaluate the model Generative Pretrained Transformer 4 with
Vision capabilities (GPT-4V) on cancer image processing with in-context
learning on three cancer histopathology tasks of high importance:
Classification of tissue subtypes in colorectal cancer, colon polyp subtyping
and breast tumor detection in lymph node sections. Our results show that
in-context learning is sufficient to match or even outperform specialized
neural networks trained for particular tasks, while only requiring a minimal
number of samples. In summary, this study demonstrates that large vision
language models trained on non-domain specific data can be applied out-of-the
box to solve medical image-processing tasks in histopathology. This
democratizes access of generalist AI models to medical experts without
technical background especially for areas where annotated data is scarce.",0
FeTrIL++: Feature Translation for Exemplar-Free Class-Incremental Learning with Hill-Climbing,2403.07406v1,http://arxiv.org/abs/2403.07406v1,2024-03-12 08:34:05+00:00,"Exemplar-free class-incremental learning (EFCIL) poses significant
challenges, primarily due to catastrophic forgetting, necessitating a delicate
balance between stability and plasticity to accurately recognize both new and
previous classes. Traditional EFCIL approaches typically skew towards either
model plasticity through successive fine-tuning or stability by employing a
fixed feature extractor beyond the initial incremental state. Building upon the
foundational FeTrIL framework, our research extends into novel experimental
domains to examine the efficacy of various oversampling techniques and dynamic
optimization strategies across multiple challenging datasets and incremental
settings. We specifically explore how oversampling impacts accuracy relative to
feature availability and how different optimization methodologies, including
dynamic recalibration and feature pool diversification, influence incremental
learning outcomes. The results from these comprehensive experiments, conducted
on CIFAR100, Tiny-ImageNet, and an ImageNet-Subset, under-score the superior
performance of FeTrIL in balancing accuracy for both new and past classes
against ten contemporary methods. Notably, our extensions reveal the nuanced
impacts of oversampling and optimization on EFCIL, contributing to a more
refined understanding of feature-space manipulation for class incremental
learning. FeTrIL and its extended analysis in this paper FeTrIL++ pave the way
for more adaptable and efficient EFCIL methodologies, promising significant
improvements in handling catastrophic forgetting without the need for
exemplars.",0
Accelerated Inference and Reduced Forgetting: The Dual Benefits of Early-Exit Networks in Continual Learning,2403.07404v1,http://arxiv.org/abs/2403.07404v1,2024-03-12 08:33:26+00:00,"Driven by the demand for energy-efficient employment of deep neural networks,
early-exit methods have experienced a notable increase in research attention.
These strategies allow for swift predictions by making decisions early in the
network, thereby conserving computation time and resources. However, so far the
early-exit networks have only been developed for stationary data distributions,
which restricts their application in real-world scenarios with continuous
non-stationary data. This study aims to explore the continual learning of the
early-exit networks. We adapt existing continual learning methods to fit with
early-exit architectures and investigate their behavior in the continual
setting. We notice that early network layers exhibit reduced forgetting and can
outperform standard networks even when using significantly fewer resources.
Furthermore, we analyze the impact of task-recency bias on early-exit inference
and propose Task-wise Logits Correction (TLC), a simple method that equalizes
this bias and improves the network performance for every given compute budget
in the class-incremental setting. We assess the accuracy and computational cost
of various continual learning techniques enhanced with early-exits and TLC
across standard class-incremental learning benchmarks such as 10 split CIFAR100
and ImageNetSubset and show that TLC can achieve the accuracy of the standard
methods using less than 70\% of their computations. Moreover, at full
computational budget, our method outperforms the accuracy of the standard
counterparts by up to 15 percentage points. Our research underscores the
inherent synergy between early-exit networks and continual learning,
emphasizing their practical utility in resource-constrained environments.",0
From Canteen Food to Daily Meals: Generalizing Food Recognition to More Practical Scenarios,2403.07403v1,http://arxiv.org/abs/2403.07403v1,2024-03-12 08:32:23+00:00,"The precise recognition of food categories plays a pivotal role for
intelligent health management, attracting significant research attention in
recent years. Prominent benchmarks, such as Food-101 and VIREO Food-172,
provide abundant food image resources that catalyze the prosperity of research
in this field. Nevertheless, these datasets are well-curated from canteen
scenarios and thus deviate from food appearances in daily life. This
discrepancy poses great challenges in effectively transferring classifiers
trained on these canteen datasets to broader daily-life scenarios encountered
by humans. Toward this end, we present two new benchmarks, namely DailyFood-172
and DailyFood-16, specifically designed to curate food images from everyday
meals. These two datasets are used to evaluate the transferability of
approaches from the well-curated food image domain to the everyday-life food
image domain. In addition, we also propose a simple yet effective baseline
method named Multi-Cluster Reference Learning (MCRL) to tackle the
aforementioned domain gap. MCRL is motivated by the observation that food
images in daily-life scenarios exhibit greater intra-class appearance variance
compared with those in well-curated benchmarks. Notably, MCRL can be seamlessly
coupled with existing approaches, yielding non-trivial performance
enhancements. We hope our new benchmarks can inspire the community to explore
the transferability of food recognition models trained on well-curated datasets
toward practical real-life applications.",0
Temporal Decisions: Leveraging Temporal Correlation for Efficient Decisions in Early Exit Neural Networks,2403.07958v1,http://arxiv.org/abs/2403.07958v1,2024-03-12 08:28:27+00:00,"Deep Learning is becoming increasingly relevant in Embedded and
Internet-of-things applications. However, deploying models on embedded devices
poses a challenge due to their resource limitations. This can impact the
model's inference accuracy and latency. One potential solution are Early Exit
Neural Networks, which adjust model depth dynamically through additional
classifiers attached between their hidden layers. However, the real-time
termination decision mechanism is critical for the system's efficiency,
latency, and sustained accuracy.
  This paper introduces Difference Detection and Temporal Patience as decision
mechanisms for Early Exit Neural Networks. They leverage the temporal
correlation present in sensor data streams to efficiently terminate the
inference. We evaluate their effectiveness in health monitoring, image
classification, and wake-word detection tasks. Our novel contributions were
able to reduce the computational footprint compared to established decision
mechanisms significantly while maintaining higher accuracy scores. We achieved
a reduction of mean operations per inference by up to 80% while maintaining
accuracy levels within 5% of the original model.
  These findings highlight the importance of considering temporal correlation
in sensor data to improve the termination decision.",0
Efficient Post-Training Augmentation for Adaptive Inference in Heterogeneous and Distributed IoT Environments,2403.07957v1,http://arxiv.org/abs/2403.07957v1,2024-03-12 08:27:53+00:00,"Early Exit Neural Networks (EENNs) present a solution to enhance the
efficiency of neural network deployments. However, creating EENNs is
challenging and requires specialized domain knowledge, due to the large amount
of additional design choices. To address this issue, we propose an automated
augmentation flow that focuses on converting an existing model into an EENN. It
performs all required design decisions for the deployment to heterogeneous or
distributed hardware targets: Our framework constructs the EENN architecture,
maps its subgraphs to the hardware targets, and configures its decision
mechanism. To the best of our knowledge, it is the first framework that is able
to perform all of these steps.
  We evaluated our approach on a collection of Internet-of-Things and standard
image classification use cases. For a speech command detection task, our
solution was able to reduce the mean operations per inference by 59.67%. For an
ECG classification task, it was able to terminate all samples early, reducing
the mean inference energy by 74.9% and computations by 78.3%. On CIFAR-10, our
solution was able to achieve up to a 58.75% reduction in computations.
  The search on a ResNet-152 base model for CIFAR-10 took less than nine hours
on a laptop CPU. Our proposed approach enables the creation of EENN optimized
for IoT environments and can reduce the inference cost of Deep Learning
applications on embedded and fog platforms, while also significantly reducing
the search cost - making it more accessible for scientists and engineers in
industry and research. The low search cost improves the accessibility of EENNs,
with the potential to improve the efficiency of neural networks in a wide range
of practical applications.",0
Complex Reasoning over Logical Queries on Commonsense Knowledge Graphs,2403.07398v1,http://arxiv.org/abs/2403.07398v1,2024-03-12 08:13:52+00:00,"Event commonsense reasoning requires the ability to reason about the
relationship between events, as well as infer implicit context underlying that
relationship. However, data scarcity makes it challenging for language models
to learn to generate commonsense inferences for contexts and questions
involving interactions between complex events. To address this demand, we
present COM2 (COMplex COMmonsense), a new dataset created by sampling multi-hop
logical queries (e.g., the joint effect or cause of both event A and B, or the
effect of the effect of event C) from an existing commonsense knowledge graph
(CSKG), and verbalizing them using handcrafted rules and large language models
into multiple-choice and text generation questions. Our experiments show that
language models trained on COM2 exhibit significant improvements in complex
reasoning ability, resulting in enhanced zero-shot performance in both
in-domain and out-of-domain tasks for question answering and generative
commonsense reasoning, without expensive human annotations.",0
DeepCDCL: An CDCL-based Neural Network Verification Framework,2403.07956v1,http://arxiv.org/abs/2403.07956v1,2024-03-12 08:07:06+00:00,"Neural networks in safety-critical applications face increasing safety and
security concerns due to their susceptibility to little disturbance. In this
paper, we propose DeepCDCL, a novel neural network verification framework based
on the Conflict-Driven Clause Learning (CDCL) algorithm. We introduce an
asynchronous clause learning and management structure, reducing redundant time
consumption compared to the direct application of the CDCL framework.
Furthermore, we also provide a detailed evaluation of the performance of our
approach on the ACAS Xu and MNIST datasets, showing that a significant speed-up
is achieved in most cases.",0
Learning on the correct class for domain inverse problems of gravimetry,2403.07393v1,http://arxiv.org/abs/2403.07393v1,2024-03-12 08:03:54+00:00,"We consider end-to-end learning approaches for inverse problems of
gravimetry. Due to ill-posedness of the inverse gravimetry, the reliability of
learning approaches is questionable. To deal with this problem, we propose the
strategy of learning on the correct class. The well-posedness theorems are
employed when designing the neural-network architecture and constructing the
training set. Given the density-contrast function as a priori information, the
domain of mass can be uniquely determined under certain constrains, and the
domain inverse problem is a correct class of the inverse gravimetry. Under this
correct class, we design the neural network for learning by mimicking the
level-set formulation for the inverse gravimetry. Numerical examples illustrate
that the method is able to recover mass models with non-constant density
contrast.",0
Learning Correction Errors via Frequency-Self Attention for Blind Image Super-Resolution,2403.07390v1,http://arxiv.org/abs/2403.07390v1,2024-03-12 07:58:14+00:00,"Previous approaches for blind image super-resolution (SR) have relied on
degradation estimation to restore high-resolution (HR) images from their
low-resolution (LR) counterparts. However, accurate degradation estimation
poses significant challenges. The SR model's incompatibility with degradation
estimation methods, particularly the Correction Filter, may significantly
impair performance as a result of correction errors. In this paper, we
introduce a novel blind SR approach that focuses on Learning Correction Errors
(LCE). Our method employs a lightweight Corrector to obtain a corrected
low-resolution (CLR) image. Subsequently, within an SR network, we jointly
optimize SR performance by utilizing both the original LR image and the
frequency learning of the CLR image. Additionally, we propose a new
Frequency-Self Attention block (FSAB) that enhances the global information
utilization ability of Transformer. This block integrates both self-attention
and frequency spatial attention mechanisms. Extensive ablation and comparison
experiments conducted across various settings demonstrate the superiority of
our method in terms of visual quality and accuracy. Our approach effectively
addresses the challenges associated with degradation estimation and correction
errors, paving the way for more accurate blind image SR.",0
Multi-source Scheduling and Resource Allocation for Age-of-Semantic-Importance Optimization in Status Update Systems,2403.07386v1,http://arxiv.org/abs/2403.07386v1,2024-03-12 07:47:07+00:00,"In recent years, semantic communication is progressively emerging as an
effective means of facilitating intelligent and context-aware communication.
However, current researches seldom simultaneously consider the reliability and
timeliness of semantic communication, where scheduling and resource allocation
(SRA) plays a crucial role. In contrast, conventional age-based approaches
cannot seamlessly extend to semantic communication due to their oversight of
semantic importance. To bridge this gap, we introduce a novel metric: Age of
Semantic Importance (AoSI), which adaptly captures both the freshness of
information and its semantic importance. Utilizing AoSI, we formulate an
average AoSI minimization problem by optimizing multi-source SRA. To address
this problem, we proposed a AoSI-aware joint SRA algorithm based on Deep
Q-Network (DQN). Simulation results validate the effectiveness of our proposed
method, demonstrating its ability to facilitate timely and reliable semantic
communication.",0
SmallToLarge (S2L): Scalable Data Selection for Fine-tuning Large Language Models by Summarizing Training Trajectories of Small Models,2403.07384v1,http://arxiv.org/abs/2403.07384v1,2024-03-12 07:45:33+00:00,"Despite the effectiveness of data selection for large language models (LLMs)
during pretraining and instruction fine-tuning phases, improving data
efficiency in supervised fine-tuning (SFT) for specialized domains poses
significant challenges due to the complexity of fine-tuning data. To bridge
this gap, we introduce an effective and scalable data selection method for SFT,
SmallToLarge (S2L), which leverages training trajectories from small models to
guide the data selection for larger models. We demonstrate through extensive
experiments that S2L significantly improves data efficiency in SFT for
mathematical problem-solving, reducing the training data to just 11% of the
original MathInstruct dataset (Yue et al., 2023) to match full dataset
performance while outperforming state-of-the-art data selection algorithms by
an average of 4.7% across 6 in- and out-domain evaluation datasets. Remarkably,
selecting only 50K data for SFT, S2L achieves a 32.7% accuracy on the most
challenging MATH (Hendrycks et al., 2021) benchmark, improving Phi-2 (Li et
al., 2023b) by 16.6%. In clinical text summarization on the MIMIC-III dataset
(Johnson et al., 2016), S2L again outperforms training on the full dataset
using only 50% of the data. Notably, S2L can perform data selection using a
reference model 40x smaller than the target model, proportionally reducing the
cost of data selection.",0
"Hallmarks of Optimization Trajectories in Neural Networks and LLMs: The Lengths, Bends, and Dead Ends",2403.07379v1,http://arxiv.org/abs/2403.07379v1,2024-03-12 07:32:47+00:00,"We propose a fresh take on understanding the mechanisms of neural networks by
analyzing the rich structure of parameters contained within their optimization
trajectories. Towards this end, we introduce some natural notions of the
complexity of optimization trajectories, both qualitative and quantitative,
which reveal the inherent nuance and interplay involved between various
optimization choices, such as momentum, weight decay, and batch size. We use
them to provide key hallmarks about the nature of optimization in deep neural
networks: when it goes right, and when it finds itself in a dead end. Further,
thanks to our trajectory perspective, we uncover an intertwined behaviour of
momentum and weight decay that promotes directional exploration, as well as a
directional regularization behaviour of some others. We perform experiments
over large-scale vision and language settings, including large language models
(LLMs) with up to 12 billion parameters, to demonstrate the value of our
approach.",0
SVD-LLM: Truncation-aware Singular Value Decomposition for Large Language Model Compression,2403.07378v1,http://arxiv.org/abs/2403.07378v1,2024-03-12 07:31:18+00:00,"The advancements in Large Language Models (LLMs) have been hindered by their
substantial sizes, which necessitate LLM compression methods for practical
deployment. Singular Value Decomposition (SVD) offers a promising solution for
LLM compression. However, state-of-the-art SVD-based LLM compression methods
have two key limitations: truncating smaller singular values may lead to higher
compression loss, and the lack of update on the remaining model parameters
after SVD truncation. In this work, we propose SVD-LLM, a new SVD-based LLM
compression method that addresses the limitations of existing methods. SVD-LLM
incorporates a truncation-aware data whitening strategy to ensure a direct
mapping between singular values and compression loss. Moreover, SVD-LLM adopts
a layer-wise closed-form model parameter update strategy to compensate for
accuracy degradation caused by SVD truncation. We evaluate SVD-LLM on a total
of 11 datasets and seven models from three different LLM families at four
different scales. Our results demonstrate the superiority of SVD-LLM over
state-of-the-arts, especially at high model compression ratios. The source code
is available at https://github.com/AIoT-MLSys-Lab/SVD-LLM.",0
NavCoT: Boosting LLM-Based Vision-and-Language Navigation via Learning Disentangled Reasoning,2403.07376v1,http://arxiv.org/abs/2403.07376v1,2024-03-12 07:27:02+00:00,"Vision-and-Language Navigation (VLN), as a crucial research problem of
Embodied AI, requires an embodied agent to navigate through complex 3D
environments following natural language instructions. Recent research has
highlighted the promising capacity of large language models (LLMs) in VLN by
improving navigational reasoning accuracy and interpretability. However, their
predominant use in an offline manner usually suffers from substantial domain
gap between the VLN task and the LLM training corpus. This paper introduces a
novel strategy called Navigational Chain-of-Thought (NavCoT), where we fulfill
parameter-efficient in-domain training to enable self-guided navigational
decision, leading to a significant mitigation of the domain gap in a
cost-effective manner. Specifically, at each timestep, the LLM is prompted to
forecast the navigational chain-of-thought by: 1) acting as a world model to
imagine the next observation according to the instruction, 2) selecting the
candidate observation that best aligns with the imagination, and 3) determining
the action based on the reasoning from the prior steps. Through constructing
formalized labels for training, the LLM can learn to generate desired and
reasonable chain-of-thought outputs for improving the action decision.
Experimental results across various training settings and popular VLN
benchmarks (e.g., Room-to-Room (R2R), Room-across-Room (RxR), Room-for-Room
(R4R)) show the significant superiority of NavCoT over the direct action
prediction variants. Through simple parameter-efficient finetuning, our NavCoT
outperforms a recent GPT4-based approach with ~7% relative improvement on the
R2R dataset. We believe that NavCoT will help unlock more task-adaptive and
scalable LLM-based embodied agents, which are helpful for developing real-world
robotics applications. Code is available at
https://github.com/expectorlin/NavCoT.",0
Towards Faithful Explanations: Boosting Rationalization with Shortcuts Discovery,2403.07955v1,http://arxiv.org/abs/2403.07955v1,2024-03-12 07:24:17+00:00,"The remarkable success in neural networks provokes the selective
rationalization. It explains the prediction results by identifying a small
subset of the inputs sufficient to support them. Since existing methods still
suffer from adopting the shortcuts in data to compose rationales and limited
large-scale annotated rationales by human, in this paper, we propose a
Shortcuts-fused Selective Rationalization (SSR) method, which boosts the
rationalization by discovering and exploiting potential shortcuts.
Specifically, SSR first designs a shortcuts discovery approach to detect
several potential shortcuts. Then, by introducing the identified shortcuts, we
propose two strategies to mitigate the problem of utilizing shortcuts to
compose rationales. Finally, we develop two data augmentations methods to close
the gap in the number of annotated rationales. Extensive experimental results
on real-world datasets clearly validate the effectiveness of our proposed
method.",0
Textual Knowledge Matters: Cross-Modality Co-Teaching for Generalized Visual Class Discovery,2403.07369v1,http://arxiv.org/abs/2403.07369v1,2024-03-12 07:06:50+00:00,"In this paper, we study the problem of Generalized Category Discovery (GCD),
which aims to cluster unlabeled data from both known and unknown categories
using the knowledge of labeled data from known categories. Current GCD methods
rely on only visual cues, which however neglect the multi-modality perceptive
nature of human cognitive processes in discovering novel visual categories. To
address this, we propose a two-phase TextGCD framework to accomplish
multi-modality GCD by exploiting powerful Visual-Language Models. TextGCD
mainly includes a retrieval-based text generation (RTG) phase and a
cross-modality co-teaching (CCT) phase. First, RTG constructs a visual lexicon
using category tags from diverse datasets and attributes from Large Language
Models, generating descriptive texts for images in a retrieval manner. Second,
CCT leverages disparities between textual and visual modalities to foster
mutual learning, thereby enhancing visual GCD. In addition, we design an
adaptive class aligning strategy to ensure the alignment of category
perceptions between modalities as well as a soft-voting mechanism to integrate
multi-modality cues. Experiments on eight datasets show the large superiority
of our approach over state-of-the-art methods. Notably, our approach
outperforms the best competitor, by 7.7% and 10.8% in All accuracy on
ImageNet-1k and CUB, respectively.",0
Entropy is not Enough for Test-Time Adaptation: From the Perspective of Disentangled Factors,2403.07366v1,http://arxiv.org/abs/2403.07366v1,2024-03-12 07:01:57+00:00,"Test-time adaptation (TTA) fine-tunes pre-trained deep neural networks for
unseen test data. The primary challenge of TTA is limited access to the entire
test dataset during online updates, causing error accumulation. To mitigate it,
TTA methods have utilized the model output's entropy as a confidence metric
that aims to determine which samples have a lower likelihood of causing error.
Through experimental studies, however, we observed the unreliability of entropy
as a confidence metric for TTA under biased scenarios and theoretically
revealed that it stems from the neglect of the influence of latent disentangled
factors of data on predictions. Building upon these findings, we introduce a
novel TTA method named Destroy Your Object (DeYO), which leverages a newly
proposed confidence metric named Pseudo-Label Probability Difference (PLPD).
PLPD quantifies the influence of the shape of an object on prediction by
measuring the difference between predictions before and after applying an
object-destructive transformation. DeYO consists of sample selection and sample
weighting, which employ entropy and PLPD concurrently. For robust adaptation,
DeYO prioritizes samples that dominantly incorporate shape information when
making predictions. Our extensive experiments demonstrate the consistent
superiority of DeYO over baseline methods across various scenarios, including
biased and wild. Project page is publicly available at
https://whitesnowdrop.github.io/DeYO/.",0
Hybrid Kinetics Embedding Framework for Dynamic PET Reconstruction,2403.07364v1,http://arxiv.org/abs/2403.07364v1,2024-03-12 06:58:37+00:00,"In dynamic positron emission tomography (PET) reconstruction, the importance
of leveraging the temporal dependence of the data has been well appreciated.
Current deep-learning solutions can be categorized in two groups in the way the
temporal dynamics is modeled: data-driven approaches use spatiotemporal neural
networks to learn the temporal dynamics of tracer kinetics from data, which
relies heavily on data supervision; physics-based approaches leverage \textit{a
priori} tracer kinetic models to focus on inferring their parameters, which
relies heavily on the accuracy of the prior kinetic model. In this paper, we
marry the strengths of these two approaches in a hybrid kinetics embedding
(HyKE-Net) framework for dynamic PET reconstruction. We first introduce a novel
\textit{hybrid} model of tracer kinetics consisting of a physics-based function
augmented by a neural component to account for its gap to data-generating
tracer kinetics, both identifiable from data. We then embed this hybrid model
at the latent space of an encoding-decoding framework to enable both supervised
and unsupervised identification of the hybrid kinetics and thereby dynamic PET
reconstruction. Through both phantom and real-data experiments, we demonstrate
the benefits of HyKE-Net -- especially in unsupervised reconstructions -- over
existing physics-based and data-driven baselines as well as its ablated
formulations where the embedded tracer kinetics are purely physics-based,
purely neural, or hybrid but with a non-adaptable neural component.",0
A New Random Forest Ensemble of Intuitionistic Fuzzy Decision Trees,2403.07363v1,http://arxiv.org/abs/2403.07363v1,2024-03-12 06:52:24+00:00,"Classification is essential to the applications in the field of data mining,
artificial intelligence, and fault detection. There exists a strong need in
developing accurate, suitable, and efficient classification methods and
algorithms with broad applicability. Random forest is a general algorithm that
is often used for classification under complex conditions. Although it has been
widely adopted, its combination with diverse fuzzy theory is still worth
exploring. In this paper, we propose the intuitionistic fuzzy random forest
(IFRF), a new random forest ensemble of intuitionistic fuzzy decision trees
(IFDT). Such trees in forest use intuitionistic fuzzy information gain to
select features and consider hesitation in information transmission. The
proposed method enjoys the power of the randomness from bootstrapped sampling
and feature selection, the flexibility of fuzzy logic and fuzzy sets, and the
robustness of multiple classifier systems. Extensive experiments demonstrate
that the IFRF has competitative and superior performance compared to other
state-of-the-art fuzzy and ensemble algorithms. IFDT is more suitable for
ensemble learning with outstanding classification accuracy. This study is the
first to propose a random forest ensemble based on the intuitionistic fuzzy
theory.",0
Challenging Forgets: Unveiling the Worst-Case Forget Sets in Machine Unlearning,2403.07362v1,http://arxiv.org/abs/2403.07362v1,2024-03-12 06:50:32+00:00,"The trustworthy machine learning (ML) community is increasingly recognizing
the crucial need for models capable of selectively 'unlearning' data points
after training. This leads to the problem of machine unlearning (MU), aiming to
eliminate the influence of chosen data points on model performance, while still
maintaining the model's utility post-unlearning. Despite various MU methods for
data influence erasure, evaluations have largely focused on random data
forgetting, ignoring the vital inquiry into which subset should be chosen to
truly gauge the authenticity of unlearning performance. To tackle this issue,
we introduce a new evaluative angle for MU from an adversarial viewpoint. We
propose identifying the data subset that presents the most significant
challenge for influence erasure, i.e., pinpointing the worst-case forget set.
Utilizing a bi-level optimization principle, we amplify unlearning challenges
at the upper optimization level to emulate worst-case scenarios, while
simultaneously engaging in standard training and unlearning at the lower level,
achieving a balance between data influence erasure and model utility. Our
proposal offers a worst-case evaluation of MU's resilience and effectiveness.
Through extensive experiments across different datasets (including CIFAR-10,
100, CelebA, Tiny ImageNet, and ImageNet) and models (including both image
classifiers and generative models), we expose critical pros and cons in
existing (approximate) unlearning strategies. Our results illuminate the
complex challenges of MU in practice, guiding the future development of more
accurate and robust unlearning algorithms. The code is available at
https://github.com/OPTML-Group/Unlearn-WorstCase.",0
Optimization of Pressure Management Strategies for Geological CO2 Sequestration Using Surrogate Model-based Reinforcement Learning,2403.07360v1,http://arxiv.org/abs/2403.07360v1,2024-03-12 06:48:16+00:00,"Injecting greenhouse gas into deep underground reservoirs for permanent
storage can inadvertently lead to fault reactivation, caprock fracturing and
greenhouse gas leakage when the injection-induced stress exceeds the critical
threshold. Extraction of pre-existing fluids at various stages of injection
process, referred as pressure management, can mitigate associated risks and
lessen environmental impact. However, identifying optimal pressure management
strategies typically requires thousands of full-order simulations due to the
need for function evaluations, making the process computationally prohibitive.
This paper introduces a novel surrogate model-based reinforcement learning
method for devising optimal pressure management strategies for geological CO2
sequestration efficiently. Our approach comprises two steps. Firstly, a
surrogate model is developed through the embed to control method, which employs
an encoder-transition-decoder structure to learn latent dynamics. Leveraging
this proxy model, reinforcement learning is utilized to find an optimal
strategy that maximizes economic benefits while satisfying various control
constraints. The reinforcement learning agent receives the latent state space
representation and immediate reward tailored for CO2 sequestration and choose
real-time controls which are subject to predefined engineering constraints in
order to maximize the long-term cumulative rewards. To demonstrate its
effectiveness, this framework is applied to a compositional simulation model
where CO2 is injected into saline aquifer. The results reveal that our
surrogate model-based reinforcement learning approach significantly optimizes
CO2 sequestration strategies, leading to notable economic gains compared to
baseline scenarios.",0
Premonition: Using Generative Models to Preempt Future Data Changes in Continual Learning,2403.07356v1,http://arxiv.org/abs/2403.07356v1,2024-03-12 06:29:54+00:00,"Continual learning requires a model to adapt to ongoing changes in the data
distribution, and often to the set of tasks to be performed. It is rare,
however, that the data and task changes are completely unpredictable. Given a
description of an overarching goal or data theme, which we call a realm, humans
can often guess what concepts are associated with it. We show here that the
combination of a large language model and an image generation model can
similarly provide useful premonitions as to how a continual learning challenge
might develop over time. We use the large language model to generate text
descriptions of semantically related classes that might potentially appear in
the data stream in future. These descriptions are then rendered using Stable
Diffusion to generate new labelled image samples. The resulting synthetic
dataset is employed for supervised pre-training, but is discarded prior to
commencing continual learning, along with the pre-training classification head.
We find that the backbone of our pre-trained networks can learn representations
useful for the downstream continual learning problem, thus becoming a valuable
input to any existing continual learning method. Although there are
complexities arising from the domain gap between real and synthetic images, we
show that pre-training models in this manner improves multiple Class Incremenal
Learning (CIL) methods on fine-grained image classification benchmarks.
Supporting code can be found at https://github.com/cl-premonition/premonition.",0
Vector Quantization for Deep-Learning-Based CSI Feedback in Massive MIMO Systems,2403.07355v2,http://arxiv.org/abs/2403.07355v2,2024-03-12 06:28:41+00:00,"This paper presents a finite-rate deep-learning (DL)-based channel state
information (CSI) feedback method for massive multiple-input multiple-output
(MIMO) systems. The presented method provides a finite-bit representation of
the latent vector based on a vector-quantized variational autoencoder (VQ-VAE)
framework while reducing its computational complexity based on shape-gain
vector quantization. In this method, the magnitude of the latent vector is
quantized using a non-uniform scalar codebook with a proper transformation
function, while the direction of the latent vector is quantized using a
trainable Grassmannian codebook. A multi-rate codebook design strategy is also
developed by introducing a codeword selection rule for a nested codebook along
with the design of a loss function. Simulation results demonstrate that the
proposed method reduces the computational complexity associated with VQ-VAE
while improving CSI reconstruction performance under a given feedback overhead.",0
Optimizing Polynomial Graph Filters: A Novel Adaptive Krylov Subspace Approach,2403.07954v1,http://arxiv.org/abs/2403.07954v1,2024-03-12 06:26:17+00:00,"Graph Neural Networks (GNNs), known as spectral graph filters, find a wide
range of applications in web networks. To bypass eigendecomposition, polynomial
graph filters are proposed to approximate graph filters by leveraging various
polynomial bases for filter training. However, no existing studies have
explored the diverse polynomial graph filters from a unified perspective for
optimization.
  In this paper, we first unify polynomial graph filters, as well as the
optimal filters of identical degrees into the Krylov subspace of the same
order, thus providing equivalent expressive power theoretically. Next, we
investigate the asymptotic convergence property of polynomials from the unified
Krylov subspace perspective, revealing their limited adaptability in graphs
with varying heterophily degrees. Inspired by those facts, we design a novel
adaptive Krylov subspace approach to optimize polynomial bases with provable
controllability over the graph spectrum so as to adapt various heterophily
graphs. Subsequently, we propose AdaptKry, an optimized polynomial graph filter
utilizing bases from the adaptive Krylov subspaces. Meanwhile, in light of the
diverse spectral properties of complex graphs, we extend AdaptKry by leveraging
multiple adaptive Krylov bases without incurring extra training costs. As a
consequence, extended AdaptKry is able to capture the intricate characteristics
of graphs and provide insights into their inherent complexity. We conduct
extensive experiments across a series of real-world datasets. The experimental
results demonstrate the superior filtering capability of AdaptKry, as well as
the optimized efficacy of the adaptive Krylov basis.",0
Abstracting Sparse DNN Acceleration via Structured Sparse Tensor Decomposition,2403.07953v1,http://arxiv.org/abs/2403.07953v1,2024-03-12 06:25:47+00:00,"Exploiting sparsity in deep neural networks (DNNs) has been a promising area
to meet the growing computation need of modern DNNs. However, in practice,
sparse DNN acceleration still faces a key challenge. To minimize the overhead
of sparse acceleration, hardware designers have proposed structured sparse
hardware support recently, which provides limited flexibility and requires
extra model fine-tuning. Moreover, any sparse model fine-tuned for certain
structured sparse hardware cannot be accelerated by other structured hardware.
To bridge the gap between sparse DNN models and hardware, this paper proposes
tensor approximation via structured decomposition (TASD), which leverages the
distributive property in linear algebra to turn any sparse tensor into a series
of structured sparse tensors. Next, we develop a software framework, TASDER, to
accelerate DNNs by searching layer-wise, high-quality structured decomposition
for both weight and activation tensors so that they can be accelerated by any
systems with structured sparse hardware support. Evaluation results show that,
by exploiting prior structured sparse hardware baselines, our method can
accelerate off-the-shelf dense and sparse DNNs without fine-tuning and improves
energy-delay-product by up to 83% and 74% on average.",0
Graph Unlearning with Efficient Partial Retraining,2403.07353v2,http://arxiv.org/abs/2403.07353v2,2024-03-12 06:22:10+00:00,"Graph Neural Networks (GNNs) have achieved remarkable success in various
real-world applications. However, GNNs may be trained on undesirable graph
data, which can degrade their performance and reliability. To enable trained
GNNs to efficiently unlearn unwanted data, a desirable solution is
retraining-based graph unlearning, which partitions the training graph into
subgraphs and trains sub-models on them, allowing fast unlearning through
partial retraining. However, the graph partition process causes information
loss in the training graph, resulting in the low model utility of sub-GNN
models. In this paper, we propose GraphRevoker, a novel graph unlearning
framework that better maintains the model utility of unlearnable GNNs.
Specifically, we preserve the graph property with graph property-aware sharding
and effectively aggregate the sub-GNN models for prediction with graph
contrastive sub-model aggregation. We conduct extensive experiments to
demonstrate the superiority of our proposed approach.",0
NightHaze: Nighttime Image Dehazing via Self-Prior Learning,2403.07408v1,http://arxiv.org/abs/2403.07408v1,2024-03-12 08:35:42+00:00,"Masked autoencoder (MAE) shows that severe augmentation during training
produces robust representations for high-level tasks. This paper brings the
MAE-like framework to nighttime image enhancement, demonstrating that severe
augmentation during training produces strong network priors that are resilient
to real-world night haze degradations. We propose a novel nighttime image
dehazing method with self-prior learning. Our main novelty lies in the design
of severe augmentation, which allows our model to learn robust priors. Unlike
MAE that uses masking, we leverage two key challenging factors of nighttime
images as augmentation: light effects and noise. During training, we
intentionally degrade clear images by blending them with light effects as well
as by adding noise, and subsequently restore the clear images. This enables our
model to learn clear background priors. By increasing the noise values to
approach as high as the pixel intensity values of the glow and light effect
blended images, our augmentation becomes severe, resulting in stronger priors.
While our self-prior learning is considerably effective in suppressing glow and
revealing details of background scenes, in some cases, there are still some
undesired artifacts that remain, particularly in the forms of over-suppression.
To address these artifacts, we propose a self-refinement module based on the
semi-supervised teacher-student framework. Our NightHaze, especially our
MAE-like self-prior learning, shows that models trained with severe
augmentation effectively improve the visibility of input haze images,
approaching the clarity of clear nighttime images. Extensive experiments
demonstrate that our NightHaze achieves state-of-the-art performance,
outperforming existing nighttime image dehazing methods by a substantial margin
of 15.5% for MUSIQ and 23.5% for ClipIQA.",0
Automated Discovery of Anomalous Features in Ultra-Large Planetary Remote Sensing Datasets using Variational Autoencoders,2403.07424v1,http://arxiv.org/abs/2403.07424v1,2024-03-12 09:04:17+00:00,"The NASA Lunar Reconnaissance Orbiter (LRO) has returned petabytes of lunar
high spatial resolution surface imagery over the past decade, impractical for
humans to fully review manually. Here we develop an automated method using a
deep generative visual model that rapidly retrieves scientifically interesting
examples of LRO surface imagery representing the first planetary image anomaly
detector. We give quantitative experimental evidence that our method
preferentially retrieves anomalous samples such as notable geological features
and known human landing and spacecraft crash sites. Our method addresses a
major capability gap in planetary science and presents a novel way to unlock
insights hidden in ever-increasing remote sensing data archives, with numerous
applications to other science domains. We publish our code and data along with
this paper.",0
Spatiotemporal Representation Learning for Short and Long Medical Image Time Series,2403.07513v1,http://arxiv.org/abs/2403.07513v1,2024-03-12 10:47:29+00:00,"Analyzing temporal developments is crucial for the accurate prognosis of many
medical conditions. Temporal changes that occur over short time scales are key
to assessing the health of physiological functions, such as the cardiac cycle.
Moreover, tracking longer term developments that occur over months or years in
evolving processes, such as age-related macular degeneration (AMD), is
essential for accurate prognosis. Despite the importance of both short and long
term analysis to clinical decision making, they remain understudied in medical
deep learning. State of the art methods for spatiotemporal representation
learning, developed for short natural videos, prioritize the detection of
temporal constants rather than temporal developments. Moreover, they do not
account for varying time intervals between acquisitions, which are essential
for contextualizing observed changes. To address these issues, we propose two
approaches. First, we combine clip-level contrastive learning with a novel
temporal embedding to adapt to irregular time series. Second, we propose
masking and predicting latent frame representations of the temporal sequence.
Our two approaches outperform all prior methods on temporally-dependent tasks
including cardiac output estimation and three prognostic AMD tasks. Overall,
this enables the automated analysis of temporal patterns which are typically
overlooked in applications of deep learning to medicine.",0
Input Data Adaptive Learning (IDAL) for Sub-acute Ischemic Stroke Lesion Segmentation,2403.07428v1,http://arxiv.org/abs/2403.07428v1,2024-03-12 09:11:02+00:00,"In machine learning larger databases are usually associated with higher
classification accuracy due to better generalization. This generalization may
lead to non-optimal classifiers in some medical applications with highly
variable expressions of pathologies. This paper presents a method for learning
from a large training base by adaptively selecting optimal training samples for
given input data. In this way heterogeneous databases are supported two-fold.
First, by being able to deal with sparsely annotated data allows a quick
inclusion of new data set and second, by training an input-dependent
classifier. The proposed approach is evaluated using the SISS challenge. The
proposed algorithm leads to a significant improvement of the classification
accuracy.",0
Reconstructions of Jupiter's magnetic field using physics informed neural networks,2403.07507v1,http://arxiv.org/abs/2403.07507v1,2024-03-12 10:43:52+00:00,"Magnetic sounding using data collected from the Juno mission can be used to
provide constraints on Jupiter's interior. However, inwards continuation of
reconstructions assuming zero electrical conductivity and a representation in
spherical harmonics are limited by the enhancement of noise at small scales. In
this paper we describe new reconstructions of Jupiter's internal magnetic field
based on physics-informed neural networks and either the first 33 (PINN33) or
the first 50 (PINN50) of Juno's orbits. The method can resolve local
structures, and allows for weak ambient electrical currents. Compared with
other methods, our reconstructions of Jupiter's magnetic field both on and
above the surface are similar, and we achieve a similar fit to the Juno data.
However, our models are not hampered by noise at depth, and so offer a much
clearer picture of the interior structure. We estimate that the dynamo boundary
is at a fractional radius of 0.8. At this depth, the magnetic field is arranged
into longitudinal bands, and the great blue spot appears to be rooted in
neighbouring structures of oppositely signed flux.",0
Constrained Optimal Fuel Consumption of HEV: A Constrained Reinforcement Learning Approach,2403.07503v1,http://arxiv.org/abs/2403.07503v1,2024-03-12 10:42:32+00:00,"Hybrid electric vehicles (HEVs) are becoming increasingly popular because
they can better combine the working characteristics of internal combustion
engines and electric motors. However, the minimum fuel consumption of an HEV
for a battery electrical balance case under a specific assembly condition and a
specific speed curve still needs to be clarified in academia and industry.
Regarding this problem, this work provides the mathematical expression of
constrained optimal fuel consumption (COFC) from the perspective of constrained
reinforcement learning (CRL) for the first time globally. Also, two mainstream
approaches of CRL, constrained variational policy optimization (CVPO) and
Lagrangian-based approaches, are utilized for the first time to obtain the
vehicle's minimum fuel consumption under the battery electrical balance
condition. We conduct case studies on the well-known Prius TOYOTA hybrid system
(THS) under the NEDC condition; we give vital steps to implement CRL approaches
and compare the performance between the CVPO and Lagrangian-based approaches.
Our case study found that CVPO and Lagrangian-based approaches can obtain the
lowest fuel consumption while maintaining the SOC balance constraint. The CVPO
approach converges stable, but the Lagrangian-based approach can obtain the
lowest fuel consumption at 3.95 L/100km, though with more significant
oscillations. This result verifies the effectiveness of our proposed CRL
approaches to the COFC problem.",0
Detecting Security-Relevant Methods using Multi-label Machine Learning,2403.07501v1,http://arxiv.org/abs/2403.07501v1,2024-03-12 10:38:54+00:00,"To detect security vulnerabilities, static analysis tools need to be
configured with security-relevant methods. Current approaches can automatically
identify such methods using binary relevance machine learning approaches.
However, they ignore dependencies among security-relevant methods,
over-generalize and perform poorly in practice. Additionally, users have to
nevertheless manually configure static analysis tools using the detected
methods. Based on feedback from users and our observations, the excessive
manual steps can often be tedious, error-prone and counter-intuitive.
  In this paper, we present Dev-Assist, an IntelliJ IDEA plugin that detects
security-relevant methods using a multi-label machine learning approach that
considers dependencies among labels. The plugin can automatically generate
configurations for static analysis tools, run the static analysis, and show the
results in IntelliJ IDEA. Our experiments reveal that Dev-Assist's machine
learning approach has a higher F1-Measure than related approaches. Moreover,
the plugin reduces and simplifies the manual effort required when configuring
and using static analysis tools.",0
FLAME: Fitting Lyα Absorption lines using Machine learning,2403.07498v1,http://arxiv.org/abs/2403.07498v1,2024-03-12 10:37:22+00:00,"We introduce FLAME, a machine learning algorithm designed to fit Voigt
profiles to HI Lyman-alpha (Ly$\alpha$) absorption lines using deep
convolutional neural networks. FLAME integrates two algorithms: the first
determines the number of components required to fit Ly$\alpha$ absorption
lines, and the second calculates the Doppler parameter $b$, the HI column
density N$_{\rm HI}$, and the velocity separation of individual components. For
the current version of FLAME, we trained it on low-redshift Ly$\alpha$ forests
observed with the Far Ultraviolet gratings of the Cosmic Origin Spectrograph
(COS) aboard the Hubble Space Telescope (HST). Drawing on this data, we trained
FLAME on $\sim$ $10^6$ simulated Voigt profiles, forward-modeled to Ly$\alpha$
absorption lines observed with HST-COS, to classify lines as either single or
double components and then determine Voigt profile fitting parameters. FLAME
shows impressive accuracy on the simulated data by identifying more than 98%
(90%) of single (double) component lines. It determines $b$ values within
$\approx \pm{8}~(15)$ km s$^{-1}$ and log $N_{\rm HI}/ {\rm cm}^2$ values
within $\approx \pm 0.3~(0.8)$ for 90% of the single (double) component lines.
However, when applied to real data, FLAME's component classification accuracy
drops by $\sim$ 10%. Despite this, there is a reasonable agreement between the
$b$ and N$_{\rm HI}$ distributions obtained from traditional Voigt profile
fitting methods and FLAME's predictions. Our mock HST-COS data analysis,
designed to emulate real data parameters, demonstrated that FLAME could achieve
consistent accuracy comparable to its performance with simulated data. This
finding suggests that the drop in FLAME's accuracy when used on real data
primarily arises from the difficulty of replicating the full complexity of real
data in the training sample.",0
Tuning diagonal scale matrices for HMC,2403.07495v1,http://arxiv.org/abs/2403.07495v1,2024-03-12 10:35:40+00:00,"Three approaches for adaptively tuning diagonal scale matrices for HMC are
discussed and compared. The common practice of scaling according to estimated
marginal standard deviations is taken as a benchmark. Scaling according to the
mean log-target gradient (ISG), and a scaling method targeting that the
frequency of when the underlying Hamiltonian dynamics crosses the respective
medians should be uniform across dimensions, are taken as alternatives.
Numerical studies suggest that the ISG method leads in many cases to more
efficient sampling than the benchmark, in particular in cases with strong
correlations or non-linear dependencies. The ISG method is also easy to
implement, computationally cheap and would be relatively simple to include in
automatically tuned codes as an alternative to the benchmark practice.",0
Signed graphs in data sciences via communicability geometry,2403.07493v1,http://arxiv.org/abs/2403.07493v1,2024-03-12 10:32:35+00:00,"Signed graphs are an emergent way of representing data in a variety of
contexts were conflicting interactions exist. These include data from
biological, ecological, and social systems. Here we propose the concept of
communicability geometry for signed graphs, proving that metrics in this space,
such as the communicability distance and angles, are Euclidean and spherical.
We then apply these metrics to solve several problems in data analysis of
signed graphs in a unified way. They include the partitioning of signed graphs,
dimensionality reduction, finding hierarchies of alliances in signed networks
as well as the quantification of the degree of polarization between the
existing factions in systems represented by this type of graphs.",0
XpertAI: uncovering model strategies for sub-manifolds,2403.07486v1,http://arxiv.org/abs/2403.07486v1,2024-03-12 10:21:31+00:00,"In recent years, Explainable AI (XAI) methods have facilitated profound
validation and knowledge extraction from ML models. While extensively studied
for classification, few XAI solutions have addressed the challenges specific to
regression models. In regression, explanations need to be precisely formulated
to address specific user queries (e.g.\ distinguishing between `Why is the
output above 0?' and `Why is the output above 50?'). They should furthermore
reflect the model's behavior on the relevant data sub-manifold. In this paper,
we introduce XpertAI, a framework that disentangles the prediction strategy
into multiple range-specific sub-strategies and allows the formulation of
precise queries about the model (the `explanandum') as a linear combination of
those sub-strategies. XpertAI is formulated generally to work alongside popular
XAI attribution techniques, based on occlusion, gradient integration, or
reverse propagation. Qualitative and quantitative results, demonstrate the
benefits of our approach.",0
PMBO: Enhancing Black-Box Optimization through Multivariate Polynomial Surrogates,2403.07485v1,http://arxiv.org/abs/2403.07485v1,2024-03-12 10:21:21+00:00,"We introduce a surrogate-based black-box optimization method, termed
Polynomial-model-based optimization (PMBO). The algorithm alternates polynomial
approximation with Bayesian optimization steps, using Gaussian processes to
model the error between the objective and its polynomial fit. We describe the
algorithmic design of PMBO and compare the results of the performance of PMBO
with several optimization methods for a set of analytic test functions.
  The results show that PMBO outperforms the classic Bayesian optimization and
is robust with respect to the choice of its correlation function family and its
hyper-parameter setting, which, on the contrary, need to be carefully tuned in
classic Bayesian optimization. Remarkably, PMBO performs comparably with
state-of-the-art evolutionary algorithms such as the Covariance Matrix
Adaptation -- Evolution Strategy (CMA-ES). This finding suggests that PMBO
emerges as the pivotal choice among surrogate-based optimization methods when
addressing low-dimensional optimization problems. Hereby, the simple nature of
polynomials opens the opportunity for interpretation and analysis of the
inferred surrogate model, providing a macroscopic perspective on the landscape
of the objective function.",0
A Deep Learning Approach to Diabetes Diagnosis,2403.07483v1,http://arxiv.org/abs/2403.07483v1,2024-03-12 10:18:59+00:00,"Diabetes, resulting from inadequate insulin production or utilization, causes
extensive harm to the body. Existing diagnostic methods are often invasive and
come with drawbacks, such as cost constraints. Although there are machine
learning models like Classwise k Nearest Neighbor (CkNN) and General Regression
Neural Network (GRNN), they struggle with imbalanced data and result in
under-performance. Leveraging advancements in sensor technology and machine
learning, we propose a non-invasive diabetes diagnosis using a Back Propagation
Neural Network (BPNN) with batch normalization, incorporating data re-sampling
and normalization for class balancing. Our method addresses existing challenges
such as limited performance associated with traditional machine learning.
Experimental results on three datasets show significant improvements in overall
accuracy, sensitivity, and specificity compared to traditional methods.
Notably, we achieve accuracies of 89.81% in Pima diabetes dataset, 75.49% in
CDC BRFSS2015 dataset, and 95.28% in Mesra Diabetes dataset. This underscores
the potential of deep learning models for robust diabetes diagnosis. See
project website https://steve-zeyu-zhang.github.io/DiabetesDiagnosis/",0
Towards Graph Foundation Models for Personalization,2403.07478v1,http://arxiv.org/abs/2403.07478v1,2024-03-12 10:12:59+00:00,"In the realm of personalization, integrating diverse information sources such
as consumption signals and content-based representations is becoming
increasingly critical to build state-of-the-art solutions. In this regard, two
of the biggest trends in research around this subject are Graph Neural Networks
(GNNs) and Foundation Models (FMs). While GNNs emerged as a popular solution in
industry for powering personalization at scale, FMs have only recently caught
attention for their promising performance in personalization tasks like ranking
and retrieval. In this paper, we present a graph-based foundation modeling
approach tailored to personalization. Central to this approach is a
Heterogeneous GNN (HGNN) designed to capture multi-hop content and consumption
relationships across a range of recommendable item types. To ensure the
generality required from a Foundation Model, we employ a Large Language Model
(LLM) text-based featurization of nodes that accommodates all item types, and
construct the graph using co-interaction signals, which inherently transcend
content specificity. To facilitate practical generalization, we further couple
the HGNN with an adaptation mechanism based on a two-tower (2T) architecture,
which also operates agnostically to content type. This multi-stage approach
ensures high scalability; while the HGNN produces general purpose embeddings,
the 2T component models in a continuous space the sheer size of user-item
interaction data. Our comprehensive approach has been rigorously tested and
proven effective in delivering recommendations across a diverse array of
products within a real-world, industrial audio streaming platform.",0
Predicting the Risk of Ischemic Stroke in Patients with Atrial Fibrillation using Heterogeneous Drug-protein-disease Network-based Deep Learning,2403.07475v1,http://arxiv.org/abs/2403.07475v1,2024-03-12 10:10:55+00:00,"We develop a deep learning model, ABioSPATH, to predict the one-year risk of
ischemic stroke (IS) in atrial fibrillation (AF) patients. The model integrates
drug-protein-disease pathways and real-world clinical data of AF patients to
generate the IS risk and potential pathways for each patient. The model uses a
multilayer network to identify the mechanism of drug action and disease
comorbidity propagation pathways. The model is tested on the Electronic Health
Record (EHR) data of 7859 AF patients from 43 hospitals in Hong Kong. The model
outperforms all baselines across all metrics and provides valuable
molecular-level insights for clinical use. The model also highlights key
proteins in common pathways and potential IS risks tied to less-studied drugs.
The model only requires routinely collected data, without requiring expensive
biomarkers to be tested.",0
Imbalance-aware Presence-only Loss Function for Species Distribution Modeling,2403.07472v1,http://arxiv.org/abs/2403.07472v1,2024-03-12 10:08:36+00:00,"In the face of significant biodiversity decline, species distribution models
(SDMs) are essential for understanding the impact of climate change on species
habitats by connecting environmental conditions to species occurrences.
Traditionally limited by a scarcity of species observations, these models have
significantly improved in performance through the integration of larger
datasets provided by citizen science initiatives. However, they still suffer
from the strong class imbalance between species within these datasets, often
resulting in the penalization of rare species--those most critical for
conservation efforts. To tackle this issue, this study assesses the
effectiveness of training deep learning models using a balanced presence-only
loss function on large citizen science-based datasets. We demonstrate that this
imbalance-aware loss function outperforms traditional loss functions across
various datasets and tasks, particularly in accurately modeling rare species
with limited observations.",0
On the nonconvexity of some push-forward constraints and its consequences in machine learning,2403.07471v1,http://arxiv.org/abs/2403.07471v1,2024-03-12 10:06:48+00:00,"The push-forward operation enables one to redistribute a probability measure
through a deterministic map. It plays a key role in statistics and
optimization: many learning problems (notably from optimal transport,
generative modeling, and algorithmic fairness) include constraints or penalties
framed as push-forward conditions on the model. However, the literature lacks
general theoretical insights on the (non)convexity of such constraints and its
consequences on the associated learning problems. This paper aims at filling
this gap. In a first part, we provide a range of sufficient and necessary
conditions for the (non)convexity of two sets of functions: the maps
transporting one probability measure to another; the maps inducing equal output
distributions across distinct probability measures. This highlights that for
most probability measures, these push-forward constraints are not convex. In a
second time, we show how this result implies critical limitations on the design
of convex optimization problems for learning generative models or group-fair
predictors. This work will hopefully help researchers and practitioners have a
better understanding of the critical impact of push-forward conditions onto
convexity.",0
One for All and All for One: GNN-based Control-Flow Attestation for Embedded Devices,2403.07465v1,http://arxiv.org/abs/2403.07465v1,2024-03-12 10:00:06+00:00,"Control-Flow Attestation (CFA) is a security service that allows an entity
(verifier) to verify the integrity of code execution on a remote computer
system (prover). Existing CFA schemes suffer from impractical assumptions, such
as requiring access to the prover's internal state (e.g., memory or code), the
complete Control-Flow Graph (CFG) of the prover's software, large sets of
measurements, or tailor-made hardware. Moreover, current CFA schemes are
inadequate for attesting embedded systems due to their high computational
overhead and resource usage.
  In this paper, we overcome the limitations of existing CFA schemes for
embedded devices by introducing RAGE, a novel, lightweight CFA approach with
minimal requirements. RAGE can detect Code Reuse Attacks (CRA), including
control- and non-control-data attacks. It efficiently extracts features from
one execution trace and leverages Unsupervised Graph Neural Networks (GNNs) to
identify deviations from benign executions. The core intuition behind RAGE is
to exploit the correspondence between execution trace, execution graph, and
execution embeddings to eliminate the unrealistic requirement of having access
to a complete CFG.
  We evaluate RAGE on embedded benchmarks and demonstrate that (i) it detects
40 real-world attacks on embedded software; (ii) Further, we stress our scheme
with synthetic return-oriented programming (ROP) and data-oriented programming
(DOP) attacks on the real-world embedded software benchmark Embench, achieving
98.03% (ROP) and 91.01% (DOP) F1-Score while maintaining a low False Positive
Rate of 3.19%; (iii) Additionally, we evaluate RAGE on OpenSSL, used by
millions of devices and achieve 97.49% and 84.42% F1-Score for ROP and DOP
attack detection, with an FPR of 5.47%.",0
On Ranking-based Tests of Independence,2403.07464v1,http://arxiv.org/abs/2403.07464v1,2024-03-12 10:00:00+00:00,"In this paper we develop a novel nonparametric framework to test the
independence of two random variables $\mathbf{X}$ and $\mathbf{Y}$ with unknown
respective marginals $H(dx)$ and $G(dy)$ and joint distribution $F(dx dy)$,
based on {\it Receiver Operating Characteristic} (ROC) analysis and bipartite
ranking. The rationale behind our approach relies on the fact that, the
independence hypothesis $\mathcal{H}\_0$ is necessarily false as soon as the
optimal scoring function related to the pair of distributions $(H\otimes G,\;
F)$, obtained from a bipartite ranking algorithm, has a ROC curve that deviates
from the main diagonal of the unit square.We consider a wide class of rank
statistics encompassing many ways of deviating from the diagonal in the ROC
space to build tests of independence. Beyond its great flexibility, this new
method has theoretical properties that far surpass those of its competitors.
Nonasymptotic bounds for the two types of testing errors are established. From
an empirical perspective, the novel procedure we promote in this paper exhibits
a remarkable ability to detect small departures, of various types, from the
null assumption $\mathcal{H}_0$, even in high dimension, as supported by the
numerical experiments presented here.",0
Backdoor Attack with Mode Mixture Latent Modification,2403.07463v1,http://arxiv.org/abs/2403.07463v1,2024-03-12 09:59:34+00:00,"Backdoor attacks become a significant security concern for deep neural
networks in recent years. An image classification model can be compromised if
malicious backdoors are injected into it. This corruption will cause the model
to function normally on clean images but predict a specific target label when
triggers are present. Previous research can be categorized into two genres:
poisoning a portion of the dataset with triggered images for users to train the
model from scratch, or training a backdoored model alongside a triggered image
generator. Both approaches require significant amount of attackable parameters
for optimization to establish a connection between the trigger and the target
label, which may raise suspicions as more people become aware of the existence
of backdoor attacks. In this paper, we propose a backdoor attack paradigm that
only requires minimal alterations (specifically, the output layer) to a clean
model in order to inject the backdoor under the guise of fine-tuning. To
achieve this, we leverage mode mixture samples, which are located between
different modes in latent space, and introduce a novel method for conducting
backdoor attacks. We evaluate the effectiveness of our method on four popular
benchmark datasets: MNIST, CIFAR-10, GTSRB, and TinyImageNet.",0
Compressed-sensing Lindbladian quantum tomography with trapped ions,2403.07462v1,http://arxiv.org/abs/2403.07462v1,2024-03-12 09:58:37+00:00,"Characterizing the dynamics of quantum systems is a central task for the
development of quantum information processors (QIPs). It serves to benchmark
different devices, learn about their specific noise, and plan the next hardware
upgrades. However, this task is also very challenging, for it requires a large
number of measurements and time-consuming classical processing. Moreover, when
interested in the time dependence of the noise, there is an additional overhead
since the characterization must be performed repeatedly within the time
interval of interest. To overcome this limitation while, at the same time,
ordering the learned sources of noise by their relevance, we focus on the
inference of the dynamical generators of the noisy dynamics using Lindbladian
quantum tomography (LQT). We propose two different improvements of LQT that
alleviate previous shortcomings. In the weak-noise regime of current QIPs, we
manage to linearize the maximum likelihood estimation of LQT, turning the
constrained optimization into a convex problem to reduce the classical
computation cost and to improve its robustness. Moreover, by introducing
compressed sensing techniques, we reduce the number of required measurements
without sacrificing accuracy. To illustrate these improvements, we apply our
LQT tools to trapped-ion experiments of single- and two-qubit gates, advancing
in this way the previous state of the art.",0
Experimental Comparison of Ensemble Methods and Time-to-Event Analysis Models Through Integrated Brier Score and Concordance Index,2403.07460v1,http://arxiv.org/abs/2403.07460v1,2024-03-12 09:57:45+00:00,"Time-to-event analysis is a branch of statistics that has increased in
popularity during the last decades due to its many application fields, such as
predictive maintenance, customer churn prediction and population lifetime
estimation. In this paper, we review and compare the performance of several
prediction models for time-to-event analysis. These consist of semi-parametric
and parametric statistical models, in addition to machine learning approaches.
Our study is carried out on three datasets and evaluated in two different
scores (the integrated Brier score and concordance index). Moreover, we show
how ensemble methods, which surprisingly have not yet been much studied in
time-to-event analysis, can improve the prediction accuracy and enhance the
robustness of the prediction performance. We conclude the analysis with a
simulation experiment in which we evaluate the factors influencing the
performance ranking of the methods using both scores.",0
A tutorial on multi-view autoencoders using the multi-view-AE library,2403.07456v1,http://arxiv.org/abs/2403.07456v1,2024-03-12 09:51:05+00:00,"There has been a growing interest in recent years in modelling multiple
modalities (or views) of data to for example, understand the relationship
between modalities or to generate missing data. Multi-view autoencoders have
gained significant traction for their adaptability and versatility in modelling
multi-modal data, demonstrating an ability to tailor their approach to suit the
characteristics of the data at hand. However, most multi-view autoencoders have
inconsistent notation and are often implemented using different coding
frameworks. To address this, we present a unified mathematical framework for
multi-view autoencoders, consolidating their formulations. Moreover, we offer
insights into the motivation and theoretical advantages of each model. To
facilitate accessibility and practical use, we extend the documentation and
functionality of the previously introduced \texttt{multi-view-AE} library. This
library offers Python implementations of numerous multi-view autoencoder
models, presented within a user-friendly framework. Through benchmarking
experiments, we evaluate our implementations against previous ones,
demonstrating comparable or superior performance. This work aims to establish a
cohesive foundation for multi-modal modelling, serving as a valuable
educational resource in the field.",0
"Fast, accurate and lightweight sequential simulation-based inference using Gaussian locally linear mappings",2403.07454v1,http://arxiv.org/abs/2403.07454v1,2024-03-12 09:48:17+00:00,"Bayesian inference for complex models with an intractable likelihood can be
tackled using algorithms performing many calls to computer simulators. These
approaches are collectively known as ""simulation-based inference"" (SBI). Recent
SBI methods have made use of neural networks (NN) to provide approximate, yet
expressive constructs for the unavailable likelihood function and the posterior
distribution. However, they do not generally achieve an optimal trade-off
between accuracy and computational demand. In this work, we propose an
alternative that provides both approximations to the likelihood and the
posterior distribution, using structured mixtures of probability distributions.
Our approach produces accurate posterior inference when compared to
state-of-the-art NN-based SBI methods, while exhibiting a much smaller
computational footprint. We illustrate our results on several benchmark models
from the SBI literature.",0
Measuring Data Similarity for Efficient Federated Learning: A Feasibility Study,2403.07450v1,http://arxiv.org/abs/2403.07450v1,2024-03-12 09:43:27+00:00,"In multiple federated learning schemes, a random subset of clients sends in
each round their model updates to the server for aggregation. Although this
client selection strategy aims to reduce communication overhead, it remains
energy and computationally inefficient, especially when considering
resource-constrained devices as clients. This is because conventional random
client selection overlooks the content of exchanged information and falls short
of providing a mechanism to reduce the transmission of semantically redundant
data. To overcome this challenge, we propose clustering the clients with the
aid of similarity metrics, where a single client from each of the formed
clusters is selected in each round to participate in the federated training. To
evaluate our approach, we perform an extensive feasibility study considering
the use of nine statistical metrics in the clustering process. Simulation
results reveal that, when considering a scenario with high data heterogeneity
of clients, similarity-based clustering can reduce the number of required
rounds compared to the baseline random client selection. In addition, energy
consumption can be notably reduced from 23.93% to 41.61%, for those similarity
metrics with an equivalent number of clients per round as the baseline random
scheme.",0
Ab-initio variational wave functions for the time-dependent many-electron Schrödinger equation,2403.07447v1,http://arxiv.org/abs/2403.07447v1,2024-03-12 09:37:22+00:00,"Describing the dynamics of many-electron quantum systems is crucial for
applications such as predicting electronic structures in quantum chemistry, the
properties of condensed matter systems, and the behaviors of complex materials.
However, the real-time evolution of non-equilibrium quantum electronic systems
poses a significant challenge for theoretical and computational approaches, due
to the system's exploration of a vast configuration space. This work introduces
a variational approach for fermionic time-dependent wave functions, surpassing
mean-field approximations by capturing many-body correlations. The proposed
methodology involves parameterizing the time-evolving quantum state, enabling
the approximation of the state's evolution. To account for electron
correlations, we employ time-dependent Jastrow factors and backflow
transformations. We also show that we can incorporate neural networks to
parameterize these functions. The time-dependent variational Monte Carlo
technique is employed to efficiently compute the optimal time-dependent
parameters. The approach is demonstrated in three distinct systems: the
solvable harmonic interaction model, the dynamics of a diatomic molecule in
intense laser fields, and a quenched quantum dot. In all cases, we show clear
signatures of many-body correlations in the dynamics not captured by mean-field
methods. The results showcase the ability of our variational approach to
accurately capture the time evolution of quantum states, providing insight into
the quantum dynamics of interacting electronic systems, beyond the capabilities
of mean-field.",0
Unsupervised self-organising map of prostate cell Raman spectra shows disease-state subclustering,2403.07960v1,http://arxiv.org/abs/2403.07960v1,2024-03-12 09:37:20+00:00,"Prostate cancer is a disease which poses an interesting clinical question:
should it be treated? A small subset of prostate cancers are aggressive and
require removal and treatment to prevent metastatic spread. However,
conventional diagnostics remain challenged to risk-stratify such patients,
hence, new methods of approach to biomolecularly subclassify the disease are
needed. Here we use an unsupervised, self-organising map approach to analyse
live-cell Raman spectroscopy data obtained from prostate cell-lines; our aim is
to test the feasibility of this method to differentiate, at the
single-cell-level, cancer from normal using high-dimensional datasets with
minimal preprocessing. The results demonstrate not only successful separation
of normal prostate and cancer cells, but also a new subclustering of the
prostate cancer cell-line into two groups. Initial analysis of the spectra from
each of the cancer subclusters demonstrates a differential expression of
lipids, which, against the normal control, may be linked to disease-related
changes in cellular signalling.",0
A Survey on Federated Learning in Intelligent Transportation Systems,2403.07444v2,http://arxiv.org/abs/2403.07444v2,2024-03-12 09:33:18+00:00,"The development of Intelligent Transportation System (ITS) has brought about
comprehensive urban traffic information that not only provides convenience to
urban residents in their daily lives but also enhances the efficiency of urban
road usage, leading to a more harmonious and sustainable urban life. Typical
scenarios in ITS mainly include traffic flow prediction, traffic target
recognition, and vehicular edge computing. However, most current ITS
applications rely on a centralized training approach where users upload source
data to a cloud server with high computing power for management and centralized
training. This approach has limitations such as poor real-time performance,
data silos, and difficulty in guaranteeing data privacy. To address these
limitations, federated learning (FL) has been proposed as a promising solution.
In this paper, we present a comprehensive review of the application of FL in
ITS, with a particular focus on three key scenarios: traffic flow prediction,
traffic target recognition, and vehicular edge computing. For each scenario, we
provide an in-depth analysis of its key characteristics, current challenges,
and specific manners in which FL is leveraged. Moreover, we discuss the
benefits that FL can offer as a potential solution to the limitations of the
centralized training approach currently used in ITS applications.",0
Proxy Methods for Domain Adaptation,2403.07442v1,http://arxiv.org/abs/2403.07442v1,2024-03-12 09:32:41+00:00,"We study the problem of domain adaptation under distribution shift, where the
shift is due to a change in the distribution of an unobserved, latent variable
that confounds both the covariates and the labels. In this setting, neither the
covariate shift nor the label shift assumptions apply. Our approach to
adaptation employs proximal causal learning, a technique for estimating causal
effects in settings where proxies of unobserved confounders are available. We
demonstrate that proxy variables allow for adaptation to distribution shift
without explicitly recovering or modeling latent variables. We consider two
settings, (i) Concept Bottleneck: an additional ''concept'' variable is
observed that mediates the relationship between the covariates and labels; (ii)
Multi-domain: training data from multiple source domains is available, where
each source domain exhibits a different distribution over the latent
confounder. We develop a two-stage kernel estimation approach to adapt to
complex distribution shifts in both settings. In our experiments, we show that
our approach outperforms other methods, notably those which explicitly recover
the latent confounder.",0
DALSA: Domain Adaptation for Supervised Learning From Sparsely Annotated MR Images,2403.07434v1,http://arxiv.org/abs/2403.07434v1,2024-03-12 09:17:21+00:00,"We propose a new method that employs transfer learning techniques to
effectively correct sampling selection errors introduced by sparse annotations
during supervised learning for automated tumor segmentation. The practicality
of current learning-based automated tissue classification approaches is
severely impeded by their dependency on manually segmented training databases
that need to be recreated for each scenario of application, site, or
acquisition setup. The comprehensive annotation of reference datasets can be
highly labor-intensive, complex, and error-prone. The proposed method derives
high-quality classifiers for the different tissue classes from sparse and
unambiguous annotations and employs domain adaptation techniques for
effectively correcting sampling selection errors introduced by the sparse
sampling. The new approach is validated on labeled, multi-modal MR images of 19
patients with malignant gliomas and by comparative analysis on the BraTS 2013
challenge data sets. Compared to training on fully labeled data, we reduced the
time for labeling and training by a factor greater than 70 and 180 respectively
without sacrificing accuracy. This dramatically eases the establishment and
constant extension of large annotated databases in various scenarios and
imaging setups and thus represents an important step towards practical
applicability of learning-based approaches in tissue classification.",0
Knowledge Transfer across Multiple Principal Component Analysis Studies,2403.07431v1,http://arxiv.org/abs/2403.07431v1,2024-03-12 09:15:12+00:00,"Transfer learning has aroused great interest in the statistical community. In
this article, we focus on knowledge transfer for unsupervised learning tasks in
contrast to the supervised learning tasks in the literature. Given the
transferable source populations, we propose a two-step transfer learning
algorithm to extract useful information from multiple source principal
component analysis (PCA) studies, thereby enhancing estimation accuracy for the
target PCA task. In the first step, we integrate the shared subspace
information across multiple studies by a proposed method named as Grassmannian
barycenter, instead of directly performing PCA on the pooled dataset. The
proposed Grassmannian barycenter method enjoys robustness and computational
advantages in more general cases. Then the resulting estimator for the shared
subspace from the first step is further utilized to estimate the target private
subspace in the second step. Our theoretical analysis credits the gain of
knowledge transfer between PCA studies to the enlarged eigenvalue gap, which is
different from the existing supervised transfer learning tasks where sparsity
plays the central role. In addition, we prove that the bilinear forms of the
empirical spectral projectors have asymptotic normality under weaker eigenvalue
gap conditions after knowledge transfer. When the set of informativesources is
unknown, we endow our algorithm with the capability of useful dataset selection
by solving a rectified optimization problem on the Grassmann manifold, which in
turn leads to a computationally friendly rectified Grassmannian K-means
procedure. In the end, extensive numerical simulation results and a real data
case concerning activity recognition are reported to support our theoretical
claims and to illustrate the empirical usefulness of the proposed transfer
learning methods.",0
On the Generalization Ability of Unsupervised Pretraining,2403.06871v1,http://arxiv.org/abs/2403.06871v1,2024-03-11 16:23:42+00:00,"Recent advances in unsupervised learning have shown that unsupervised
pre-training, followed by fine-tuning, can improve model generalization.
However, a rigorous understanding of how the representation function learned on
an unlabeled dataset affects the generalization of the fine-tuned model is
lacking. Existing theoretical research does not adequately account for the
heterogeneity of the distribution and tasks in pre-training and fine-tuning
stage. To bridge this gap, this paper introduces a novel theoretical framework
that illuminates the critical factor influencing the transferability of
knowledge acquired during unsupervised pre-training to the subsequent
fine-tuning phase, ultimately affecting the generalization capabilities of the
fine-tuned model on downstream tasks. We apply our theoretical framework to
analyze generalization bound of two distinct scenarios: Context Encoder
pre-training with deep neural networks and Masked Autoencoder pre-training with
deep transformers, followed by fine-tuning on a binary classification task.
Finally, inspired by our findings, we propose a novel regularization method
during pre-training to further enhances the generalization of fine-tuned model.
Overall, our results contribute to a better understanding of unsupervised
pre-training and fine-tuning paradigm, and can shed light on the design of more
effective pre-training algorithms.",0
Semantic Residual Prompts for Continual Learning,2403.06870v2,http://arxiv.org/abs/2403.06870v2,2024-03-11 16:23:38+00:00,"Prompt-tuning methods for Continual Learning (CL) freeze a large pre-trained
model and focus training on a few parameter vectors termed prompts. Most of
these methods organize these vectors in a pool of key-value pairs, and use the
input image as query to retrieve the prompts (values). However, as keys are
learned while tasks progress, the prompting selection strategy is itself
subject to catastrophic forgetting, an issue often overlooked by existing
approaches. For instance, prompts introduced to accommodate new tasks might end
up interfering with previously learned prompts. To make the selection strategy
more stable, we ask a foundational model (CLIP) to select our prompt within a
two-level adaptation mechanism. Specifically, the first level leverages
standard textual prompts for the CLIP textual encoder, leading to stable class
prototypes. The second level, instead, uses these prototypes along with the
query image as keys to index a second pool. The retrieved prompts serve to
adapt a pre-trained ViT, granting plasticity. In doing so, we also propose a
novel residual mechanism to transfer CLIP semantics to the ViT layers. Through
extensive analysis on established CL benchmarks, we show that our method
significantly outperforms both state-of-the-art CL approaches and the zero-shot
CLIP test. Notably, our findings hold true even for datasets with a substantial
domain gap w.r.t. the pre-training knowledge of the backbone model, as
showcased by experiments on satellite imagery and medical datasets.",0
Learning with Noisy Foundation Models,2403.06869v1,http://arxiv.org/abs/2403.06869v1,2024-03-11 16:22:41+00:00,"Foundation models are usually pre-trained on large-scale datasets and then
adapted to downstream tasks through tuning. However, the large-scale
pre-training datasets, often inaccessible or too expensive to handle, can
contain label noise that may adversely affect the generalization of the model
and pose unexpected risks. This paper stands out as the first work to
comprehensively understand and analyze the nature of noise in pre-training
datasets and then effectively mitigate its impacts on downstream tasks.
Specifically, through extensive experiments of fully-supervised and image-text
contrastive pre-training on synthetic noisy ImageNet-1K, YFCC15M, and CC12M
datasets, we demonstrate that, while slight noise in pre-training can benefit
in-domain (ID) performance, where the training and testing data share a similar
distribution, it always deteriorates out-of-domain (OOD) performance, where
training and testing distributions are significantly different. These
observations are agnostic to scales of pre-training datasets, pre-training
noise types, model architectures, pre-training objectives, downstream tuning
methods, and downstream applications. We empirically ascertain that the reason
behind this is that the pre-training noise shapes the feature space
differently. We then propose a tuning method (NMTune) to affine the feature
space to mitigate the malignant effect of noise and improve generalization,
which is applicable in both parameter-efficient and black-box tuning manners.
We additionally conduct extensive experiments on popular vision and language
models, including APIs, which are supervised and self-supervised pre-trained on
realistic noisy data for evaluation. Our analysis and results demonstrate the
importance of this novel and fundamental research direction, which we term as
Noisy Model Learning.",0
Almost Optimal Agnostic Control of Unknown Linear Dynamics,2403.06320v1,http://arxiv.org/abs/2403.06320v1,2024-03-10 21:41:38+00:00,"We consider a simple control problem in which the underlying dynamics depend
on a parameter $a$ that is unknown and must be learned. We study three variants
of the control problem: Bayesian control, in which we have a prior belief about
$a$; bounded agnostic control, in which we have no prior belief about $a$ but
we assume that $a$ belongs to a bounded set; and fully agnostic control, in
which $a$ is allowed to be an arbitrary real number about which we have no
prior belief. In the Bayesian variant, a control strategy is optimal if it
minimizes a certain expected cost. In the agnostic variants, a control strategy
is optimal if it minimizes a quantity called the worst-case regret. For the
Bayesian and bounded agnostic variants above, we produce optimal control
strategies. For the fully agnostic variant, we produce almost optimal control
strategies, i.e., for any $\varepsilon>0$ we produce a strategy that minimizes
the worst-case regret to within a multiplicative factor of $(1+\varepsilon)$.",0
FlowVQTalker: High-Quality Emotional Talking Face Generation through Normalizing Flow and Quantization,2403.06375v2,http://arxiv.org/abs/2403.06375v2,2024-03-11 01:58:04+00:00,"Generating emotional talking faces is a practical yet challenging endeavor.
To create a lifelike avatar, we draw upon two critical insights from a human
perspective: 1) The connection between audio and the non-deterministic facial
dynamics, encompassing expressions, blinks, poses, should exhibit synchronous
and one-to-many mapping. 2) Vibrant expressions are often accompanied by
emotion-aware high-definition (HD) textures and finely detailed teeth. However,
both aspects are frequently overlooked by existing methods. To this end, this
paper proposes using normalizing Flow and Vector-Quantization modeling to
produce emotional talking faces that satisfy both insights concurrently
(FlowVQTalker). Specifically, we develop a flow-based coefficient generator
that encodes the dynamics of facial emotion into a multi-emotion-class latent
space represented as a mixture distribution. The generation process commences
with random sampling from the modeled distribution, guided by the accompanying
audio, enabling both lip-synchronization and the uncertain nonverbal facial
cues generation. Furthermore, our designed vector-quantization image generator
treats the creation of expressive facial images as a code query task, utilizing
a learned codebook to provide rich, high-quality textures that enhance the
emotional perception of the results. Extensive experiments are conducted to
showcase the effectiveness of our approach.",0
FeatAug: Automatic Feature Augmentation From One-to-Many Relationship Tables,2403.06367v1,http://arxiv.org/abs/2403.06367v1,2024-03-11 01:44:14+00:00,"Feature augmentation from one-to-many relationship tables is a critical but
challenging problem in ML model development. To augment good features, data
scientists need to come up with SQL queries manually, which is time-consuming.
Featuretools [1] is a widely used tool by the data science community to
automatically augment the training data by extracting new features from
relevant tables. It represents each feature as a group-by aggregation SQL query
on relevant tables and can automatically generate these SQL queries. However,
it does not include predicates in these queries, which significantly limits its
application in many real-world scenarios. To overcome this limitation, we
propose FEATAUG, a new feature augmentation framework that automatically
extracts predicate-aware SQL queries from one-to-many relationship tables. This
extension is not trivial because considering predicates will exponentially
increase the number of candidate queries. As a result, the original
Featuretools framework, which materializes all candidate queries, will not work
and needs to be redesigned. We formally define the problem and model it as a
hyperparameter optimization problem. We discuss how the Bayesian Optimization
can be applied here and propose a novel warm-up strategy to optimize it. To
make our algorithm more practical, we also study how to identify promising
attribute combinations for predicates. We show that how the beam search idea
can partially solve the problem and propose several techniques to further
optimize it. Our experiments on four real-world datasets demonstrate that
FeatAug extracts more effective features compared to Featuretools and other
baselines. The code is open-sourced at https://github.com/sfu-db/FeatAug",0
Finite-Time Error Analysis of Soft Q-Learning: Switching System Approach,2403.06366v1,http://arxiv.org/abs/2403.06366v1,2024-03-11 01:36:37+00:00,"Soft Q-learning is a variation of Q-learning designed to solve entropy
regularized Markov decision problems where an agent aims to maximize the
entropy regularized value function. Despite its empirical success, there have
been limited theoretical studies of soft Q-learning to date. This paper aims to
offer a novel and unified finite-time, control-theoretic analysis of soft
Q-learning algorithms. We focus on two types of soft Q-learning algorithms: one
utilizing the log-sum-exp operator and the other employing the Boltzmann
operator. By using dynamical switching system models, we derive novel
finite-time error bounds for both soft Q-learning algorithms. We hope that our
analysis will deepen the current understanding of soft Q-learning by
establishing connections with switching system models and may even pave the way
for new frameworks in the finite-time analysis of other reinforcement learning
algorithms.",0
Say Anything with Any Style,2403.06363v2,http://arxiv.org/abs/2403.06363v2,2024-03-11 01:20:03+00:00,"Generating stylized talking head with diverse head motions is crucial for
achieving natural-looking videos but still remains challenging. Previous works
either adopt a regressive method to capture the speaking style, resulting in a
coarse style that is averaged across all training data, or employ a universal
network to synthesize videos with different styles which causes suboptimal
performance. To address these, we propose a novel dynamic-weight method, namely
Say Anything withAny Style (SAAS), which queries the discrete style
representation via a generative model with a learned style codebook.
Specifically, we develop a multi-task VQ-VAE that incorporates three closely
related tasks to learn a style codebook as a prior for style extraction. This
discrete prior, along with the generative model, enhances the precision and
robustness when extracting the speaking styles of the given style clips. By
utilizing the extracted style, a residual architecture comprising a canonical
branch and style-specific branch is employed to predict the mouth shapes
conditioned on any driving audio while transferring the speaking style from the
source to any desired one. To adapt to different speaking styles, we steer
clear of employing a universal network by exploring an elaborate HyperStyle to
produce the style-specific weights offset for the style branch. Furthermore, we
construct a pose generator and a pose codebook to store the quantized pose
representation, allowing us to sample diverse head motions aligned with the
audio and the extracted style. Experiments demonstrate that our approach
surpasses state-of-theart methods in terms of both lip-synchronization and
stylized expression. Besides, we extend our SAAS to video-driven style editing
field and achieve satisfactory performance.",0
See Through Their Minds: Learning Transferable Neural Representation from Cross-Subject fMRI,2403.06361v1,http://arxiv.org/abs/2403.06361v1,2024-03-11 01:18:49+00:00,"Deciphering visual content from functional Magnetic Resonance Imaging (fMRI)
helps illuminate the human vision system. However, the scarcity of fMRI data
and noise hamper brain decoding model performance. Previous approaches
primarily employ subject-specific models, sensitive to training sample size. In
this paper, we explore a straightforward but overlooked solution to address
data scarcity. We propose shallow subject-specific adapters to map
cross-subject fMRI data into unified representations. Subsequently, a shared
deeper decoding model decodes cross-subject features into the target feature
space. During training, we leverage both visual and textual supervision for
multi-modal brain decoding. Our model integrates a high-level perception
decoding pipeline and a pixel-wise reconstruction pipeline guided by high-level
perceptions, simulating bottom-up and top-down processes in neuroscience.
Empirical experiments demonstrate robust neural representation learning across
subjects for both pipelines. Moreover, merging high-level and low-level
information improves both low-level and high-level reconstruction metrics.
Additionally, we successfully transfer learned general knowledge to new
subjects by training new adapters with limited training data. Compared to
previous state-of-the-art methods, notably pre-training-based methods (Mind-Vis
and fMRI-PTE), our approach achieves comparable or superior results across
diverse tasks, showing promise as an alternative method for cross-subject fMRI
data pre-training. Our code and pre-trained weights will be publicly released
at https://github.com/YulongBonjour/See_Through_Their_Minds.",0
Multi-modal Semantic Understanding with Contrastive Cross-modal Feature Alignment,2403.06355v1,http://arxiv.org/abs/2403.06355v1,2024-03-11 01:07:36+00:00,"Multi-modal semantic understanding requires integrating information from
different modalities to extract users' real intention behind words. Most
previous work applies a dual-encoder structure to separately encode image and
text, but fails to learn cross-modal feature alignment, making it hard to
achieve cross-modal deep information interaction. This paper proposes a novel
CLIP-guided contrastive-learning-based architecture to perform multi-modal
feature alignment, which projects the features derived from different
modalities into a unified deep space. On multi-modal sarcasm detection (MMSD)
and multi-modal sentiment analysis (MMSA) tasks, the experimental results show
that our proposed model significantly outperforms several baselines, and our
feature alignment strategy brings obvious performance gain over models with
different aggregating methods and models even enriched with knowledge. More
importantly, our model is simple to implement without using task-specific
external knowledge, and thus can easily migrate to other multi-modal tasks. Our
source codes are available at https://github.com/ChangKe123/CLFA.",0
Separable Physics-informed Neural Networks for Solving the BGK Model of the Boltzmann Equation,2403.06342v1,http://arxiv.org/abs/2403.06342v1,2024-03-10 23:44:55+00:00,"In this study, we introduce a method based on Separable Physics-Informed
Neural Networks (SPINNs) for effectively solving the BGK model of the Boltzmann
equation. While the mesh-free nature of PINNs offers significant advantages in
handling high-dimensional partial differential equations (PDEs), challenges
arise when applying quadrature rules for accurate integral evaluation in the
BGK operator, which can compromise the mesh-free benefit and increase
computational costs. To address this, we leverage the canonical polyadic
decomposition structure of SPINNs and the linear nature of moment calculation,
achieving a substantial reduction in computational expense for quadrature rule
application. The multi-scale nature of the particle density function poses
difficulties in precisely approximating macroscopic moments using neural
networks. To improve SPINN training, we introduce the integration of Gaussian
functions into SPINNs, coupled with a relative loss approach. This modification
enables SPINNs to decay as rapidly as Maxwellian distributions, thereby
enhancing the accuracy of macroscopic moment approximations. The relative loss
design further ensures that both large and small-scale features are effectively
captured by the SPINNs. The efficacy of our approach is demonstrated through a
series of five numerical experiments, including the solution to a challenging
3D Riemann problem. These results highlight the potential of our novel method
in efficiently and accurately addressing complex challenges in computational
physics.",0
Disentangling shared and private latent factors in multimodal Variational Autoencoders,2403.06338v1,http://arxiv.org/abs/2403.06338v1,2024-03-10 23:11:05+00:00,"Generative models for multimodal data permit the identification of latent
factors that may be associated with important determinants of observed data
heterogeneity. Common or shared factors could be important for explaining
variation across modalities whereas other factors may be private and important
only for the explanation of a single modality. Multimodal Variational
Autoencoders, such as MVAE and MMVAE, are a natural choice for inferring those
underlying latent factors and separating shared variation from private. In this
work, we investigate their capability to reliably perform this disentanglement.
In particular, we highlight a challenging problem setting where
modality-specific variation dominates the shared signal. Taking a cross-modal
prediction perspective, we demonstrate limitations of existing models, and
propose a modification how to make them more robust to modality-specific
variation. Our findings are supported by experiments on synthetic as well as
various real-world multi-omics data sets.",0
Active Learning for Rapid Targeted Synthesis of Compositionally Complex Alloys,2403.06329v1,http://arxiv.org/abs/2403.06329v1,2024-03-10 22:28:54+00:00,"The next generation of advanced materials is tending toward increasingly
complex compositions. Synthesizing precise composition is time-consuming and
becomes exponentially demanding with increasing compositional complexity. An
experienced human operator does significantly better than a beginner but still
struggles to consistently achieve precision when synthesis parameters are
coupled. The time to optimize synthesis becomes a barrier to exploring
scientifically and technologically exciting compositionally complex materials.
This investigation demonstrates an Active Learning (AL) approach for optimizing
physical vapor deposition synthesis of thin-film alloys with up to five
principal elements. We compared AL based on Gaussian Process (GP) and Random
Forest (RF) models. The best performing models were able to discover synthesis
parameters for a target quinary alloy in 14 iterations. We also demonstrate the
capability of these models to be used in transfer learning tasks. RF and GP
models trained on lower dimensional systems (i.e. ternary, quarternary) show an
immediate improvement in prediction accuracy compared to models trained only on
quinary samples. Furthermore, samples that only share a few elements in common
with the target composition can be used for model pre-training. We believe that
such AL approaches can be widely adapted to significantly accelerate the
exploration of compositionally complex materials.",0
Transferable Reinforcement Learning via Generalized Occupancy Models,2403.06328v1,http://arxiv.org/abs/2403.06328v1,2024-03-10 22:27:21+00:00,"Intelligent agents must be generalists - showing the ability to quickly adapt
and generalize to varying tasks. Within the framework of reinforcement learning
(RL), model-based RL algorithms learn a task-agnostic dynamics model of the
world, in principle allowing them to generalize to arbitrary rewards. However,
one-step models naturally suffer from compounding errors, making them
ineffective for problems with long horizons and large state spaces. In this
work, we propose a novel class of models - generalized occupancy models (GOMs)
- that retain the generality of model-based RL while avoiding compounding
error. The key idea behind GOMs is to model the distribution of all possible
long-term outcomes from a given state under the coverage of a stationary
dataset, along with a policy that realizes a particular outcome from the given
state. These models can then quickly be used to select the optimal action for
arbitrary new tasks, without having to redo policy optimization. By directly
modeling long-term outcomes, GOMs avoid compounding error while retaining
generality across arbitrary reward functions. We provide a practical
instantiation of GOMs using diffusion models and show its efficacy as a new
class of transferable models, both theoretically and empirically across a
variety of simulated robotics problems. Videos and code at
https://weirdlabuw.github.io/gom/.",0
From Instructions to Constraints: Language Model Alignment with Automatic Constraint Verification,2403.06326v1,http://arxiv.org/abs/2403.06326v1,2024-03-10 22:14:54+00:00,"User alignment is crucial for adapting general-purpose language models (LMs)
to downstream tasks, but human annotations are often not available for all
types of instructions, especially those with customized constraints. We observe
that user instructions typically contain constraints. While assessing response
quality in terms of the whole instruction is often costly, efficiently
evaluating the satisfaction rate of constraints is feasible. We investigate
common constraints in NLP tasks, categorize them into three classes based on
the types of their arguments, and propose a unified framework, ACT (Aligning to
ConsTraints), to automatically produce supervision signals for user alignment
with constraints. Specifically, ACT uses constraint verifiers, which are
typically easy to implement in practice, to compute constraint satisfaction
rate (CSR) of each response. It samples multiple responses for each prompt and
collect preference labels based on their CSR automatically. Subsequently, ACT
adapts the LM to the target task through a ranking-based learning process.
Experiments on fine-grained entity typing, abstractive summarization, and
temporal question answering show that ACT is able to enhance LMs' capability to
adhere to different classes of constraints, thereby improving task performance.
Further experiments show that the constraint-following capabilities are
transferable.",0
ACM MMSys 2024 Bandwidth Estimation in Real Time Communications Challenge,2403.06324v1,http://arxiv.org/abs/2403.06324v1,2024-03-10 21:53:42+00:00,"The quality of experience (QoE) delivered by video conferencing systems to
end users depends in part on correctly estimating the capacity of the
bottleneck link between the sender and the receiver over time. Bandwidth
estimation for real-time communications (RTC) remains a significant challenge,
primarily due to the continuously evolving heterogeneous network architectures
and technologies. From the first bandwidth estimation challenge which was
hosted at ACM MMSys 2021, we learnt that bandwidth estimation models trained
with reinforcement learning (RL) in simulations to maximize network-based
reward functions may not be optimal in reality due to the sim-to-real gap and
the difficulty of aligning network-based rewards with user-perceived QoE. This
grand challenge aims to advance bandwidth estimation model design by aligning
reward maximization with user-perceived QoE optimization using offline RL and a
real-world dataset with objective rewards which have high correlations with
subjective user-perceived audio/video quality in Microsoft Teams. All models
submitted to the grand challenge underwent initial evaluation on our emulation
platform. For a comprehensive evaluation under diverse network conditions with
temporal fluctuations, top models were further evaluated on our geographically
distributed testbed by using each model to conduct 600 calls within a 12-day
period. The winning model is shown to deliver comparable performance to the top
behavior policy in the released dataset. By leveraging real-world data and
integrating objective audio/video quality scores as rewards, offline RL can
therefore facilitate the development of competitive bandwidth estimators for
RTC.",0
Risk-Sensitive RL with Optimized Certainty Equivalents via Reduction to Standard RL,2403.06323v1,http://arxiv.org/abs/2403.06323v1,2024-03-10 21:45:12+00:00,"We study Risk-Sensitive Reinforcement Learning (RSRL) with the Optimized
Certainty Equivalent (OCE) risk, which generalizes Conditional Value-at-risk
(CVaR), entropic risk and Markowitz's mean-variance. Using an augmented Markov
Decision Process (MDP), we propose two general meta-algorithms via reductions
to standard RL: one based on optimistic algorithms and another based on policy
optimization. Our optimistic meta-algorithm generalizes almost all prior RSRL
theory with entropic risk or CVaR. Under discrete rewards, our optimistic
theory also certifies the first RSRL regret bounds for MDPs with bounded
coverability, e.g., exogenous block MDPs. Under discrete rewards, our policy
optimization meta-algorithm enjoys both global convergence and local
improvement guarantees in a novel metric that lower bounds the true OCE risk.
Finally, we instantiate our framework with PPO, construct an MDP, and show that
it learns the optimal risk-sensitive policy while prior algorithms provably
fail.",0
Fake or Compromised? Making Sense of Malicious Clients in Federated Learning,2403.06319v1,http://arxiv.org/abs/2403.06319v1,2024-03-10 21:37:21+00:00,"Federated learning (FL) is a distributed machine learning paradigm that
enables training models on decentralized data. The field of FL security against
poisoning attacks is plagued with confusion due to the proliferation of
research that makes different assumptions about the capabilities of adversaries
and the adversary models they operate under. Our work aims to clarify this
confusion by presenting a comprehensive analysis of the various poisoning
attacks and defensive aggregation rules (AGRs) proposed in the literature, and
connecting them under a common framework. To connect existing adversary models,
we present a hybrid adversary model, which lies in the middle of the spectrum
of adversaries, where the adversary compromises a few clients, trains a
generative (e.g., DDPM) model with their compromised samples, and generates new
synthetic data to solve an optimization for a stronger (e.g., cheaper, more
practical) attack against different robust aggregation rules. By presenting the
spectrum of FL adversaries, we aim to provide practitioners and researchers
with a clear understanding of the different types of threats they need to
consider when designing FL systems, and identify areas where further research
is needed.",0
A Survey of Learned Indexes for the Multi-dimensional Space,2403.06456v1,http://arxiv.org/abs/2403.06456v1,2024-03-11 06:32:32+00:00,"A recent research trend involves treating database index structures as
Machine Learning (ML) models. In this domain, single or multiple ML models are
trained to learn the mapping from keys to positions inside a data set. This
class of indexes is known as ""Learned Indexes."" Learned indexes have
demonstrated improved search performance and reduced space requirements for
one-dimensional data. The concept of one-dimensional learned indexes has
naturally been extended to multi-dimensional (e.g., spatial) data, leading to
the development of ""Learned Multi-dimensional Indexes"". This survey focuses on
learned multi-dimensional index structures. Specifically, it reviews the
current state of this research area, explains the core concepts behind each
proposed method, and classifies these methods based on several well-defined
criteria. We present a taxonomy that classifies and categorizes each learned
multi-dimensional index, and survey the existing literature on learned
multi-dimensional indexes according to this taxonomy. Additionally, we present
a timeline to illustrate the evolution of research on learned indexes. Finally,
we highlight several open challenges and future research directions in this
emerging and highly active field.",0
An End-to-End Deep Learning Generative Framework for Refinable Shape Matching and Generation,2403.06317v1,http://arxiv.org/abs/2403.06317v1,2024-03-10 21:33:53+00:00,"Generative modelling for shapes is a prerequisite for In-Silico Clinical
Trials (ISCTs), which aim to cost-effectively validate medical device
interventions using synthetic anatomical shapes, often represented as 3D
surface meshes. However, constructing AI models to generate shapes closely
resembling the real mesh samples is challenging due to variable vertex counts,
connectivities, and the lack of dense vertex-wise correspondences across the
training data. Employing graph representations for meshes, we develop a novel
unsupervised geometric deep-learning model to establish refinable shape
correspondences in a latent space, construct a population-derived atlas and
generate realistic synthetic shapes. We additionally extend our proposed base
model to a joint shape generative-clustering multi-atlas framework to
incorporate further variability and preserve more details in the generated
shapes. Experimental results using liver and left-ventricular models
demonstrate the approach's applicability to computational medicine,
highlighting its suitability for ISCTs through a comparative analysis.",0
A Study on Domain Generalization for Failure Detection through Human Reactions in HRI,2403.06315v1,http://arxiv.org/abs/2403.06315v1,2024-03-10 21:30:22+00:00,"Machine learning models are commonly tested in-distribution (same dataset);
performance almost always drops in out-of-distribution settings. For HRI
research, the goal is often to develop generalized models. This makes domain
generalization - retaining performance in different settings - a critical
issue. In this study, we present a concise analysis of domain generalization in
failure detection models trained on human facial expressions. Using two
distinct datasets of humans reacting to videos where error occurs, one from a
controlled lab setting and another collected online, we trained deep learning
models on each dataset. When testing these models on the alternate dataset, we
observed a significant performance drop. We reflect on the causes for the
observed model behavior and leave recommendations. This work emphasizes the
need for HRI research focusing on improving model robustness and real-life
applicability.",0
Optimal Policy Sparsification and Low Rank Decomposition for Deep Reinforcement Learning,2403.06313v1,http://arxiv.org/abs/2403.06313v1,2024-03-10 21:18:54+00:00,"Deep reinforcement learning(DRL) has shown significant promise in a wide
range of applications including computer games and robotics. Yet, training DRL
policies consume extraordinary computing resources resulting in dense policies
which are prone to overfitting. Moreover, inference with dense DRL policies
limit their practical applications, especially in edge computing. Techniques
such as pruning and singular value decomposition have been used with deep
learning models to achieve sparsification and model compression to limit
overfitting and reduce memory consumption. However, these techniques resulted
in sub-optimal performance with notable decay in rewards. $L_1$ and $L_2$
regularization techniques have been proposed for neural network sparsification
and sparse auto-encoder development, but their implementation in DRL
environments has not been apparent. We propose a novel
$L_0$-norm-regularization technique using an optimal sparsity map to sparsify
DRL policies and promote their decomposition to a lower rank without decay in
rewards. We evaluated our $L_0$-norm-regularization technique across five
different environments (Cartpole-v1, Acrobat-v1, LunarLander-v2,
SuperMarioBros-7.1.v0 and Surgical Robot Learning) using several on-policy and
off-policy algorithms. We demonstrated that the $L_0$-norm-regularized DRL
policy in the SuperMarioBros environment achieved 93% sparsity and gained 70%
compression when subjected to low-rank decomposition, while significantly
outperforming the dense policy. Additionally, the $L_0$-norm-regularized DRL
policy in the Surgical Robot Learning environment achieved a 36% sparsification
and gained 46% compression when decomposed to a lower rank, while being
performant. The results suggest that our custom $L_0$-norm-regularization
technique for sparsification of DRL policies is a promising avenue to reduce
computational resources and limit overfitting.",0
How much data do you need? Part 2: Predicting DL class specific training dataset sizes,2403.06311v1,http://arxiv.org/abs/2403.06311v1,2024-03-10 21:08:29+00:00,"This paper targets the question of predicting machine learning classification
model performance, when taking into account the number of training examples per
class and not just the overall number of training examples. This leads to the a
combinatorial question, which combinations of number of training examples per
class should be considered, given a fixed overall training dataset size. In
order to solve this question, an algorithm is suggested which is motivated from
special cases of space filling design of experiments. The resulting data are
modeled using models like powerlaw curves and similar models, extended like
generalized linear models i.e. by replacing the overall training dataset size
by a parametrized linear combination of the number of training examples per
label class. The proposed algorithm has been applied on the CIFAR10 and the
EMNIST datasets.",0
Nonparametric Automatic Differentiation Variational Inference with Spline Approximation,2403.06302v1,http://arxiv.org/abs/2403.06302v1,2024-03-10 20:22:06+00:00,"Automatic Differentiation Variational Inference (ADVI) is efficient in
learning probabilistic models. Classic ADVI relies on the parametric approach
to approximate the posterior. In this paper, we develop a spline-based
nonparametric approximation approach that enables flexible posterior
approximation for distributions with complicated structures, such as skewness,
multimodality, and bounded support. Compared with widely-used nonparametric
variational inference methods, the proposed method is easy to implement and
adaptive to various data structures. By adopting the spline approximation, we
derive a lower bound of the importance weighted autoencoder and establish the
asymptotic consistency. Experiments demonstrate the efficiency of the proposed
method in approximating complex posterior distributions and improving the
performance of generative models with incomplete data.",0
Cross-ecosystem categorization: A manual-curation protocol for the categorization of Java Maven libraries along Python PyPI Topics,2403.06300v1,http://arxiv.org/abs/2403.06300v1,2024-03-10 20:15:08+00:00,"Context: Software of different functional categories, such as text processing
vs. networking, has different profiles in terms of metrics like security and
updates. Using popularity to compare e.g. Java vs. Python libraries might give
a skewed perspective, as the categories of the most popular software vary from
one ecosystem to the next. How can one compare libraries datasets across
software ecosystems, when not even the category names are uniform among them?
Objective: We study how to generate a language-agnostic categorisation of
software by functional purpose, that enables cross-ecosystem studies of
libraries datasets. This provides the functional fingerprint information needed
for software metrics comparisons. Method: We designed and implemented a
human-guided protocol to categorise libraries from software ecosystems.
Category names mirror PyPI Topic classifiers, but the protocol is generic and
can be applied to any ecosystem. We demonstrate it by categorising 256
Java/Maven libraries with severe security vulnerabilities. Results: The
protocol allows three or more people to categorise any number of libraries. The
categorisation produced is functional-oriented and language-agnostic. The
Java/Maven dataset demonstration resulted in a majority of Internet-oriented
libraries, coherent with its selection by severe vulnerabilities. To allow
replication and updates, we make the dataset and the protocol individual steps
available as open data. Conclusions: Libraries categorisation by functional
purpose is feasible with our protocol, which produced the fingerprint of a
256-libraries Java dataset. While this was labour intensive, humans excel in
the required inference tasks, so full automation of the process is not
envisioned. However, results can provide the ground truth needed for machine
learning in large-scale cross-ecosystem empirical studies.",0
"Disentangling Resilience from Robustness: Contextual Dualism, Interactionism, and Game-Theoretic Paradigms",2403.06299v1,http://arxiv.org/abs/2403.06299v1,2024-03-10 20:09:23+00:00,"This article explains the distinctions between robustness and resilience in
control systems. Resilience confronts a distinct set of challenges, posing new
ones for designing controllers for feedback systems, networks, and machines
that prioritize resilience over robustness. The concept of resilience is
explored through a three-stage model, emphasizing the need for a proactive
preparation and automated response to elastic events. A toy model is first used
to illustrate the tradeoffs between resilience and robustness. Then, it delves
into contextual dualism and interactionism, and introduces game-theoretic
paradigms as a unifying framework to consolidate resilience and robustness. The
article concludes by discussing the interplay between robustness and
resilience, suggesting that a comprehensive theory of resilience and
quantification metrics, and formalization through game-theoretic frameworks are
necessary. The exploration extends to system-of-systems resilience and various
mechanisms, including the integration of AI techniques and non-technical
solutions, like cyber insurance, to achieve comprehensive resilience in control
systems. As we approach 2030, the systems and control community is at the
opportune moment to lay scientific foundations of resilience by bridging
feedback control theory, game theory, and learning theory. Resilient control
systems will enhance overall quality of life, enable the development of a
resilient society, and create a societal-scale impact amid global challenges
such as climate change, conflicts, and cyber insecurity.",0
Analysis of Total Variation Minimization for Clustered Federated Learning,2403.06298v1,http://arxiv.org/abs/2403.06298v1,2024-03-10 20:07:14+00:00,"A key challenge in federated learning applications is the statistical
heterogeneity of local datasets. Clustered federated learning addresses this
challenge by identifying clusters of local datasets that are approximately
homogeneous. One recent approach to clustered federated learning is generalized
total variation minimization (GTVMin). This approach requires a similarity
graph which can be obtained by domain expertise or in a data-driven fashion via
graph learning techniques. Under a widely applicable clustering assumption, we
derive an upper bound the deviation between GTVMin solutions and their
cluster-wise averages. This bound provides valuable insights into the
effectiveness and robustness of GTVMin in addressing statistical heterogeneity
within federated learning environments.",0
A streamlined Approach to Multimodal Few-Shot Class Incremental Learning for Fine-Grained Datasets,2403.06295v1,http://arxiv.org/abs/2403.06295v1,2024-03-10 19:50:03+00:00,"Few-shot Class-Incremental Learning (FSCIL) poses the challenge of retaining
prior knowledge while learning from limited new data streams, all without
overfitting. The rise of Vision-Language models (VLMs) has unlocked numerous
applications, leveraging their existing knowledge to fine-tune on custom data.
However, training the whole model is computationally prohibitive, and VLMs
while being versatile in general domains still struggle with fine-grained
datasets crucial for many applications. We tackle these challenges with two
proposed simple modules. The first, Session-Specific Prompts (SSP), enhances
the separability of image-text embeddings across sessions. The second,
Hyperbolic distance, compresses representations of image-text pairs within the
same class while expanding those from different classes, leading to better
representations. Experimental results demonstrate an average 10-point increase
compared to baselines while requiring at least 8 times fewer trainable
parameters. This improvement is further underscored on our three newly
introduced fine-grained datasets.",0
Transformer based Multitask Learning for Image Captioning and Object Detection,2403.06292v1,http://arxiv.org/abs/2403.06292v1,2024-03-10 19:31:13+00:00,"In several real-world scenarios like autonomous navigation and mobility, to
obtain a better visual understanding of the surroundings, image captioning and
object detection play a crucial role. This work introduces a novel multitask
learning framework that combines image captioning and object detection into a
joint model. We propose TICOD, Transformer-based Image Captioning and Object
detection model for jointly training both tasks by combining the losses
obtained from image captioning and object detection networks. By leveraging
joint training, the model benefits from the complementary information shared
between the two tasks, leading to improved performance for image captioning.
Our approach utilizes a transformer-based architecture that enables end-to-end
network integration for image captioning and object detection and performs both
tasks jointly. We evaluate the effectiveness of our approach through
comprehensive experiments on the MS-COCO dataset. Our model outperforms the
baselines from image captioning literature by achieving a 3.65% improvement in
BERTScore.",0
FWin transformer for dengue prediction under climate and ocean influence,2403.07027v1,http://arxiv.org/abs/2403.07027v1,2024-03-10 19:20:55+00:00,"Dengue fever is one of the most deadly mosquito-born tropical infectious
diseases. Detailed long range forecast model is vital in controlling the spread
of disease and making mitigation efforts. In this study, we examine methods
used to forecast dengue cases for long range predictions. The dataset consists
of local climate/weather in addition to global climate indicators of Singapore
from 2000 to 2019. We utilize newly developed deep neural networks to learn the
intricate relationship between the features. The baseline models in this study
are in the class of recent transformers for long sequence forecasting tasks. We
found that a Fourier mixed window attention (FWin) based transformer performed
the best in terms of both the mean square error and the maximum absolute error
on the long range dengue forecast up to 60 weeks.",0
Understanding and Mitigating Human-Labelling Errors in Supervised Contrastive Learning,2403.06289v1,http://arxiv.org/abs/2403.06289v1,2024-03-10 19:05:12+00:00,"Human-annotated vision datasets inevitably contain a fraction of human
mislabelled examples. While the detrimental effects of such mislabelling on
supervised learning are well-researched, their influence on Supervised
Contrastive Learning (SCL) remains largely unexplored. In this paper, we show
that human-labelling errors not only differ significantly from synthetic label
errors, but also pose unique challenges in SCL, different to those in
traditional supervised learning methods. Specifically, our results indicate
they adversely impact the learning process in the ~99% of cases when they occur
as false positive samples. Existing noise-mitigating methods primarily focus on
synthetic label errors and tackle the unrealistic setting of very high
synthetic noise rates (40-80%), but they often underperform on common image
datasets due to overfitting. To address this issue, we introduce a novel SCL
objective with robustness to human-labelling errors, SCL-RHE. SCL-RHE is
designed to mitigate the effects of real-world mislabelled examples, typically
characterized by much lower noise rates (<5%). We demonstrate that SCL-RHE
consistently outperforms state-of-the-art representation learning and
noise-mitigating methods across various vision benchmarks, by offering improved
resilience against human-labelling errors.",0
Probing Image Compression For Class-Incremental Learning,2403.06288v1,http://arxiv.org/abs/2403.06288v1,2024-03-10 18:58:14+00:00,"Image compression emerges as a pivotal tool in the efficient handling and
transmission of digital images. Its ability to substantially reduce file size
not only facilitates enhanced data storage capacity but also potentially brings
advantages to the development of continual machine learning (ML) systems, which
learn new knowledge incrementally from sequential data. Continual ML systems
often rely on storing representative samples, also known as exemplars, within a
limited memory constraint to maintain the performance on previously learned
data. These methods are known as memory replay-based algorithms and have proven
effective at mitigating the detrimental effects of catastrophic forgetting.
Nonetheless, the limited memory buffer size often falls short of adequately
representing the entire data distribution. In this paper, we explore the use of
image compression as a strategy to enhance the buffer's capacity, thereby
increasing exemplar diversity. However, directly using compressed exemplars
introduces domain shift during continual ML, marked by a discrepancy between
compressed training data and uncompressed testing data. Additionally, it is
essential to determine the appropriate compression algorithm and select the
most effective rate for continual ML systems to balance the trade-off between
exemplar quality and quantity. To this end, we introduce a new framework to
incorporate image compression for continual ML including a pre-processing data
compression step and an efficient compression rate/algorithm selection method.
We conduct extensive experiments on CIFAR-100 and ImageNet datasets and show
that our method significantly improves image classification accuracy in
continual ML settings.",0
Eliminating Warping Shakes for Unsupervised Online Video Stitching,2403.06378v1,http://arxiv.org/abs/2403.06378v1,2024-03-11 02:05:31+00:00,"In this paper, we retarget video stitching to an emerging issue, named
warping shake, when extending image stitching to video stitching. It unveils
the temporal instability of warped content in non-overlapping regions, despite
image stitching having endeavored to preserve the natural structures.
Therefore, in most cases, even if the input videos to be stitched are stable,
the stitched video will inevitably cause undesired warping shakes and affect
the visual experience. To eliminate the shakes, we propose StabStitch to
simultaneously realize video stitching and video stabilization in a unified
unsupervised learning framework. Starting from the camera paths in video
stabilization, we first derive the expression of stitching trajectories in
video stitching by elaborately integrating spatial and temporal warps. Then a
warp smoothing model is presented to optimize them with a comprehensive
consideration regarding content alignment, trajectory smoothness, spatial
consistency, and online collaboration. To establish an evaluation benchmark and
train the learning framework, we build a video stitching dataset with a rich
diversity in camera motions and scenes. Compared with existing stitching
solutions, StabStitch exhibits significant superiority in scene robustness and
inference speed in addition to stitching and stabilization performance,
contributing to a robust and real-time online video stitching system. The code
and dataset will be available at https://github.com/nie-lang/StabStitch.",0
An Efficient Learning-based Solver Comparable to Metaheuristics for the Capacitated Arc Routing Problem,2403.07028v1,http://arxiv.org/abs/2403.07028v1,2024-03-11 02:17:42+00:00,"Recently, neural networks (NN) have made great strides in combinatorial
optimization. However, they face challenges when solving the capacitated arc
routing problem (CARP) which is to find the minimum-cost tour covering all
required edges on a graph, while within capacity constraints. In tackling CARP,
NN-based approaches tend to lag behind advanced metaheuristics, since they lack
directed arc modeling and efficient learning methods tailored for complex CARP.
In this paper, we introduce an NN-based solver to significantly narrow the gap
with advanced metaheuristics while exhibiting superior efficiency. First, we
propose the direction-aware attention model (DaAM) to incorporate
directionality into the embedding process, facilitating more effective
one-stage decision-making. Second, we design a supervised reinforcement
learning scheme that involves supervised pre-training to establish a robust
initial policy for subsequent reinforcement fine-tuning. It proves particularly
valuable for solving CARP that has a higher complexity than the node routing
problems (NRPs). Finally, a path optimization method is proposed to adjust the
depot return positions within the path generated by DaAM. Experiments
illustrate that our approach surpasses heuristics and achieves decision quality
comparable to state-of-the-art metaheuristics for the first time while
maintaining superior efficiency.",0
Pre-Trained Model Recommendation for Downstream Fine-tuning,2403.06382v1,http://arxiv.org/abs/2403.06382v1,2024-03-11 02:24:32+00:00,"As a fundamental problem in transfer learning, model selection aims to rank
off-the-shelf pre-trained models and select the most suitable one for the new
target task. Existing model selection techniques are often constrained in their
scope and tend to overlook the nuanced relationships between models and tasks.
In this paper, we present a pragmatic framework \textbf{Fennec}, delving into a
diverse, large-scale model repository while meticulously considering the
intricate connections between tasks and models. The key insight is to map all
models and historical tasks into a transfer-related subspace, where the
distance between model vectors and task vectors represents the magnitude of
transferability. A large vision model, as a proxy, infers a new task's
representation in the transfer space, thereby circumventing the computational
burden of extensive forward passes. We also investigate the impact of the
inherent inductive bias of models on transfer results and propose a novel
method called \textbf{archi2vec} to encode the intricate structures of models.
The transfer score is computed through straightforward vector arithmetic with a
time complexity of $\mathcal{O}(1)$. Finally, we make a substantial
contribution to the field by releasing a comprehensive benchmark. We validate
the effectiveness of our framework through rigorous testing on two benchmarks.
The benchmark and the code will be publicly available in the near future.",0
A Zero Trust Framework for Realization and Defense Against Generative AI Attacks in Power Grid,2403.06388v1,http://arxiv.org/abs/2403.06388v1,2024-03-11 02:47:21+00:00,"Understanding the potential of generative AI (GenAI)-based attacks on the
power grid is a fundamental challenge that must be addressed in order to
protect the power grid by realizing and validating risk in new attack vectors.
In this paper, a novel zero trust framework for a power grid supply chain
(PGSC) is proposed. This framework facilitates early detection of potential
GenAI-driven attack vectors (e.g., replay and protocol-type attacks),
assessment of tail risk-based stability measures, and mitigation of such
threats. First, a new zero trust system model of PGSC is designed and
formulated as a zero-trust problem that seeks to guarantee for a stable PGSC by
realizing and defending against GenAI-driven cyber attacks. Second, in which a
domain-specific generative adversarial networks (GAN)-based attack generation
mechanism is developed to create a new vulnerability cyberspace for further
understanding that threat. Third, tail-based risk realization metrics are
developed and implemented for quantifying the extreme risk of a potential
attack while leveraging a trust measurement approach for continuous validation.
Fourth, an ensemble learning-based bootstrap aggregation scheme is devised to
detect the attacks that are generating synthetic identities with convincing
user and distributed energy resources device profiles. Experimental results
show the efficacy of the proposed zero trust framework that achieves an
accuracy of 95.7% on attack vector generation, a risk measure of 9.61% for a
95% stable PGSC, and a 99% confidence in defense against GenAI-driven attack.",0
CoRAL: Collaborative Retrieval-Augmented Large Language Models Improve Long-tail Recommendation,2403.06447v1,http://arxiv.org/abs/2403.06447v1,2024-03-11 05:49:34+00:00,"The long-tail recommendation is a challenging task for traditional
recommender systems, due to data sparsity and data imbalance issues. The recent
development of large language models (LLMs) has shown their abilities in
complex reasoning, which can help to deduce users' preferences based on very
few previous interactions. However, since most LLM-based systems rely on items'
semantic meaning as the sole evidence for reasoning, the collaborative
information of user-item interactions is neglected, which can cause the LLM's
reasoning to be misaligned with task-specific collaborative information of the
dataset. To further align LLMs' reasoning to task-specific user-item
interaction knowledge, we introduce collaborative retrieval-augmented LLMs,
CoRAL, which directly incorporate collaborative evidence into the prompts.
Based on the retrieved user-item interactions, the LLM can analyze shared and
distinct preferences among users, and summarize the patterns indicating which
types of users would be attracted by certain items. The retrieved collaborative
evidence prompts the LLM to align its reasoning with the user-item interaction
patterns in the dataset. However, since the capacity of the input prompt is
limited, finding the minimally-sufficient collaborative information for
recommendation tasks can be challenging. We propose to find the optimal
interaction set through a sequential decision-making process and develop a
retrieval policy learned through a reinforcement learning (RL) framework,
CoRAL. Our experimental results show that CoRAL can significantly improve LLMs'
reasoning abilities on specific recommendation tasks. Our analysis also reveals
that CoRAL can more efficiently explore collaborative information through
reinforcement learning.",0
Interpreting What Typical Fault Signals Look Like via Prototype-matching,2403.07033v1,http://arxiv.org/abs/2403.07033v1,2024-03-11 05:47:07+00:00,"Neural networks, with powerful nonlinear mapping and classification
capabilities, are widely applied in mechanical fault diagnosis to ensure
safety. However, being typical black-box models, their application is limited
in high-reliability-required scenarios. To understand the classification logic
and explain what typical fault signals look like, the prototype matching
network (PMN) is proposed by combining the human-inherent prototype-matching
with autoencoder (AE). The PMN matches AE-extracted feature with each prototype
and selects the most similar prototype as the prediction result. It has three
interpreting paths on classification logic, fault prototypes, and matching
contributions. Conventional diagnosis and domain generalization experiments
demonstrate its competitive diagnostic performance and distinguished advantages
in representation learning. Besides, the learned typical fault signals (i.e.,
sample-level prototypes) showcase the ability for denoising and extracting
subtle key features that experts find challenging to capture. This ability
broadens human understanding and provides a promising solution from
interpretability research to AI-for-Science.",0
"Wide-Field, High-Resolution Reconstruction in Computational Multi-Aperture Miniscope Using a Fourier Neural Network",2403.06439v1,http://arxiv.org/abs/2403.06439v1,2024-03-11 05:15:19+00:00,"Traditional fluorescence microscopy is constrained by inherent trade-offs
among resolution, field-of-view, and system complexity. To navigate these
challenges, we introduce a simple and low-cost computational multi-aperture
miniature microscope, utilizing a microlens array for single-shot wide-field,
high-resolution imaging. Addressing the challenges posed by extensive view
multiplexing and non-local, shift-variant aberrations in this device, we
present SV-FourierNet, a novel multi-channel Fourier neural network.
SV-FourierNet facilitates high-resolution image reconstruction across the
entire imaging field through its learned global receptive field. We establish a
close relationship between the physical spatially-varying point-spread
functions and the network's learned effective receptive field. This ensures
that SV-FourierNet has effectively encapsulated the spatially-varying
aberrations in our system, and learned a physically meaningful function for
image reconstruction. Training of SV-FourierNet is conducted entirely on a
physics-based simulator. We showcase wide-field, high-resolution video
reconstructions on colonies of freely moving C. elegans and imaging of a mouse
brain section. Our computational multi-aperture miniature microscope, augmented
with SV-FourierNet, represents a major advancement in computational microscopy
and may find broad applications in biomedical research and other fields
requiring compact microscopy solutions.",0
STARFlow: Spatial Temporal Feature Re-embedding with Attentive Learning for Real-world Scene Flow,2403.07032v1,http://arxiv.org/abs/2403.07032v1,2024-03-11 04:56:10+00:00,"Scene flow prediction is a crucial underlying task in understanding dynamic
scenes as it offers fundamental motion information. However, contemporary scene
flow methods encounter three major challenges. Firstly, flow estimation solely
based on local receptive fields lacks long-dependency matching of point pairs.
To address this issue, we propose global attentive flow embedding to match
all-to-all point pairs in both feature space and Euclidean space, providing
global initialization before local refinement. Secondly, there are deformations
existing in non-rigid objects after warping, which leads to variations in the
spatiotemporal relation between the consecutive frames. For a more precise
estimation of residual flow, a spatial temporal feature re-embedding module is
devised to acquire the sequence features after deformation. Furthermore,
previous methods perform poor generalization due to the significant domain gap
between the synthesized and LiDAR-scanned datasets. We leverage novel domain
adaptive losses to effectively bridge the gap of motion inference from
synthetic to real-world. Experiments demonstrate that our approach achieves
state-of-the-art performance across various datasets, with particularly
outstanding results on real-world LiDAR-scanned datasets. Our code is available
at https://github.com/O-VIGIA/StarFlow.",0
Joint-Embedding Masked Autoencoder for Self-supervised Learning of Dynamic Functional Connectivity from the Human Brain,2403.06432v1,http://arxiv.org/abs/2403.06432v1,2024-03-11 04:49:41+00:00,"Graph Neural Networks (GNNs) have shown promise in learning dynamic
functional connectivity for distinguishing phenotypes from human brain
networks. However, obtaining extensive labeled clinical data for training is
often resource-intensive, making practical application difficult. Leveraging
unlabeled data thus becomes crucial for representation learning in a
label-scarce setting. Although generative self-supervised learning techniques,
especially masked autoencoders, have shown promising results in representation
learning in various domains, their application to dynamic graphs for dynamic
functional connectivity remains underexplored, facing challenges in capturing
high-level semantic representations. Here, we introduce the Spatio-Temporal
Joint Embedding Masked Autoencoder (ST-JEMA), drawing inspiration from the
Joint Embedding Predictive Architecture (JEPA) in computer vision. ST-JEMA
employs a JEPA-inspired strategy for reconstructing dynamic graphs, which
enables the learning of higher-level semantic representations considering
temporal perspectives, addressing the challenges in fMRI data representation
learning. Utilizing the large-scale UK Biobank dataset for self-supervised
learning, ST-JEMA shows exceptional representation learning performance on
dynamic functional connectivity demonstrating superiority over previous methods
in predicting phenotypes and psychiatric diagnoses across eight benchmark fMRI
datasets even with limited samples and effectiveness of temporal reconstruction
on missing data scenarios. These findings highlight the potential of our
approach as a robust representation learning method for leveraging label-scarce
fMRI data.",0
From Fitting Participation to Forging Relationships: The Art of Participatory ML,2403.06431v1,http://arxiv.org/abs/2403.06431v1,2024-03-11 04:44:34+00:00,"Participatory machine learning (ML) encourages the inclusion of end users and
people affected by ML systems in design and development processes. We
interviewed 18 participation brokers -- individuals who facilitate such
inclusion and transform the products of participants' labour into inputs for an
ML artefact or system -- across a range of organisational settings and project
locations. Our findings demonstrate the inherent challenges of integrating
messy contextual information generated through participation with the
structured data formats required by ML workflows and the uneven power dynamics
in project contexts. We advocate for evolution in the role of brokers to more
equitably balance value generated in Participatory ML projects for design and
development teams with value created for participants. To move beyond `fitting'
participation to existing processes and empower participants to envision
alternative futures through ML, brokers must become educators and advocates for
end users, while attending to frustration and dissent from indirect
stakeholders.",0
AS-FIBA: Adaptive Selective Frequency-Injection for Backdoor Attack on Deep Face Restoration,2403.06430v1,http://arxiv.org/abs/2403.06430v1,2024-03-11 04:44:26+00:00,"Deep learning-based face restoration models, increasingly prevalent in smart
devices, have become targets for sophisticated backdoor attacks. These attacks,
through subtle trigger injection into input face images, can lead to unexpected
restoration outcomes. Unlike conventional methods focused on classification
tasks, our approach introduces a unique degradation objective tailored for
attacking restoration models. Moreover, we propose the Adaptive Selective
Frequency Injection Backdoor Attack (AS-FIBA) framework, employing a neural
network for input-specific trigger generation in the frequency domain,
seamlessly blending triggers with benign images. This results in imperceptible
yet effective attacks, guiding restoration predictions towards subtly degraded
outputs rather than conspicuous targets. Extensive experiments demonstrate the
efficacy of the degradation objective on state-of-the-art face restoration
models. Additionally, it is notable that AS-FIBA can insert effective backdoors
that are more imperceptible than existing backdoor attack methods, including
WaNet, ISSBA, and FIBA.",0
A Differential Geometric View and Explainability of GNN on Evolving Graphs,2403.06425v1,http://arxiv.org/abs/2403.06425v1,2024-03-11 04:26:18+00:00,"Graphs are ubiquitous in social networks and biochemistry, where Graph Neural
Networks (GNN) are the state-of-the-art models for prediction. Graphs can be
evolving and it is vital to formally model and understand how a trained GNN
responds to graph evolution. We propose a smooth parameterization of the GNN
predicted distributions using axiomatic attribution, where the distributions
are on a low-dimensional manifold within a high-dimensional embedding space. We
exploit the differential geometric viewpoint to model distributional evolution
as smooth curves on the manifold. We reparameterize families of curves on the
manifold and design a convex optimization problem to find a unique curve that
concisely approximates the distributional evolution for human interpretation.
Extensive experiments on node classification, link prediction, and graph
classification tasks with evolving graphs demonstrate the better sparsity,
faithfulness, and intuitiveness of the proposed method over the
state-of-the-art methods.",0
Bridging Domains with Approximately Shared Features,2403.06424v1,http://arxiv.org/abs/2403.06424v1,2024-03-11 04:25:41+00:00,"Multi-source domain adaptation aims to reduce performance degradation when
applying machine learning models to unseen domains. A fundamental challenge is
devising the optimal strategy for feature selection. Existing literature is
somewhat paradoxical: some advocate for learning invariant features from source
domains, while others favor more diverse features. To address the challenge, we
propose a statistical framework that distinguishes the utilities of features
based on the variance of their correlation to label $y$ across domains. Under
our framework, we design and analyze a learning procedure consisting of
learning approximately shared feature representation from source tasks and
fine-tuning it on the target task. Our theoretical analysis necessitates the
importance of learning approximately shared features instead of only the
strictly invariant features and yields an improved population risk compared to
previous results on both source and target tasks, thus partly resolving the
paradox mentioned above. Inspired by our theory, we proposed a more practical
way to isolate the content (invariant+approximately shared) from environmental
features and further consolidate our theoretical findings.",0
The Cram Method for Efficient Simultaneous Learning and Evaluation,2403.07031v1,http://arxiv.org/abs/2403.07031v1,2024-03-11 04:19:05+00:00,"We introduce the ""cram"" method, a general and efficient approach to
simultaneous learning and evaluation using a generic machine learning (ML)
algorithm. In a single pass of batched data, the proposed method repeatedly
trains an ML algorithm and tests its empirical performance. Because it utilizes
the entire sample for both learning and evaluation, cramming is significantly
more data-efficient than sample-splitting. The cram method also naturally
accommodates online learning algorithms, making its implementation
computationally efficient. To demonstrate the power of the cram method, we
consider the standard policy learning setting where cramming is applied to the
same data to both develop an individualized treatment rule (ITR) and estimate
the average outcome that would result if the learned ITR were to be deployed.
We show that under a minimal set of assumptions, the resulting crammed
evaluation estimator is consistent and asymptotically normal. While our
asymptotic results require a relatively weak stabilization condition of ML
algorithm, we develop a simple, generic method that can be used with any policy
learning algorithm to satisfy this condition. Our extensive simulation studies
show that, when compared to sample-splitting, cramming reduces the evaluation
standard error by more than 40% while improving the performance of learned
policy. We also apply the cram method to a randomized clinical trial to
demonstrate its applicability to real-world problems. Finally, we briefly
discuss future extensions of the cram method to other learning and evaluation
settings.",0
RLingua: Improving Reinforcement Learning Sample Efficiency in Robotic Manipulations With Large Language Models,2403.06420v1,http://arxiv.org/abs/2403.06420v1,2024-03-11 04:13:26+00:00,"Reinforcement learning (RL) has demonstrated its capability in solving
various tasks but is notorious for its low sample efficiency. In this paper, we
propose RLingua, a framework that can leverage the internal knowledge of large
language models (LLMs) to reduce the sample complexity of RL in robotic
manipulations. To this end, we first present how to extract the prior knowledge
of LLMs by prompt engineering so that a preliminary rule-based robot controller
for a specific task can be generated. Despite being imperfect, the
LLM-generated robot controller is utilized to produce action samples during
rollouts with a decaying probability, thereby improving RL's sample efficiency.
We employ the actor-critic framework and modify the actor loss to regularize
the policy learning towards the LLM-generated controller. RLingua also provides
a novel method of improving the imperfect LLM-generated robot controllers by
RL. We demonstrated that RLingua can significantly reduce the sample complexity
of TD3 in the robot tasks of panda_gym and achieve high success rates in
sparsely rewarded robot tasks in RLBench, where the standard TD3 fails.
Additionally, We validated RLingua's effectiveness in real-world robot
experiments through Sim2Real, demonstrating that the learned policies are
effectively transferable to real robot tasks. Further details and videos about
our work are available at our project website https://rlingua.github.io.",0
Causal Multi-Label Feature Selection in Federated Setting,2403.06419v1,http://arxiv.org/abs/2403.06419v1,2024-03-11 04:11:48+00:00,"Multi-label feature selection serves as an effective mean for dealing with
high-dimensional multi-label data. To achieve satisfactory performance,
existing methods for multi-label feature selection often require the
centralization of substantial data from multiple sources. However, in Federated
setting, centralizing data from all sources and merging them into a single
dataset is not feasible. To tackle this issue, in this paper, we study a
challenging problem of causal multi-label feature selection in federated
setting and propose a Federated Causal Multi-label Feature Selection (FedCMFS)
algorithm with three novel subroutines. Specifically, FedCMFS first uses the
FedCFL subroutine that considers the correlations among label-label,
label-feature, and feature-feature to learn the relevant features (candidate
parents and children) of each class label while preserving data privacy without
centralizing data. Second, FedCMFS employs the FedCFR subroutine to selectively
recover the missed true relevant features. Finally, FedCMFS utilizes the FedCFC
subroutine to remove false relevant features. The extensive experiments on 8
datasets have shown that FedCMFS is effect for causal multi-label feature
selection in federated setting.",0
Efficient Estimation of the Convective Cooling Rate of Photovoltaic Arrays with Various Geometric Configurations: a Physics-Informed Machine Learning Approach,2403.06418v1,http://arxiv.org/abs/2403.06418v1,2024-03-11 04:09:48+00:00,"Convective heat transfer is crucial for photovoltaic (PV) systems, as the
power generation of PV is sensitive to temperature. The configuration of PV
arrays have a significant impact on convective heat transfer by influencing
turbulent characteristics. Conventional methods of quantifying the
configuration effects are either through Computational Fluid Dynamics (CFD)
simulations or empirical methods, which face the challenge of either high
computational demand or low accuracy, especially when complex array
configurations are considered. This work introduces a novel methodology to
quantify the impact of geometric configurations of PV arrays on their
convective heat transfer rate in wind field. The methodology combines Physics
Informed Machine Learning (PIML) and Deep Convolution Neural Network (DCNN) to
construct a robust PIML-DCNN model to predict convective heat transfer rates.
In addition, an innovative loss function, termed Pocket Loss is proposed to
enhance the interpretability of the PIML-DCNN model. The model exhibits
promising performance, with a relative error of 1.9\% and overall $R^2$ of 0.99
over all CFD cases in estimating the coefficient of convective heat transfer,
when compared with full CFD simulations. Therefore, the proposed model has the
potential to efficiently guide the configuration design of PV arrays for power
generation enhancement in real-world operations.",0
Evolving Knowledge Distillation with Large Language Models and Active Learning,2403.06414v1,http://arxiv.org/abs/2403.06414v1,2024-03-11 03:55:24+00:00,"Large language models (LLMs) have demonstrated remarkable capabilities across
various NLP tasks. However, their computational costs are prohibitively high.
To address this issue, previous research has attempted to distill the knowledge
of LLMs into smaller models by generating annotated data. Nonetheless, these
works have mainly focused on the direct use of LLMs for text generation and
labeling, without fully exploring their potential to comprehend the target task
and acquire valuable knowledge. In this paper, we propose EvoKD: Evolving
Knowledge Distillation, which leverages the concept of active learning to
interactively enhance the process of data generation using large language
models, simultaneously improving the task capabilities of small domain model
(student model). Different from previous work, we actively analyze the student
model's weaknesses, and then synthesize labeled samples based on the analysis.
In addition, we provide iterative feedback to the LLMs regarding the student
model's performance to continuously construct diversified and challenging
samples. Experiments and analysis on different NLP tasks, namely, text
classification and named entity recognition show the effectiveness of EvoKD.",0
A Logical Pattern Memory Pre-trained Model for Entailment Tree Generation,2403.06410v1,http://arxiv.org/abs/2403.06410v1,2024-03-11 03:45:09+00:00,"Generating coherent and credible explanations remains a significant challenge
in the field of AI. In recent years, researchers have delved into the
utilization of entailment trees to depict explanations, which exhibit a
reasoning process of how a hypothesis is deduced from the supporting facts.
However, existing models often overlook the importance of generating
intermediate conclusions with logical consistency from the given facts, leading
to inaccurate conclusions and undermining the overall credibility of entailment
trees. To address this limitation, we propose the logical pattern memory
pre-trained model (LMPM). LMPM incorporates an external memory structure to
learn and store the latent representations of logical patterns, which aids in
generating logically consistent conclusions. Furthermore, to mitigate the
influence of logically irrelevant domain knowledge in the Wikipedia-based data,
we introduce an entity abstraction approach to construct the dataset for
pre-training LMPM. The experimental results highlight the effectiveness of our
approach in improving the quality of entailment tree generation. By leveraging
logical entailment patterns, our model produces more coherent and reasonable
conclusions that closely align with the underlying premises. Code and Data are
released at https://github.com/YuanLi95/T5-LMPM",0
A Mathematical Framework for the Problem of Security for Cognition in Neurotechnology,2403.07945v1,http://arxiv.org/abs/2403.07945v1,2024-03-11 03:44:18+00:00,"The rapid advancement in neurotechnology in recent years has created an
emerging critical intersection between neurotechnology and security.
Implantable devices, non-invasive monitoring, and non-invasive therapies all
carry with them the prospect of violating the privacy and autonomy of
individuals' cognition. A growing number of scientists and physicians have made
calls to address this issue -- which we term Cognitive Security -- but applied
efforts have been limited. A major barrier hampering scientific and engineering
efforts to address Cognitive Security is the lack of a clear means of
describing and analyzing relevant problems. In this paper we develop Cognitive
Security, a mathematical framework which enables such description and analysis
by drawing on methods and results from multiple fields. We demonstrate certain
statistical properties which have significant implications for Cognitive
Security, and then present descriptions of the algorithmic problems faced by
attackers attempting to violate privacy and autonomy, and defenders attempting
to obstruct such attempts.",0
What Makes Quantization for Large Language Models Hard? An Empirical Study from the Lens of Perturbation,2403.06408v1,http://arxiv.org/abs/2403.06408v1,2024-03-11 03:42:51+00:00,"Quantization has emerged as a promising technique for improving the memory
and computational efficiency of large language models (LLMs). Though the
trade-off between performance and efficiency is well-known, there is still much
to be learned about the relationship between quantization and LLM performance.
To shed light on this relationship, we propose a new perspective on
quantization, viewing it as perturbations added to the weights and activations
of LLMs. We call this approach ""the lens of perturbation"". Using this lens, we
conduct experiments with various artificial perturbations to explore their
impact on LLM performance. Our findings reveal several connections between the
properties of perturbations and LLM performance, providing insights into the
failure cases of uniform quantization and suggesting potential solutions to
improve the robustness of LLM quantization. To demonstrate the significance of
our findings, we implement a simple non-uniform quantization approach based on
our insights. Our experiments show that this approach achieves minimal
performance degradation on both 4-bit weight quantization and 8-bit
quantization for weights and activations. These results validate the
correctness of our approach and highlight its potential to improve the
efficiency of LLMs without sacrificing performance.",0
Can LLMs' Tuning Methods Work in Medical Multimodal Domain?,2403.06407v1,http://arxiv.org/abs/2403.06407v1,2024-03-11 03:38:48+00:00,"While large language models (LLMs) excel in world knowledge understanding,
adapting them to specific subfields requires precise adjustments. Due to the
model's vast scale, traditional global fine-tuning methods for large models can
be computationally expensive and impact generalization. To address this
challenge, a range of innovative Parameters-Efficient Fine-Tuning (PEFT)
methods have emerged and achieved remarkable success in both LLMs and Large
Vision-Language Models (LVLMs). In the medical domain, fine-tuning a medical
Vision-Language Pretrained (VLP) model is essential for adapting it to specific
tasks. Can the fine-tuning methods for large models be transferred to the
medical field to enhance transfer learning efficiency? In this paper, we delve
into the fine-tuning methods of LLMs and conduct extensive experiments to
investigate the impact of fine-tuning methods for large models on existing
multimodal models in the medical domain from the training data level and the
model structure level. We show the different impacts of fine-tuning methods for
large models on medical VLMs and develop the most efficient ways to fine-tune
medical VLP models. We hope this research can guide medical domain researchers
in optimizing VLMs' training costs, fostering the broader application of VLMs
in healthcare fields. Code and dataset will be released upon acceptance.",0
AuG-KD: Anchor-Based Mixup Generation for Out-of-Domain Knowledge Distillation,2403.07030v1,http://arxiv.org/abs/2403.07030v1,2024-03-11 03:34:14+00:00,"Due to privacy or patent concerns, a growing number of large models are
released without granting access to their training data, making transferring
their knowledge inefficient and problematic. In response, Data-Free Knowledge
Distillation (DFKD) methods have emerged as direct solutions. However, simply
adopting models derived from DFKD for real-world applications suffers
significant performance degradation, due to the discrepancy between teachers'
training data and real-world scenarios (student domain). The degradation stems
from the portions of teachers' knowledge that are not applicable to the student
domain. They are specific to the teacher domain and would undermine students'
performance. Hence, selectively transferring teachers' appropriate knowledge
becomes the primary challenge in DFKD. In this work, we propose a simple but
effective method AuG-KD. It utilizes an uncertainty-guided and sample-specific
anchor to align student-domain data with the teacher domain and leverages a
generative method to progressively trade off the learning process between OOD
knowledge distillation and domain-specific information learning via mixup
learning. Extensive experiments in 3 datasets and 8 settings demonstrate the
stability and superiority of our approach. Code available at
https://github.com/IshiKura-a/AuG-KD .",0
Cosine Scoring with Uncertainty for Neural Speaker Embedding,2403.06404v1,http://arxiv.org/abs/2403.06404v1,2024-03-11 03:31:35+00:00,"Uncertainty modeling in speaker representation aims to learn the variability
present in speech utterances. While the conventional cosine-scoring is
computationally efficient and prevalent in speaker recognition, it lacks the
capability to handle uncertainty. To address this challenge, this paper
proposes an approach for estimating uncertainty at the speaker embedding
front-end and propagating it to the cosine scoring back-end. Experiments
conducted on the VoxCeleb and SITW datasets confirmed the efficacy of the
proposed method in handling uncertainty arising from embedding estimation. It
achieved improvement with 8.5% and 9.8% average reductions in EER and minDCF
compared to the conventional cosine similarity. It is also computationally
efficient in practice.",0
PointSeg: A Training-Free Paradigm for 3D Scene Segmentation via Foundation Models,2403.06403v1,http://arxiv.org/abs/2403.06403v1,2024-03-11 03:28:20+00:00,"Recent success of vision foundation models have shown promising performance
for the 2D perception tasks. However, it is difficult to train a 3D foundation
network directly due to the limited dataset and it remains under explored
whether existing foundation models can be lifted to 3D space seamlessly. In
this paper, we present PointSeg, a novel training-free paradigm that leverages
off-the-shelf vision foundation models to address 3D scene perception tasks.
PointSeg can segment anything in 3D scene by acquiring accurate 3D prompts to
align their corresponding pixels across frames. Concretely, we design a
two-branch prompts learning structure to construct the 3D point-box prompts
pairs, combining with the bidirectional matching strategy for accurate point
and proposal prompts generation. Then, we perform the iterative post-refinement
adaptively when cooperated with different vision foundation models. Moreover,
we design a affinity-aware merging algorithm to improve the final ensemble
masks. PointSeg demonstrates impressive segmentation performance across various
datasets, all without training. Specifically, our approach significantly
surpasses the state-of-the-art specialist model by 13.4$\%$, 11.3$\%$, and
12$\%$ mAP on ScanNet, ScanNet++, and KITTI-360 datasets, respectively. On top
of that, PointSeg can incorporate with various segmentation models and even
surpasses the supervised methods.",0
'One size doesn't fit all': Learning how many Examples to use for In-Context Learning for Improved Text Classification,2403.06402v1,http://arxiv.org/abs/2403.06402v1,2024-03-11 03:28:13+00:00,"Predictive models in natural language processing (NLP) have evolved from
training models from scratch to fine-tuning pre-trained models with labelled
data. An extreme form of this fine-tuning involves in-context learning (ICL),
where the output of a pre-trained generative model (frozen decoder parameters)
is controlled only with variations in the input strings (called instructions or
prompts). An important component of ICL is the use of a small number of
labelled data instances as examples in the prompt. While existing work uses a
static number of examples during inference for each data instance, in this
paper we propose a novel methodology of dynamically adapting the number of
examples as per the data. This is analogous to the use of a variable-sized
neighborhood in k-nearest neighbors (k-NN) classifier. In our proposed workflow
of adaptive ICL (AICL), the number of demonstrations to employ during the
inference on a particular data instance is predicted by the Softmax posteriors
of a classifier. The parameters of this classifier are fitted on the optimal
number of examples in ICL required to correctly infer the label of each
instance in the training set with the hypothesis that a test instance that is
similar to a training instance should use the same (or a closely matching)
number of few-shot examples. Our experiments show that our AICL method results
in improvement in text classification task on several standard datasets.",0
On the Diminishing Returns of Width for Continual Learning,2403.06398v2,http://arxiv.org/abs/2403.06398v2,2024-03-11 03:19:45+00:00,"While deep neural networks have demonstrated groundbreaking performance in
various settings, these models often suffer from \emph{catastrophic forgetting}
when trained on new tasks in sequence. Several works have empirically
demonstrated that increasing the width of a neural network leads to a decrease
in catastrophic forgetting but have yet to characterize the exact relationship
between width and continual learning. We design one of the first frameworks to
analyze Continual Learning Theory and prove that width is directly related to
forgetting in Feed-Forward Networks (FFN). Specifically, we demonstrate that
increasing network widths to reduce forgetting yields diminishing returns. We
empirically verify our claims at widths hitherto unexplored in prior studies
where the diminishing returns are clearly observed as predicted by our theory.",0
DeepSafeMPC: Deep Learning-Based Model Predictive Control for Safe Multi-Agent Reinforcement Learning,2403.06397v2,http://arxiv.org/abs/2403.06397v2,2024-03-11 03:17:33+00:00,"Safe Multi-agent reinforcement learning (safe MARL) has increasingly gained
attention in recent years, emphasizing the need for agents to not only optimize
the global return but also adhere to safety requirements through behavioral
constraints. Some recent work has integrated control theory with multi-agent
reinforcement learning to address the challenge of ensuring safety. However,
there have been only very limited applications of Model Predictive Control
(MPC) methods in this domain, primarily due to the complex and implicit
dynamics characteristic of multi-agent environments. To bridge this gap, we
propose a novel method called Deep Learning-Based Model Predictive Control for
Safe Multi-Agent Reinforcement Learning (DeepSafeMPC). The key insight of
DeepSafeMPC is leveraging a entralized deep learning model to well predict
environmental dynamics. Our method applies MARL principles to search for
optimal solutions. Through the employment of MPC, the actions of agents can be
restricted within safe states concurrently. We demonstrate the effectiveness of
our approach using the Safe Multi-agent MuJoCo environment, showcasing
significant advancements in addressing safety concerns in MARL.",0
A Segmentation Foundation Model for Diverse-type Tumors,2403.06396v1,http://arxiv.org/abs/2403.06396v1,2024-03-11 03:05:05+00:00,"Large pre-trained models with their numerous model parameters and extensive
training datasets have shown excellent performance in various tasks. Many
publicly available medical image datasets do not have a sufficient amount of
data so there are few large-scale models in medical imaging. We propose a
large-scale Tumor Segmentation Foundation Model (TSFM) with 1.6 billion
parameters using Resblock-backbone and Transformer-bottleneck,which has good
transfer ability for downstream tasks. To make TSFM exhibit good performance in
tumor segmentation, we make full use of the strong spatial correlation between
tumors and organs in the medical image, innovatively fuse 7 tumor datasets and
3 multi-organ datasets to build a 3D medical dataset pool, including 2779 cases
with totally 300k medical images, whose size currently exceeds many other
single publicly available datasets. TSFM is the pre-trained model for medical
image segmentation, which also can be transferred to multiple downstream tasks
for fine-tuning learning. The average performance of our pre-trained model is
2% higher than that of nnU-Net across various tumor types. In the transfer
learning task, TSFM only needs 5% training epochs of nnU-Net to achieve similar
performance and can surpass nnU-Net by 2% on average with 10% training epoch.
Pre-trained TSFM and its code will be released soon.",0
FSViewFusion: Few-Shots View Generation of Novel Objects,2403.06394v2,http://arxiv.org/abs/2403.06394v2,2024-03-11 02:59:30+00:00,"Novel view synthesis has observed tremendous developments since the arrival
of NeRFs. However, Nerf models overfit on a single scene, lacking
generalization to out of distribution objects. Recently, diffusion models have
exhibited remarkable performance on introducing generalization in view
synthesis. Inspired by these advancements, we explore the capabilities of a
pretrained stable diffusion model for view synthesis without explicit 3D
priors. Specifically, we base our method on a personalized text to image model,
Dreambooth, given its strong ability to adapt to specific novel objects with a
few shots. Our research reveals two interesting findings. First, we observe
that Dreambooth can learn the high level concept of a view, compared to
arguably more complex strategies which involve finetuning diffusions on large
amounts of multi-view data. Second, we establish that the concept of a view can
be disentangled and transferred to a novel object irrespective of the original
object's identify from which the views are learnt. Motivated by this, we
introduce a learning strategy, FSViewFusion, which inherits a specific view
through only one image sample of a single scene, and transfers the knowledge to
a novel object, learnt from few shots, using low rank adapters. Through
extensive experiments we demonstrate that our method, albeit simple, is
efficient in generating reliable view samples for in the wild images. Code and
models will be released.",0
Towards Robust Out-of-Distribution Generalization Bounds via Sharpness,2403.06392v1,http://arxiv.org/abs/2403.06392v1,2024-03-11 02:57:27+00:00,"Generalizing to out-of-distribution (OOD) data or unseen domain, termed OOD
generalization, still lacks appropriate theoretical guarantees. Canonical OOD
bounds focus on different distance measurements between source and target
domains but fail to consider the optimization property of the learned model. As
empirically shown in recent work, the sharpness of learned minima influences
OOD generalization. To bridge this gap between optimization and OOD
generalization, we study the effect of sharpness on how a model tolerates data
change in domain shift which is usually captured by ""robustness"" in
generalization. In this paper, we give a rigorous connection between sharpness
and robustness, which gives better OOD guarantees for robust algorithms. It
also provides a theoretical backing for ""flat minima leads to better OOD
generalization"". Overall, we propose a sharpness-based OOD generalization bound
by taking robustness into consideration, resulting in a tighter bound than
non-robust guarantees. Our findings are supported by the experiments on a ridge
regression model, as well as the experiments on deep learning classification
tasks.",0
Developing an AI-Based Psychometric System for Assessing Learning Difficulties and Adaptive System to Overcome: A Qualitative and Conceptual Framework,2403.06284v1,http://arxiv.org/abs/2403.06284v1,2024-03-10 18:39:49+00:00,"Learning difficulties pose significant challenges for students, impacting
their academic performance and overall educational experience. These
difficulties could sometimes put students into a downward spiral that lack of
educational resources for personalized support consistently led to
under-accommodation of students special needs, and the student lose
opportunities in the longer term academic and work development. This research
aims to propose a conceptual framework for an adaptive AI-based virtual tutor
system that incorporates psychometric assessment to support students with
learning difficulties. This process involves the careful selection and
integration of validated current mature psychometric scales that assess key
dimensions of learning, such as cognitive abilities, learning styles, and
academic skills. By incorporating scales that specifically assess these
difficulties, the psychometric test will provide a comprehensive understanding
of each students unique learning profile and inform targeted interventions
within the adaptive tutoring system. The paper also proposes using autoencoders
to identify the latent patterns to generate the students profile vector for
collection of psychometric data, defining state space and action space
representing the students desired combination of images, sound and text
engagements, employing extended Bayesian knowledge tracing and hierarchical
model and Metropolis-Hastings to continuously estimate and monitor the students
performance in various psychometric constructs. The proposed system will
leverage the capabilities of LLMs, visual generation models, and psychometric
assessments to provide personalized instruction and support tailored to each
students unique learning characteristics and needs.",0
Fine-tuning of diffusion models via stochastic control: entropy regularization and beyond,2403.06279v2,http://arxiv.org/abs/2403.06279v2,2024-03-10 18:13:22+00:00,"This paper aims to develop and provide a rigorous treatment to the problem of
entropy regularized fine-tuning in the context of continuous-time diffusion
models, which was recently proposed by Uehara et al. (arXiv:2402.15194, 2024).
The idea is to use stochastic control for sample generation, where the entropy
regularizer is introduced to mitigate reward collapse. We also show how the
analysis can be extended to fine-tuning involving a general $f$-divergence
regularizer.",0
UNICORN: Ultrasound Nakagami Imaging via Score Matching and Adaptation,2403.06275v1,http://arxiv.org/abs/2403.06275v1,2024-03-10 18:05:41+00:00,"Nakagami imaging holds promise for visualizing and quantifying tissue
scattering in ultrasound waves, with potential applications in tumor diagnosis
and fat fraction estimation which are challenging to discern by conventional
ultrasound B-mode images. Existing methods struggle with optimal window size
selection and suffer from estimator instability, leading to degraded resolution
images. To address this, here we propose a novel method called UNICORN
(Ultrasound Nakagami Imaging via Score Matching and Adaptation), that offers an
accurate, closed-form estimator for Nakagami parameter estimation in terms of
the score function of ultrasonic envelope. Extensive experiments using
simulation and real ultrasound RF data demonstrate UNICORN's superiority over
conventional approaches in accuracy and resolution quality.",0
GlanceVAD: Exploring Glance Supervision for Label-efficient Video Anomaly Detection,2403.06154v2,http://arxiv.org/abs/2403.06154v2,2024-03-10 09:57:10+00:00,"In recent years, video anomaly detection has been extensively investigated in
both unsupervised and weakly supervised settings to alleviate costly temporal
labeling. Despite significant progress, these methods still suffer from
unsatisfactory results such as numerous false alarms, primarily due to the
absence of precise temporal anomaly annotation. In this paper, we present a
novel labeling paradigm, termed ""glance annotation"", to achieve a better
balance between anomaly detection accuracy and annotation cost. Specifically,
glance annotation is a random frame within each abnormal event, which can be
easily accessed and is cost-effective. To assess its effectiveness, we manually
annotate the glance annotations for two standard video anomaly detection
datasets: UCF-Crime and XD-Violence. Additionally, we propose a customized
GlanceVAD method, that leverages gaussian kernels as the basic unit to compose
the temporal anomaly distribution, enabling the learning of diverse and robust
anomaly representations from the glance annotations. Through comprehensive
analysis and experiments, we verify that the proposed labeling paradigm can
achieve an excellent trade-off between annotation cost and model performance.
Extensive experimental results also demonstrate the effectiveness of our
GlanceVAD approach, which significantly outperforms existing advanced
unsupervised and weakly supervised methods. Code and annotations will be
publicly available at https://github.com/pipixin321/GlanceVAD.",0
Decoupled Contrastive Learning for Long-Tailed Recognition,2403.06151v1,http://arxiv.org/abs/2403.06151v1,2024-03-10 09:46:28+00:00,"Supervised Contrastive Loss (SCL) is popular in visual representation
learning. Given an anchor image, SCL pulls two types of positive samples, i.e.,
its augmentation and other images from the same class together, while pushes
negative images apart to optimize the learned embedding. In the scenario of
long-tailed recognition, where the number of samples in each class is
imbalanced, treating two types of positive samples equally leads to the biased
optimization for intra-category distance. In addition, similarity relationship
among negative samples, that are ignored by SCL, also presents meaningful
semantic cues. To improve the performance on long-tailed recognition, this
paper addresses those two issues of SCL by decoupling the training objective.
Specifically, it decouples two types of positives in SCL and optimizes their
relations toward different objectives to alleviate the influence of the
imbalanced dataset. We further propose a patch-based self distillation to
transfer knowledge from head to tail classes to relieve the
under-representation of tail classes. It uses patch-based features to mine
shared visual patterns among different instances and leverages a self
distillation procedure to transfer such knowledge. Experiments on different
long-tailed classification benchmarks demonstrate the superiority of our
method. For instance, it achieves the 57.7% top-1 accuracy on the ImageNet-LT
dataset. Combined with the ensemble-based method, the performance can be
further boosted to 59.7%, which substantially outperforms many recent works.
The code is available at https://github.com/SY-Xuan/DSCL.",0
"All-in-one platform for AI R&D in medical imaging, encompassing data collection, selection, annotation, and pre-processing",2403.06145v1,http://arxiv.org/abs/2403.06145v1,2024-03-10 09:24:53+00:00,"Deep Learning is advancing medical imaging Research and Development (R&D),
leading to the frequent clinical use of Artificial Intelligence/Machine
Learning (AI/ML)-based medical devices. However, to advance AI R&D, two
challenges arise: 1) significant data imbalance, with most data from
Europe/America and under 10% from Asia, despite its 60% global population
share; and 2) hefty time and investment needed to curate proprietary datasets
for commercial use. In response, we established the first commercial medical
imaging platform, encompassing steps like: 1) data collection, 2) data
selection, 3) annotation, and 4) pre-processing. Moreover, we focus on
harnessing under-represented data from Japan and broader Asia, including
Computed Tomography, Magnetic Resonance Imaging, and Whole Slide Imaging scans.
Using the collected data, we are preparing/providing ready-to-use datasets for
medical AI R&D by 1) offering these datasets to AI firms, biopharma, and
medical device makers and 2) using them as training/test data to develop
tailored AI solutions for such entities. We also aim to merge Blockchain for
data security and plan to synthesize rare disease data via generative AI.
DataHub Website: https://medical-datahub.ai/",0
Fluent: Round-efficient Secure Aggregation for Private Federated Learning,2403.06143v1,http://arxiv.org/abs/2403.06143v1,2024-03-10 09:11:57+00:00,"Federated learning (FL) facilitates collaborative training of machine
learning models among a large number of clients while safeguarding the privacy
of their local datasets. However, FL remains susceptible to vulnerabilities
such as privacy inference and inversion attacks. Single-server secure
aggregation schemes were proposed to address these threats. Nonetheless, they
encounter practical constraints due to their round and communication
complexities. This work introduces Fluent, a round and communication-efficient
secure aggregation scheme for private FL. Fluent has several improvements
compared to state-of-the-art solutions like Bell et al. (CCS 2020) and Ma et
al. (SP 2023): (1) it eliminates frequent handshakes and secret sharing
operations by efficiently reusing the shares across multiple training
iterations without leaking any private information; (2) it accomplishes both
the consistency check and gradient unmasking in one logical step, thereby
reducing another round of communication. With these innovations, Fluent
achieves the fewest communication rounds (i.e., two in the collection phase) in
the malicious server setting, in contrast to at least three rounds in existing
schemes. This significantly minimizes the latency for geographically
distributed clients; (3) Fluent also introduces Fluent-Dynamic with a
participant selection algorithm and an alternative secret sharing scheme. This
can facilitate dynamic client joining and enhance the system flexibility and
scalability. We implemented Fluent and compared it with existing solutions.
Experimental results show that Fluent improves the computational cost by at
least 75% and communication overhead by at least 25% for normal clients. Fluent
also reduces the communication overhead for the server at the expense of a
marginal increase in computational cost.",0
Bayesian Random Semantic Data Augmentation for Medical Image Classification,2403.06138v1,http://arxiv.org/abs/2403.06138v1,2024-03-10 08:56:02+00:00,"Data augmentation is a critical regularization technique for deep neural
networks, particularly in medical image classification. Popular data
augmentation approaches include image transformation-based methods, generative
data augmentation, and automatic data augmentation. However, these approaches
encounter notable limitations: image transformation-based and automated data
augmentation techniques cannot implement semantic transformations, leading to a
constrained variety of augmented samples, and generative data augmentation
methods are computationally expensive. In response to these challenges, we
proposed Bayesian Random Semantic Data Augmentation (BRSDA), a novel,
efficient, and plug-and-play semantic data augmentation method. BRSDA is
motivated by a simple translation in the feature space along specific
directions that can effectuate semantic transformations. When given a feature,
we define its augmentable semantic magnitude as a random variable and estimate
its distribution using variational Bayesian, then sample semantic magnitude and
add to the randomly selected semantic direction to achieve semantic data
augmentation. We demonstrate the effectiveness of BRSDA on five 2D and six 3D
medical image datasets covering nine modalities. We also test BRSDA with
mainstream neural network architectures, showcasing its robustness.
Furthermore, combining BRSDA with other leading data augmentation methods
achieves superior performance. Code is available online at
\url{https://github.com/YaoyaoZhu19/BRSDA}.",0
RESTORE: Towards Feature Shift for Vision-Language Prompt Learning,2403.06136v1,http://arxiv.org/abs/2403.06136v1,2024-03-10 08:52:48+00:00,"Prompt learning is effective for fine-tuning foundation models to improve
their generalization across a variety of downstream tasks. However, the prompts
that are independently optimized along a single modality path, may sacrifice
the vision-language alignment of pre-trained models in return for improved
performance on specific tasks and classes, leading to poorer generalization. In
this paper, we first demonstrate that prompt tuning along only one single
branch of CLIP (e.g., language or vision) is the reason why the misalignment
occurs. Without proper regularization across the learnable parameters in
different modalities, prompt learning violates the original pre-training
constraints inherent in the two-tower architecture. To address such
misalignment, we first propose feature shift, which is defined as the variation
of embeddings after introducing the learned prompts, to serve as an explanatory
tool. We dive into its relation with generalizability and thereafter propose
RESTORE, a multi-modal prompt learning method that exerts explicit constraints
on cross-modal consistency. To be more specific, to prevent feature
misalignment, a feature shift consistency is introduced to synchronize
inter-modal feature shifts by measuring and regularizing the magnitude of
discrepancy during prompt tuning. In addition, we propose a ""surgery"" block to
avoid short-cut hacking, where cross-modal misalignment can still be severe if
the feature shift of each modality varies drastically at the same rate. It is
implemented as feed-forward adapters upon both modalities to alleviate the
misalignment problem. Extensive experiments on 15 datasets demonstrate that our
method outperforms the state-of-the-art prompt tuning methods without
compromising feature alignment.",0
MACE: Mass Concept Erasure in Diffusion Models,2403.06135v1,http://arxiv.org/abs/2403.06135v1,2024-03-10 08:50:56+00:00,"The rapid expansion of large-scale text-to-image diffusion models has raised
growing concerns regarding their potential misuse in creating harmful or
misleading content. In this paper, we introduce MACE, a finetuning framework
for the task of mass concept erasure. This task aims to prevent models from
generating images that embody unwanted concepts when prompted. Existing concept
erasure methods are typically restricted to handling fewer than five concepts
simultaneously and struggle to find a balance between erasing concept synonyms
(generality) and maintaining unrelated concepts (specificity). In contrast,
MACE differs by successfully scaling the erasure scope up to 100 concepts and
by achieving an effective balance between generality and specificity. This is
achieved by leveraging closed-form cross-attention refinement along with LoRA
finetuning, collectively eliminating the information of undesirable concepts.
Furthermore, MACE integrates multiple LoRAs without mutual interference. We
conduct extensive evaluations of MACE against prior methods across four
different tasks: object erasure, celebrity erasure, explicit content erasure,
and artistic style erasure. Our results reveal that MACE surpasses prior
methods in all evaluated tasks. Code is available at
https://github.com/Shilin-LU/MACE.",0
FedPIT: Towards Privacy-preserving and Few-shot Federated Instruction Tuning,2403.06131v1,http://arxiv.org/abs/2403.06131v1,2024-03-10 08:41:22+00:00,"Instruction tuning has proven essential for enhancing the performance of
large language models (LLMs) in generating human-aligned responses. However,
collecting diverse, high-quality instruction data for tuning poses challenges,
particularly in privacy-sensitive domains. Federated instruction tuning (FedIT)
has emerged as a solution, leveraging federated learning from multiple data
owners while preserving privacy. Yet, it faces challenges due to limited
instruction data and vulnerabilities to training data extraction attacks. To
address these issues, we propose a novel federated algorithm, FedPIT, which
utilizes LLMs' in-context learning capability to self-generate task-specific
synthetic data for training autonomously. Our method employs parameter-isolated
training to maintain global parameters trained on synthetic data and local
parameters trained on augmented local data, effectively thwarting data
extraction attacks. Extensive experiments on real-world medical data
demonstrate the effectiveness of FedPIT in improving federated few-shot
performance while preserving privacy and robustness against data heterogeneity.",0
Blockchain-Enabled Variational Information Bottleneck for IoT Networks,2403.06129v1,http://arxiv.org/abs/2403.06129v1,2024-03-10 08:27:38+00:00,"In Internet of Things (IoT) networks, the amount of data sensed by user
devices may be huge, resulting in the serious network congestion. To solve this
problem, intelligent data compression is critical. The variational information
bottleneck (VIB) approach, combined with machine learning, can be employed to
train the encoder and decoder, so that the required transmission data size can
be reduced significantly. However, VIB suffers from the computing burden and
network insecurity. In this paper, we propose a blockchain-enabled VIB (BVIB)
approach to relieve the computing burden while guaranteeing network security.
Extensive simulations conducted by Python and C++ demonstrate that BVIB
outperforms VIB by 36%, 22% and 57% in terms of time and CPU cycles cost,
mutual information, and accuracy under attack, respectively.",0
Low-dose CT Denoising with Language-engaged Dual-space Alignment,2403.06128v1,http://arxiv.org/abs/2403.06128v1,2024-03-10 08:21:50+00:00,"While various deep learning methods were proposed for low-dose computed
tomography (CT) denoising, they often suffer from over-smoothing, blurring, and
lack of explainability. To alleviate these issues, we propose a plug-and-play
Language-Engaged Dual-space Alignment loss (LEDA) to optimize low-dose CT
denoising models. Our idea is to leverage large language models (LLMs) to align
denoised CT and normal dose CT images in both the continuous perceptual space
and discrete semantic space, which is the first LLM-based scheme for low-dose
CT denoising. LEDA involves two steps: the first is to pretrain an LLM-guided
CT autoencoder, which can encode a CT image into continuous high-level features
and quantize them into a token space to produce semantic tokens derived from
the LLM's vocabulary; and the second is to minimize the discrepancy between the
denoised CT images and normal dose CT in terms of both encoded high-level
features and quantized token embeddings derived by the LLM-guided CT
autoencoder. Extensive experimental results on two public LDCT denoising
datasets demonstrate that our LEDA can enhance existing denoising models in
terms of quantitative metrics and qualitative evaluation, and also provide
explainability through language-level image understanding. Source code is
available at https://github.com/hao1635/LEDA.",0
In-context Prompt Learning for Test-time Vision Recognition with Frozen Vision-language Model,2403.06126v1,http://arxiv.org/abs/2403.06126v1,2024-03-10 08:15:51+00:00,"Existing pre-trained vision-language models, e.g., CLIP, have demonstrated
impressive zero-shot generalization capabilities in various downstream tasks.
However, the performance of these models will degrade significantly when test
inputs present different distributions. To this end, we explore the concept of
test-time prompt tuning (TTPT), which enables the adaptation of the CLIP model
to novel downstream tasks through only one step of optimization on an
unsupervised objective that involves the test sample. Motivated by in-context
learning within field of natural language processing (NLP), we propose
In-Context Prompt Learning (InCPL) for test-time visual recognition task. InCPL
involves associating a new test sample with very few or even just one labeled
example as its in-context prompt. As a result, it can reliably estimate a label
for the test sample, thereby facilitating the model adaptation process. InCPL
first employs a token net to represent language descriptions as visual prompts
that the vision encoder of a CLIP model can comprehend. Paired with in-context
examples, we further propose a context-aware unsupervised loss to optimize test
sample-aware visual prompts. This optimization allows a pre-trained, frozen
CLIP model to be adapted to a test sample from any task using its learned
adaptive prompt. Our method has demonstrated superior performance and achieved
state-of-the-art results across various downstream datasets.",0
Style Blind Domain Generalized Semantic Segmentation via Covariance Alignment and Semantic Consistence Contrastive Learning,2403.06122v1,http://arxiv.org/abs/2403.06122v1,2024-03-10 07:44:41+00:00,"Deep learning models for semantic segmentation often experience performance
degradation when deployed to unseen target domains unidentified during the
training phase. This is mainly due to variations in image texture (\ie style)
from different data sources. To tackle this challenge, existing domain
generalized semantic segmentation (DGSS) methods attempt to remove style
variations from the feature. However, these approaches struggle with the
entanglement of style and content, which may lead to the unintentional removal
of crucial content information, causing performance degradation. This study
addresses this limitation by proposing BlindNet, a novel DGSS approach that
blinds the style without external modules or datasets. The main idea behind our
proposed approach is to alleviate the effect of style in the encoder whilst
facilitating robust segmentation in the decoder. To achieve this, BlindNet
comprises two key components: covariance alignment and semantic consistency
contrastive learning. Specifically, the covariance alignment trains the encoder
to uniformly recognize various styles and preserve the content information of
the feature, rather than removing the style-sensitive factor. Meanwhile,
semantic consistency contrastive learning enables the decoder to construct
discriminative class embedding space and disentangles features that are
vulnerable to misclassification. Through extensive experiments, our approach
outperforms existing DGSS methods, exhibiting robustness and superior
performance for semantic segmentation on unseen target domains.",0
Large Language Models on Fine-grained Emotion Detection Dataset with Data Augmentation and Transfer Learning,2403.06108v1,http://arxiv.org/abs/2403.06108v1,2024-03-10 06:30:54+00:00,"This paper delves into enhancing the classification performance on the
GoEmotions dataset, a large, manually annotated dataset for emotion detection
in text. The primary goal of this paper is to address the challenges of
detecting subtle emotions in text, a complex issue in Natural Language
Processing (NLP) with significant practical applications. The findings offer
valuable insights into addressing the challenges of emotion detection in text
and suggest directions for future research, including the potential for a
survey paper that synthesizes methods and performances across various datasets
in this domain.",0
Generative LSTM Models and Asset Hierarchy Creation in Industrial Facilities,2403.06103v1,http://arxiv.org/abs/2403.06103v1,2024-03-10 06:07:16+00:00,"In the evolving field of maintenance and reliability engineering, the
organization of equipment into hierarchical structures presents both a
challenge and a necessity, directly impacting the operational integrity of
industrial facilities. This paper introduces an innovative approach employing
machine learning, specifically Long Short-Term Memory (LSTM) models, to
automate and enhance the creation and management of these hierarchies. By
adapting techniques commonly used in natural language processing, the study
explores the potential of LSTM models to interpret and predict relationships
within equipment tags, offering a novel perspective on understanding facility
design. This methodology involved character-wise tokenization of asset tags
from approximately 29,000 entries across 50 upstream oil and gas facilities,
followed by modeling these sequences using an LSTM-based recurrent neural
network. The model's architecture capitalizes on LSTM's ability to learn
long-term dependencies, facilitating the prediction of hierarchical
relationships and contextual understanding of equipment tags.",0
Coherent Temporal Synthesis for Incremental Action Segmentation,2403.06102v1,http://arxiv.org/abs/2403.06102v1,2024-03-10 06:07:06+00:00,"Data replay is a successful incremental learning technique for images. It
prevents catastrophic forgetting by keeping a reservoir of previous data,
original or synthesized, to ensure the model retains past knowledge while
adapting to novel concepts. However, its application in the video domain is
rudimentary, as it simply stores frame exemplars for action recognition. This
paper presents the first exploration of video data replay techniques for
incremental action segmentation, focusing on action temporal modeling. We
propose a Temporally Coherent Action (TCA) model, which represents actions
using a generative model instead of storing individual frames. The integration
of a conditioning variable that captures temporal coherence allows our model to
understand the evolution of action features over time. Therefore, action
segments generated by TCA for replay are diverse and temporally coherent. In a
10-task incremental setup on the Breakfast dataset, our approach achieves
significant increases in accuracy for up to 22% compared to the baselines.",0
Automatic design optimization of preference-based subjective evaluation with online learning in crowdsourcing environment,2403.06100v1,http://arxiv.org/abs/2403.06100v1,2024-03-10 05:55:00+00:00,"A preference-based subjective evaluation is a key method for evaluating
generative media reliably. However, its huge combinations of pairs prohibit it
from being applied to large-scale evaluation using crowdsourcing. To address
this issue, we propose an automatic optimization method for preference-based
subjective evaluation in terms of pair combination selections and allocation of
evaluation volumes with online learning in a crowdsourcing environment. We use
a preference-based online learning method based on a sorting algorithm to
identify the total order of evaluation targets with minimum sample volumes. Our
online learning algorithm supports parallel and asynchronous execution under
fixed-budget conditions required for crowdsourcing. Our experiment on
preference-based subjective evaluation of synthetic speech shows that our
method successfully optimizes the test by reducing pair combinations from 351
to 83 and allocating optimal evaluation volumes for each pair ranging from 30
to 663 without compromising evaluation accuracies and wasting budget
allocations.",0
Is Vanilla MLP in Neural Radiance Field Enough for Few-shot View Synthesis?,2403.06092v1,http://arxiv.org/abs/2403.06092v1,2024-03-10 04:27:06+00:00,"Neural Radiance Field (NeRF) has achieved superior performance for novel view
synthesis by modeling the scene with a Multi-Layer Perception (MLP) and a
volume rendering procedure, however, when fewer known views are given (i.e.,
few-shot view synthesis), the model is prone to overfit the given views. To
handle this issue, previous efforts have been made towards leveraging learned
priors or introducing additional regularizations. In contrast, in this paper,
we for the first time provide an orthogonal method from the perspective of
network structure. Given the observation that trivially reducing the number of
model parameters alleviates the overfitting issue, but at the cost of missing
details, we propose the multi-input MLP (mi-MLP) that incorporates the inputs
(i.e., location and viewing direction) of the vanilla MLP into each layer to
prevent the overfitting issue without harming detailed synthesis. To further
reduce the artifacts, we propose to model colors and volume density separately
and present two regularization terms. Extensive experiments on multiple
datasets demonstrate that: 1) although the proposed mi-MLP is easy to
implement, it is surprisingly effective as it boosts the PSNR of the baseline
from $14.73$ to $24.23$. 2) the overall framework achieves state-of-the-art
results on a wide range of benchmarks. We will release the code upon
publication.",0
Fish-inspired tracking of underwater turbulent plumes,2403.06091v1,http://arxiv.org/abs/2403.06091v1,2024-03-10 04:24:29+00:00,"Autonomous ocean-exploring vehicles have begun to take advantage of onboard
sensor measurements of water properties such as salinity and temperature to
locate oceanic features in real time. Such targeted sampling strategies enable
more rapid study of ocean environments by actively steering towards areas of
high scientific value. Inspired by the ability of aquatic animals to navigate
via flow sensing, this work investigates hydrodynamic cues for accomplishing
targeted sampling using a palm-sized robotic swimmer. As proof-of-concept
analogy for tracking hydrothermal vent plumes in the ocean, the robot is tasked
with locating the center of turbulent jet flows in a 13,000-liter water tank
using data from onboard pressure sensors. To learn a navigation strategy, we
first implemented Reinforcement Learning (RL) on a simulated version of the
robot navigating in proximity to turbulent jets. After training, the RL
algorithm discovered an effective strategy for locating the jets by following
transverse velocity gradients sensed by pressure sensors located on opposite
sides of the robot. When implemented on the physical robot, this gradient
following strategy enabled the robot to successfully locate the turbulent
plumes at more than twice the rate of random searching. Additionally, we found
that navigation performance improved as the distance between the pressure
sensors increased, which can inform the design of distributed flow sensors in
ocean robots. Our results demonstrate the effectiveness and limits of
flow-based navigation for autonomously locating hydrodynamic features of
interest.",0
Knowledge Distillation of Convolutional Neural Networks through Feature Map Transformation using Decision Trees,2403.06089v1,http://arxiv.org/abs/2403.06089v1,2024-03-10 04:20:51+00:00,"The interpretation of reasoning by Deep Neural Networks (DNN) is still
challenging due to their perceived black-box nature. Therefore, deploying DNNs
in several real-world tasks is restricted by the lack of transparency of these
models. We propose a distillation approach by extracting features from the
final layer of the convolutional neural network (CNN) to address insights to
its reasoning. The feature maps in the final layer of a CNN are transformed
into a one-dimensional feature vector using a fully connected layer.
Subsequently, the extracted features are used to train a decision tree to
achieve the best accuracy under constraints of depth and nodes. We use the
medical images of dermaMNIST, octMNIST, and pneumoniaMNIST from the medical
MNIST datasets to demonstrate our proposed work. We observed that performance
of the decision tree is as good as a CNN with minimum complexity. The results
encourage interpreting decisions made by the CNNs using decision trees.",0
Towards In-Vehicle Multi-Task Facial Attribute Recognition: Investigating Synthetic Data and Vision Foundation Models,2403.06088v1,http://arxiv.org/abs/2403.06088v1,2024-03-10 04:17:54+00:00,"In the burgeoning field of intelligent transportation systems, enhancing
vehicle-driver interaction through facial attribute recognition, such as facial
expression, eye gaze, age, etc., is of paramount importance for safety,
personalization, and overall user experience. However, the scarcity of
comprehensive large-scale, real-world datasets poses a significant challenge
for training robust multi-task models. Existing literature often overlooks the
potential of synthetic datasets and the comparative efficacy of
state-of-the-art vision foundation models in such constrained settings. This
paper addresses these gaps by investigating the utility of synthetic datasets
for training complex multi-task models that recognize facial attributes of
passengers of a vehicle, such as gaze plane, age, and facial expression.
Utilizing transfer learning techniques with both pre-trained Vision Transformer
(ViT) and Residual Network (ResNet) models, we explore various training and
adaptation methods to optimize performance, particularly when data availability
is limited. We provide extensive post-evaluation analysis, investigating the
effects of synthetic data distributions on model performance in in-distribution
data and out-of-distribution inference. Our study unveils counter-intuitive
findings, notably the superior performance of ResNet over ViTs in our specific
multi-task context, which is attributed to the mismatch in model complexity
relative to task complexity. Our results highlight the challenges and
opportunities for enhancing the use of synthetic data and vision foundation
models in practical applications.",0
Learning the irreversible progression trajectory of Alzheimer's disease,2403.06087v1,http://arxiv.org/abs/2403.06087v1,2024-03-10 04:17:42+00:00,"Alzheimer's disease (AD) is a progressive and irreversible brain disorder
that unfolds over the course of 30 years. Therefore, it is critical to capture
the disease progression in an early stage such that intervention can be applied
before the onset of symptoms. Machine learning (ML) models have been shown
effective in predicting the onset of AD. Yet for subjects with follow-up
visits, existing techniques for AD classification only aim for accurate group
assignment, where the monotonically increasing risk across follow-up visits is
usually ignored. Resulted fluctuating risk scores across visits violate the
irreversibility of AD, hampering the trustworthiness of models and also
providing little value to understanding the disease progression. To address
this issue, we propose a novel regularization approach to predict AD
longitudinally. Our technique aims to maintain the expected monotonicity of
increasing disease risk during progression while preserving expressiveness.
Specifically, we introduce a monotonicity constraint that encourages the model
to predict disease risk in a consistent and ordered manner across follow-up
visits. We evaluate our method using the longitudinal structural MRI and
amyloid-PET imaging data from the Alzheimer's Disease Neuroimaging Initiative
(ADNI). Our model outperforms existing techniques in capturing the
progressiveness of disease risk, and at the same time preserves prediction
accuracy.",0
Towards Generalizable and Interpretable Motion Prediction: A Deep Variational Bayes Approach,2403.06086v1,http://arxiv.org/abs/2403.06086v1,2024-03-10 04:16:04+00:00,"Estimating the potential behavior of the surrounding human-driven vehicles is
crucial for the safety of autonomous vehicles in a mixed traffic flow. Recent
state-of-the-art achieved accurate prediction using deep neural networks.
However, these end-to-end models are usually black boxes with weak
interpretability and generalizability. This paper proposes the Goal-based
Neural Variational Agent (GNeVA), an interpretable generative model for motion
prediction with robust generalizability to out-of-distribution cases. For
interpretability, the model achieves target-driven motion prediction by
estimating the spatial distribution of long-term destinations with a
variational mixture of Gaussians. We identify a causal structure among maps and
agents' histories and derive a variational posterior to enhance
generalizability. Experiments on motion prediction datasets validate that the
fitted model can be interpretable and generalizable and can achieve comparable
performance to state-of-the-art results.",0
pETNNs: Partial Evolutionary Tensor Neural Networks for Solving Time-dependent Partial Differential Equations,2403.06084v1,http://arxiv.org/abs/2403.06084v1,2024-03-10 04:05:45+00:00,"We present partial evolutionary tensor neural networks (pETNNs), a novel
framework for solving time-dependent partial differential equations with both
of high accuracy and remarkable extrapolation. Our proposed architecture
leverages the inherent accuracy of tensor neural networks, while incorporating
evolutionary parameters that enable remarkable extrapolation capabilities. By
adopting innovative parameter update strategies, the pETNNs achieve a
significant reduction in computational cost while maintaining precision and
robustness. Notably, the pETNNs enhance the accuracy of conventional
evolutional deep neural networks and empowers computational abilities to
address high-dimensional problems. Numerical experiments demonstrate the
superior performance of the pETNNs in solving time-dependent complex equations,
including the Navier-Stokes equations, high-dimensional heat equation,
high-dimensional transport equation and Korteweg-de Vries type equation.",0
FrameQuant: Flexible Low-Bit Quantization for Transformers,2403.06082v1,http://arxiv.org/abs/2403.06082v1,2024-03-10 04:01:49+00:00,"Transformers are the backbone of powerful foundation models for many Vision
and Natural Language Processing tasks. But their compute and memory/storage
footprint is large, and so, serving such models is expensive often requiring
high-end hardware. To mitigate this difficulty, Post-Training Quantization
seeks to modify a pre-trained model and quantize it to eight bits or lower,
significantly boosting compute/memory/latency efficiency. Such models have been
successfully quantized to four bits with some performance loss. In this work,
we outline a simple scheme to quantize Transformer-based models to just two
bits (plus some overhead) with only a small drop in accuracy. Key to our
formulation is a concept borrowed from Harmonic analysis called Fusion Frames.
Our main finding is that the quantization must take place not in the original
weight space, but instead in the Fusion Frame representations. If quantization
is interpreted as the addition of noise, our casting of the problem allows
invoking an extensive body of known consistent recovery and noise robustness
guarantees. Further, if desired, de-noising filters are known in closed form.
We show empirically, via a variety of experiments, that (almost) two-bit
quantization for Transformer models promises sizable efficiency gains.",0
Local Vertex Colouring Graph Neural Networks,2403.06080v1,http://arxiv.org/abs/2403.06080v1,2024-03-10 03:59:24+00:00,"In recent years, there has been a significant amount of research focused on
expanding the expressivity of Graph Neural Networks (GNNs) beyond the
Weisfeiler-Lehman (1-WL) framework. While many of these studies have yielded
advancements in expressivity, they have frequently come at the expense of
decreased efficiency or have been restricted to specific types of graphs. In
this study, we investigate the expressivity of GNNs from the perspective of
graph search. Specifically, we propose a new vertex colouring scheme and
demonstrate that classical search algorithms can efficiently compute graph
representations that extend beyond the 1-WL. We show the colouring scheme
inherits useful properties from graph search that can help solve problems like
graph biconnectivity. Furthermore, we show that under certain conditions, the
expressivity of GNNs increases hierarchically with the radius of the search
neighbourhood. To further investigate the proposed scheme, we develop a new
type of GNN based on two search strategies, breadth-first search and
depth-first search, highlighting the graph properties they can capture on top
of 1-WL. Our code is available at https://github.com/seanli3/lvc.",0
Generalization of Graph Neural Networks through the Lens of Homomorphism,2403.06079v1,http://arxiv.org/abs/2403.06079v1,2024-03-10 03:51:59+00:00,"Despite the celebrated popularity of Graph Neural Networks (GNNs) across
numerous applications, the ability of GNNs to generalize remains less explored.
In this work, we propose to study the generalization of GNNs through a novel
perspective - analyzing the entropy of graph homomorphism. By linking graph
homomorphism with information-theoretic measures, we derive generalization
bounds for both graph and node classifications. These bounds are capable of
capturing subtleties inherent in various graph structures, including but not
limited to paths, cycles and cliques. This enables a data-dependent
generalization analysis with robust theoretical guarantees. To shed light on
the generality of of our proposed bounds, we present a unifying framework that
can characterize a broad spectrum of GNN models through the lens of graph
homomorphism. We validate the practical applicability of our theoretical
findings by showing the alignment between the proposed bounds and the
empirically observed generalization gaps over both real-world and synthetic
datasets.",0
Reframe Anything: LLM Agent for Open World Video Reframing,2403.06070v1,http://arxiv.org/abs/2403.06070v1,2024-03-10 03:29:56+00:00,"The proliferation of mobile devices and social media has revolutionized
content dissemination, with short-form video becoming increasingly prevalent.
This shift has introduced the challenge of video reframing to fit various
screen aspect ratios, a process that highlights the most compelling parts of a
video. Traditionally, video reframing is a manual, time-consuming task
requiring professional expertise, which incurs high production costs. A
potential solution is to adopt some machine learning models, such as video
salient object detection, to automate the process. However, these methods often
lack generalizability due to their reliance on specific training data. The
advent of powerful large language models (LLMs) open new avenues for AI
capabilities. Building on this, we introduce Reframe Any Video Agent (RAVA), a
LLM-based agent that leverages visual foundation models and human instructions
to restructure visual content for video reframing. RAVA operates in three
stages: perception, where it interprets user instructions and video content;
planning, where it determines aspect ratios and reframing strategies; and
execution, where it invokes the editing tools to produce the final video. Our
experiments validate the effectiveness of RAVA in video salient object
detection and real-world reframing tasks, demonstrating its potential as a tool
for AI-powered video editing.",0
Implicit Image-to-Image Schrodinger Bridge for CT Super-Resolution and Denoising,2403.06069v1,http://arxiv.org/abs/2403.06069v1,2024-03-10 03:22:57+00:00,"Conditional diffusion models have gained recognition for their effectiveness
in image restoration tasks, yet their iterative denoising process, starting
from Gaussian noise, often leads to slow inference speeds. As a promising
alternative, the Image-to-Image Schr\""odinger Bridge (I2SB) initializes the
generative process from corrupted images and integrates training techniques
from conditional diffusion models. In this study, we extended the I2SB method
by introducing the Implicit Image-to-Image Schrodinger Bridge (I3SB),
transitioning its generative process to a non-Markovian process by
incorporating corrupted images in each generative step. This enhancement
empowers I3SB to generate images with better texture restoration using a small
number of generative steps. The proposed method was validated on CT
super-resolution and denoising tasks and outperformed existing methods,
including the conditional denoising diffusion probabilistic model (cDDPM) and
I2SB, in both visual quality and quantitative metrics. These findings
underscore the potential of I3SB in improving medical image restoration by
providing fast and accurate generative modeling.",0
The AL$\ell_0$CORE Tensor Decomposition for Sparse Count Data,2403.06153v2,http://arxiv.org/abs/2403.06153v2,2024-03-10 09:54:56+00:00,"This paper introduces AL$\ell_0$CORE, a new form of probabilistic
non-negative tensor decomposition. AL$\ell_0$CORE is a Tucker decomposition
where the number of non-zero elements (i.e., the $\ell_0$-norm) of the core
tensor is constrained to a preset value $Q$ much smaller than the size of the
core. While the user dictates the total budget $Q$, the locations and values of
the non-zero elements are latent variables and allocated across the core tensor
during inference. AL$\ell_0$CORE -- i.e., $allo$cated $\ell_0$-$co$nstrained
$core$-- thus enjoys both the computational tractability of CP decomposition
and the qualitatively appealing latent structure of Tucker. In a suite of
real-data experiments, we demonstrate that AL$\ell_0$CORE typically requires
only tiny fractions (e.g.,~1%) of the full core to achieve the same results as
full Tucker decomposition at only a correspondingly tiny fraction of the cost.",0
Cracking the neural code for word recognition in convolutional neural networks,2403.06159v1,http://arxiv.org/abs/2403.06159v1,2024-03-10 10:12:32+00:00,"Learning to read places a strong challenge on the visual system. Years of
expertise lead to a remarkable capacity to separate highly similar letters and
encode their relative positions, thus distinguishing words such as FORM and
FROM, invariantly over a large range of sizes and absolute positions. How
neural circuits achieve invariant word recognition remains unknown. Here, we
address this issue by training deep neural network models to recognize written
words and then analyzing how reading-specialized units emerge and operate
across different layers of the network. With literacy, a small subset of units
becomes specialized for word recognition in the learned script, similar to the
""visual word form area"" of the human brain. We show that these units are
sensitive to specific letter identities and their distance from the blank space
at the left or right of a word, thus acting as ""space bigrams"". These units
specifically encode ordinal positions and operate by pooling across low and
high-frequency detector units from early layers of the network. The proposed
neural code provides a mechanistic insight into how information on letter
identity and position is extracted and allow for invariant word recognition,
and leads to predictions for reading behavior, error patterns, and the
neurophysiology of reading.",0
Physics-Guided Abnormal Trajectory Gap Detection,2403.06268v1,http://arxiv.org/abs/2403.06268v1,2024-03-10 17:07:28+00:00,"Given trajectories with gaps (i.e., missing data), we investigate algorithms
to identify abnormal gaps in trajectories which occur when a given moving
object did not report its location, but other moving objects in the same
geographic region periodically did. The problem is important due to its
societal applications, such as improving maritime safety and regulatory
enforcement for global security concerns such as illegal fishing, illegal oil
transfers, and trans-shipments. The problem is challenging due to the
difficulty of bounding the possible locations of the moving object during a
trajectory gap, and the very high computational cost of detecting gaps in such
a large volume of location data. The current literature on anomalous trajectory
detection assumes linear interpolation within gaps, which may not be able to
detect abnormal gaps since objects within a given region may have traveled away
from their shortest path. In preliminary work, we introduced an abnormal gap
measure that uses a classical space-time prism model to bound an object's
possible movement during the trajectory gap and provided a scalable memoized
gap detection algorithm (Memo-AGD). In this paper, we propose a Space
Time-Aware Gap Detection (STAGD) approach to leverage space-time indexing and
merging of trajectory gaps. We also incorporate a Dynamic Region Merge-based
(DRM) approach to efficiently compute gap abnormality scores. We provide
theoretical proofs that both algorithms are correct and complete and also
provide analysis of asymptotic time complexity. Experimental results on
synthetic and real-world maritime trajectory data show that the proposed
approach substantially improves computation time over the baseline technique.",0
Speeding up 6-DoF Grasp Sampling with Quality-Diversity,2403.06173v1,http://arxiv.org/abs/2403.06173v1,2024-03-10 10:58:54+00:00,"Recent advances in AI have led to significant results in robotic learning,
including natural language-conditioned planning and efficient optimization of
controllers using generative models. However, the interaction data remains the
bottleneck for generalization. Getting data for grasping is a critical
challenge, as this skill is required to complete many manipulation tasks.
Quality-Diversity (QD) algorithms optimize a set of solutions to get diverse,
high-performing solutions to a given problem. This paper investigates how QD
can be combined with priors to speed up the generation of diverse grasps poses
in simulation compared to standard 6-DoF grasp sampling schemes. Experiments
conducted on 4 grippers with 2-to-5 fingers on standard objects show that QD
outperforms commonly used methods by a large margin. Further experiments show
that QD optimization automatically finds some efficient priors that are usually
hard coded. The deployment of generated grasps on a 2-finger gripper and an
Allegro hand shows that the diversity produced maintains sim-to-real
transferability. We believe these results to be a significant step toward the
generation of large datasets that can lead to robust and generalizing robotic
grasping policies.",0
FARPLS: A Feature-Augmented Robot Trajectory Preference Labeling System to Assist Human Labelers' Preference Elicitation,2403.06267v1,http://arxiv.org/abs/2403.06267v1,2024-03-10 17:07:20+00:00,"Preference-based learning aims to align robot task objectives with human
values. One of the most common methods to infer human preferences is by
pairwise comparisons of robot task trajectories. Traditional comparison-based
preference labeling systems seldom support labelers to digest and identify
critical differences between complex trajectories recorded in videos. Our
formative study (N = 12) suggests that individuals may overlook non-salient
task features and establish biased preference criteria during their preference
elicitation process because of partial observations. In addition, they may
experience mental fatigue when given many pairs to compare, causing their label
quality to deteriorate. To mitigate these issues, we propose FARPLS, a
Feature-Augmented Robot trajectory Preference Labeling System. FARPLS
highlights potential outliers in a wide variety of task features that matter to
humans and extracts the corresponding video keyframes for easy review and
comparison. It also dynamically adjusts the labeling order according to users'
familiarities, difficulties of the trajectory pair, and level of disagreements.
At the same time, the system monitors labelers' consistency and provides
feedback on labeling progress to keep labelers engaged. A between-subjects
study (N = 42, 105 pairs of robot pick-and-place trajectories per person) shows
that FARPLS can help users establish preference criteria more easily and notice
more relevant details in the presented trajectories than the conventional
interface. FARPLS also improves labeling consistency and engagement, mitigating
challenges in preference elicitation without raising cognitive loads
significantly",0
Unpacking Tokenization: Evaluating Text Compression and its Correlation with Model Performance,2403.06265v1,http://arxiv.org/abs/2403.06265v1,2024-03-10 17:02:53+00:00,"Despite it being the cornerstone of BPE, the most common tokenization
algorithm, the importance of compression in the tokenization process is still
unclear. In this paper, we argue for the theoretical importance of compression,
that can be viewed as 0-gram language modeling where equal probability is
assigned to all tokens. We also demonstrate the empirical importance of
compression for downstream success of pre-trained language models. We control
the compression ability of several BPE tokenizers by varying the amount of
documents available during their training: from 1 million documents to a
character-based tokenizer equivalent to no training data at all. We then
pre-train English language models based on those tokenizers and fine-tune them
over several tasks. We show that there is a correlation between tokenizers'
compression and models' downstream performance, suggesting that compression is
a reliable intrinsic indicator of tokenization quality. These correlations are
more pronounced for generation tasks (over classification) or for smaller
models (over large ones). We replicated a representative part of our
experiments on Turkish and found similar results, confirming that our results
hold for languages with typological characteristics dissimilar to English. We
conclude that building better compressing tokenizers is a fruitful avenue for
further research and for improving overall model performance.",0
SCORE: Self-supervised Correspondence Fine-tuning for Improved Content Representations,2403.06260v1,http://arxiv.org/abs/2403.06260v1,2024-03-10 16:57:51+00:00,"There is a growing interest in cost-effective self-supervised fine-tuning
(SSFT) of self-supervised learning (SSL)-based speech models to obtain
task-specific representations. These task-specific representations are used for
robust performance on various downstream tasks by fine-tuning on the labelled
data. This work presents a cost-effective SSFT method named Self-supervised
Correspondence (SCORE) fine-tuning to adapt the SSL speech representations for
content-related tasks. The proposed method uses a correspondence training
strategy, aiming to learn similar representations from perturbed speech and
original speech. Commonly used data augmentation techniques for content-related
tasks (ASR) are applied to obtain perturbed speech. SCORE fine-tuned HuBERT
outperforms the vanilla HuBERT on SUPERB benchmark with only a few hours of
fine-tuning (< 5 hrs) on a single GPU for automatic speech recognition, phoneme
recognition, and query-by-example tasks, with relative improvements of 1.09%,
3.58%, and 12.65%, respectively. SCORE provides competitive results with the
recently proposed SSFT method SPIN, using only 1/3 of the processed speech
compared to SPIN.",0
Editing Conceptual Knowledge for Large Language Models,2403.06259v1,http://arxiv.org/abs/2403.06259v1,2024-03-10 16:57:10+00:00,"Recently, there has been a growing interest in knowledge editing for Large
Language Models (LLMs). Current approaches and evaluations merely explore the
instance-level editing, while whether LLMs possess the capability to modify
concepts remains unclear. This paper pioneers the investigation of editing
conceptual knowledge for LLMs, by constructing a novel benchmark dataset
ConceptEdit and establishing a suite of new metrics for evaluation. The
experimental results reveal that, although existing editing methods can
efficiently modify concept-level definition to some extent, they also have the
potential to distort the related instantial knowledge in LLMs, leading to poor
performance. We anticipate this can inspire further progress in better
understanding LLMs. Our project homepage is available at
https://zjunlp.github.io/project/ConceptEdit.",0
Online Multi-spectral Neuron Tracing,2403.06251v1,http://arxiv.org/abs/2403.06251v1,2024-03-10 16:34:21+00:00,"In this paper, we propose an online multi-spectral neuron tracing method with
uniquely designed modules, where no offline training are required. Our method
is trained online to update our enhanced discriminative correlation filter to
conglutinate the tracing process. This distinctive offline-training-free schema
differentiates us from other training-dependent tracing approaches like deep
learning methods since no annotation is needed for our method. Besides,
compared to other tracing methods requiring complicated set-up such as for
clustering and graph multi-cut, our approach is much easier to be applied to
new images. In fact, it only needs a starting bounding box of the tracing
neuron, significantly reducing users' configuration effort. Our extensive
experiments show that our training-free and easy-configured methodology allows
fast and accurate neuron reconstructions in multi-spectral images.",0
Text-Guided Variational Image Generation for Industrial Anomaly Detection and Segmentation,2403.06247v1,http://arxiv.org/abs/2403.06247v1,2024-03-10 16:11:17+00:00,"We propose a text-guided variational image generation method to address the
challenge of getting clean data for anomaly detection in industrial
manufacturing. Our method utilizes text information about the target object,
learned from extensive text library documents, to generate non-defective data
images resembling the input image. The proposed framework ensures that the
generated non-defective images align with anticipated distributions derived
from textual and image-based knowledge, ensuring stability and generality.
Experimental results demonstrate the effectiveness of our approach, surpassing
previous methods even with limited non-defective data. Our approach is
validated through generalization tests across four baseline models and three
distinct datasets. We present an additional analysis to enhance the
effectiveness of anomaly detection models by utilizing the generated images.",0
WorldGPT: A Sora-Inspired Video AI Agent as Rich World Models from Text and Image Inputs,2403.07944v1,http://arxiv.org/abs/2403.07944v1,2024-03-10 16:09:02+00:00,"Several text-to-video diffusion models have demonstrated commendable
capabilities in synthesizing high-quality video content. However, it remains a
formidable challenge pertaining to maintaining temporal consistency and
ensuring action smoothness throughout the generated sequences. In this paper,
we present an innovative video generation AI agent that harnesses the power of
Sora-inspired multimodal learning to build skilled world models framework based
on textual prompts and accompanying images. The framework includes two parts:
prompt enhancer and full video translation. The first part employs the
capabilities of ChatGPT to meticulously distill and proactively construct
precise prompts for each subsequent step, thereby guaranteeing the utmost
accuracy in prompt communication and accurate execution in following model
operations. The second part employ compatible with existing advanced diffusion
techniques to expansively generate and refine the key frame at the conclusion
of a video. Then we can expertly harness the power of leading and trailing key
frames to craft videos with enhanced temporal consistency and action
smoothness. The experimental results confirm that our method has strong
effectiveness and novelty in constructing world models from text and image
inputs over the other methods.",0
A Dataset for the Validation of Truth Inference Algorithms Suitable for Online Deployment,2403.08826v1,http://arxiv.org/abs/2403.08826v1,2024-03-10 16:00:41+00:00,"For the purpose of efficient and cost-effective large-scale data labeling,
crowdsourcing is increasingly being utilized. To guarantee the quality of data
labeling, multiple annotations need to be collected for each data sample, and
truth inference algorithms have been developed to accurately infer the true
labels. Despite previous studies having released public datasets to evaluate
the efficacy of truth inference algorithms, these have typically focused on a
single type of crowdsourcing task and neglected the temporal information
associated with workers' annotation activities. These limitations significantly
restrict the practical applicability of these algorithms, particularly in the
context of long-term and online truth inference. In this paper, we introduce a
substantial crowdsourcing annotation dataset collected from a real-world
crowdsourcing platform. This dataset comprises approximately two thousand
workers, one million tasks, and six million annotations. The data was gathered
over a period of approximately six months from various types of tasks, and the
timestamps of each annotation were preserved. We analyze the characteristics of
the dataset from multiple perspectives and evaluate the effectiveness of
several representative truth inference algorithms on this dataset. We
anticipate that this dataset will stimulate future research on tracking
workers' abilities over time in relation to different types of tasks, as well
as enhancing online truth inference.",0
BlazeBVD: Make Scale-Time Equalization Great Again for Blind Video Deflickering,2403.06243v1,http://arxiv.org/abs/2403.06243v1,2024-03-10 15:56:55+00:00,"Developing blind video deflickering (BVD) algorithms to enhance video
temporal consistency, is gaining importance amid the flourish of image
processing and video generation. However, the intricate nature of video data
complicates the training of deep learning methods, leading to high resource
consumption and instability, notably under severe lighting flicker. This
underscores the critical need for a compact representation beyond pixel values
to advance BVD research and applications. Inspired by the classic scale-time
equalization (STE), our work introduces the histogram-assisted solution, called
BlazeBVD, for high-fidelity and rapid BVD. Compared with STE, which directly
corrects pixel values by temporally smoothing color histograms, BlazeBVD
leverages smoothed illumination histograms within STE filtering to ease the
challenge of learning temporal data using neural networks. In technique,
BlazeBVD begins by condensing pixel values into illumination histograms that
precisely capture flickering and local exposure variations. These histograms
are then smoothed to produce singular frames set, filtered illumination maps,
and exposure maps. Resorting to these deflickering priors, BlazeBVD utilizes a
2D network to restore faithful and consistent texture impacted by lighting
changes or localized exposure issues. BlazeBVD also incorporates a lightweight
3D network to amend slight temporal inconsistencies, avoiding the resource
consumption issue. Comprehensive experiments on synthetic, real-world and
generated videos, showcase the superior qualitative and quantitative results of
BlazeBVD, achieving inference speeds up to 10x faster than state-of-the-arts.",0
Revisiting Edge Perturbation for Graph Neural Network in Graph Data Augmentation and Attack,2403.07943v1,http://arxiv.org/abs/2403.07943v1,2024-03-10 15:50:04+00:00,"Edge perturbation is a basic method to modify graph structures. It can be
categorized into two veins based on their effects on the performance of graph
neural networks (GNNs), i.e., graph data augmentation and attack. Surprisingly,
both veins of edge perturbation methods employ the same operations, yet yield
opposite effects on GNNs' accuracy. A distinct boundary between these methods
in using edge perturbation has never been clearly defined. Consequently,
inappropriate perturbations may lead to undesirable outcomes, necessitating
precise adjustments to achieve desired effects. Therefore, questions of ``why
edge perturbation has a two-faced effect?'' and ``what makes edge perturbation
flexible and effective?'' still remain unanswered.
  In this paper, we will answer these questions by proposing a unified
formulation and establishing a clear boundary between two categories of edge
perturbation methods. Specifically, we conduct experiments to elucidate the
differences and similarities between these methods and theoretically unify the
workflow of these methods by casting it to one optimization problem. Then, we
devise Edge Priority Detector (EPD) to generate a novel priority metric,
bridging these methods up in the workflow. Experiments show that EPD can make
augmentation or attack flexibly and achieve comparable or superior performance
to other counterparts with less time overhead.",0
Whiteness-based bilevel learning of regularization parameters in imaging,2403.07026v1,http://arxiv.org/abs/2403.07026v1,2024-03-10 15:45:39+00:00,"We consider an unsupervised bilevel optimization strategy for learning
regularization parameters in the context of imaging inverse problems in the
presence of additive white Gaussian noise. Compared to supervised and
semi-supervised metrics relying either on the prior knowledge of reference data
and/or on some (partial) knowledge on the noise statistics, the proposed
approach optimizes the whiteness of the residual between the observed data and
the observation model with no need of ground-truth data.We validate the
approach on standard Total Variation-regularized image deconvolution problems
which show that the proposed quality metric provides estimates close to the
mean-square error oracle and to discrepancy-based principles.",0
Cooperative Classification and Rationalization for Graph Generalization,2403.06239v1,http://arxiv.org/abs/2403.06239v1,2024-03-10 15:38:20+00:00,"Graph Neural Networks (GNNs) have achieved impressive results in graph
classification tasks, but they struggle to generalize effectively when faced
with out-of-distribution (OOD) data. Several approaches have been proposed to
address this problem. Among them, one solution is to diversify training
distributions in vanilla classification by modifying the data environment, yet
accessing the environment information is complex. Besides, another promising
approach involves rationalization, extracting invariant rationales for
predictions. However, extracting rationales is difficult due to limited
learning signals, resulting in less accurate rationales and diminished
predictions. To address these challenges, in this paper, we propose a
Cooperative Classification and Rationalization (C2R) method, consisting of the
classification and the rationalization module. Specifically, we first assume
that multiple environments are available in the classification module. Then, we
introduce diverse training distributions using an environment-conditional
generative network, enabling robust graph representations. Meanwhile, the
rationalization module employs a separator to identify relevant rationale
subgraphs while the remaining non-rationale subgraphs are de-correlated with
labels. Next, we align graph representations from the classification module
with rationale subgraph representations using the knowledge distillation
methods, enhancing the learning signal for rationales. Finally, we infer
multiple environments by gathering non-rationale representations and
incorporate them into the classification module for cooperative learning.
Extensive experimental results on both benchmarks and synthetic datasets
demonstrate the effectiveness of C2R. Code is available at
https://github.com/yuelinan/Codes-of-C2R.",0
Enhancing Quantum Variational Algorithms with Zero Noise Extrapolation via Neural Networks,2403.07025v1,http://arxiv.org/abs/2403.07025v1,2024-03-10 15:35:41+00:00,"In the emergent realm of quantum computing, the Variational Quantum
Eigensolver (VQE) stands out as a promising algorithm for solving complex
quantum problems, especially in the noisy intermediate-scale quantum (NISQ)
era. However, the ubiquitous presence of noise in quantum devices often limits
the accuracy and reliability of VQE outcomes. This research introduces a novel
approach to ameliorate this challenge by utilizing neural networks for zero
noise extrapolation (ZNE) in VQE computations. By employing the Qiskit
framework, we crafted parameterized quantum circuits using the RY-RZ ansatz and
examined their behavior under varying levels of depolarizing noise. Our
investigations spanned from determining the expectation values of a
Hamiltonian, defined as a tensor product of Z operators, under different noise
intensities to extracting the ground state energy. To bridge the observed
outcomes under noise with the ideal noise-free scenario, we trained a Feed
Forward Neural Network on the error probabilities and their associated
expectation values. Remarkably, our model proficiently predicted the VQE
outcome under hypothetical noise-free conditions. By juxtaposing the simulation
results with real quantum device executions, we unveiled the discrepancies
induced by noise and showcased the efficacy of our neural network-based ZNE
technique in rectifying them. This integrative approach not only paves the way
for enhanced accuracy in VQE computations on NISQ devices but also underlines
the immense potential of hybrid quantum-classical paradigms in circumventing
the challenges posed by quantum noise. Through this research, we envision a
future where quantum algorithms can be reliably executed on noisy devices,
bringing us one step closer to realizing the full potential of quantum
computing.",0
Aqueous Solution Chemistry In Silico and the Role of Data Driven Approaches,2403.06236v1,http://arxiv.org/abs/2403.06236v1,2024-03-10 15:25:53+00:00,"The use of computer simulations to study the properties of aqueous systems
is, today more than ever, an active area of research. In this context, during
the last decade there has been a tremendous growth in the use of data-driven
approaches to develop more accurate potentials for water as well as to
characterize its complexity in chemical and biological contexts. We highlight
the progress, giving a historical context, on the path to the development of
many-body and reactive potentials to model aqueous chemistry, including the
role of machine learning strategies. We focus specifically on conceptual and
methodological challenges along the way in performing simulations that seek to
tackle problems in modeling the chemistry of aqueous solutions. In conclusion,
we summarize our perspectives on the use and integration of advanced
data-science techniques to provide chemical insights in physical chemistry and
how this will influence computer simulations of aqueous systems in the future.",0
Probabilistic Neural Circuits,2403.06235v1,http://arxiv.org/abs/2403.06235v1,2024-03-10 15:25:49+00:00,"Probabilistic circuits (PCs) have gained prominence in recent years as a
versatile framework for discussing probabilistic models that support tractable
queries and are yet expressive enough to model complex probability
distributions. Nevertheless, tractability comes at a cost: PCs are less
expressive than neural networks. In this paper we introduce probabilistic
neural circuits (PNCs), which strike a balance between PCs and neural nets in
terms of tractability and expressive power. Theoretically, we show that PNCs
can be interpreted as deep mixtures of Bayesian networks. Experimentally, we
demonstrate that PNCs constitute powerful function approximators.",0
LinearAPT: An Adaptive Algorithm for the Fixed-Budget Thresholding Linear Bandit Problem,2403.06230v1,http://arxiv.org/abs/2403.06230v1,2024-03-10 15:01:50+00:00,"In this study, we delve into the Thresholding Linear Bandit (TLB) problem, a
nuanced domain within stochastic Multi-Armed Bandit (MAB) problems, focusing on
maximizing decision accuracy against a linearly defined threshold under
resource constraints. We present LinearAPT, a novel algorithm designed for the
fixed budget setting of TLB, providing an efficient solution to optimize
sequential decision-making. This algorithm not only offers a theoretical upper
bound for estimated loss but also showcases robust performance on both
synthetic and real-world datasets. Our contributions highlight the
adaptability, simplicity, and computational efficiency of LinearAPT, making it
a valuable addition to the toolkit for addressing complex sequential
decision-making challenges.",0
PEPSI: Pathology-Enhanced Pulse-Sequence-Invariant Representations for Brain MRI,2403.06227v1,http://arxiv.org/abs/2403.06227v1,2024-03-10 14:33:55+00:00,"Remarkable progress has been made by data-driven machine-learning methods in
the analysis of MRI scans. However, most existing MRI analysis approaches are
crafted for specific MR pulse sequences (MR contrasts) and usually require
nearly isotropic acquisitions. This limits their applicability to diverse
real-world clinical data, where scans commonly exhibit variations in
appearances due to being obtained with varying sequence parameters,
resolutions, and orientations -- especially in the presence of pathology. In
this paper, we propose PEPSI, the first pathology-enhanced, and
pulse-sequence-invariant feature representation learning model for brain MRI.
PEPSI is trained entirely on synthetic images with a novel pathology encoding
strategy, and enables co-training across datasets with diverse pathologies and
missing modalities. Despite variations in pathology appearances across
different MR pulse sequences or the quality of acquired images (e.g.,
resolution, orientation, artifacts, etc), PEPSI produces a high-resolution
image of reference contrast (MP-RAGE) that captures anatomy, along with an
image specifically highlighting the pathology. Our experiments demonstrate
PEPSI's remarkable capability for image synthesis compared with the
state-of-the-art, contrast-agnostic synthesis models, as it accurately
reconstructs anatomical structures while differentiating between pathology and
normal tissue. We further illustrate the efficiency and effectiveness of PEPSI
features for downstream pathology segmentations on five public datasets
covering white matter hyperintensities and stroke lesions. Code is available at
https://github.com/peirong26/PEPSI.",0
Robust Predictive Motion Planning by Learning Obstacle Uncertainty,2403.06222v1,http://arxiv.org/abs/2403.06222v1,2024-03-10 13:59:18+00:00,"Safe motion planning for robotic systems in dynamic environments is
nontrivial in the presence of uncertain obstacles, where estimation of obstacle
uncertainties is crucial in predicting future motions of dynamic obstacles. The
worst-case characterization gives a conservative uncertainty prediction and may
result in infeasible motion planning for the ego robotic system. In this paper,
an efficient, robust, and safe motion-planing algorithm is developed by
learning the obstacle uncertainties online. More specifically, the unknown yet
intended control set of obstacles is efficiently computed by solving a linear
programming problem. The learned control set is used to compute forward
reachable sets of obstacles that are less conservative than the worst-case
prediction. Based on the forward prediction, a robust model predictive
controller is designed to compute a safe reference trajectory for the ego
robotic system that remains outside the reachable sets of obstacles over the
prediction horizon. The method is applied to a car-like mobile robot in both
simulations and hardware experiments to demonstrate its effectiveness.",0
$V_kD:$ Improving Knowledge Distillation using Orthogonal Projections,2403.06213v1,http://arxiv.org/abs/2403.06213v1,2024-03-10 13:26:24+00:00,"Knowledge distillation is an effective method for training small and
efficient deep learning models. However, the efficacy of a single method can
degenerate when transferring to other tasks, modalities, or even other
architectures. To address this limitation, we propose a novel constrained
feature distillation method. This method is derived from a small set of core
principles, which results in two emerging components: an orthogonal projection
and a task-specific normalisation. Equipped with both of these components, our
transformer models can outperform all previous methods on ImageNet and reach up
to a 4.4% relative improvement over the previous state-of-the-art methods. To
further demonstrate the generality of our method, we apply it to object
detection and image generation, whereby we obtain consistent and substantial
performance improvements over state-of-the-art. Code and models are publicly
available: https://github.com/roymiles/vkd",0
Personalized LoRA for Human-Centered Text Understanding,2403.06208v1,http://arxiv.org/abs/2403.06208v1,2024-03-10 13:04:54+00:00,"Effectively and efficiently adapting a pre-trained language model (PLM) for
human-centered text understanding (HCTU) is challenging since user tokens are
million-level in most personalized applications and do not have concrete
explicit semantics. A standard and parameter-efficient approach (e.g., LoRA)
necessitates memorizing numerous suits of adapters for each user. In this work,
we introduce a personalized LoRA (PLoRA) with a plug-and-play (PnP) framework
for the HCTU task. PLoRA is effective, parameter-efficient, and dynamically
deploying in PLMs. Moreover, a personalized dropout and a mutual information
maximizing strategies are adopted and hence the proposed PLoRA can be well
adapted to few/zero-shot learning scenarios for the cold-start issue.
Experiments conducted on four benchmark datasets show that the proposed method
outperforms existing methods in full/few/zero-shot learning scenarios for the
HCTU task, even though it has fewer trainable parameters. For reproducibility,
the code for this paper is available at: https://github.com/yoyo-yun/PLoRA.",0
Identifying and interpreting non-aligned human conceptual representations using language modeling,2403.06204v1,http://arxiv.org/abs/2403.06204v1,2024-03-10 13:02:27+00:00,"The question of whether people's experience in the world shapes conceptual
representation and lexical semantics is longstanding. Word-association,
feature-listing and similarity rating tasks aim to address this question but
require a subjective interpretation of the latent dimensions identified. In
this study, we introduce a supervised representational-alignment method that
(i) determines whether two groups of individuals share the same basis of a
certain category, and (ii) explains in what respects they differ. In applying
this method, we show that congenital blindness induces conceptual
reorganization in both a-modal and sensory-related verbal domains, and we
identify the associated semantic shifts. We first apply supervised
feature-pruning to a language model (GloVe) to optimize prediction accuracy of
human similarity judgments from word embeddings. Pruning identifies one subset
of retained GloVe features that optimizes prediction of judgments made by
sighted individuals and another subset that optimizes judgments made by blind.
A linear probing analysis then interprets the latent semantics of these
feature-subsets by learning a mapping from the retained GloVe features to 65
interpretable semantic dimensions. We applied this approach to seven semantic
domains, including verbs related to motion, sight, touch, and amodal verbs
related to knowledge acquisition. We find that blind individuals more strongly
associate social and cognitive meanings to verbs related to motion or those
communicating non-speech vocal utterances (e.g., whimper, moan). Conversely,
for amodal verbs, they demonstrate much sparser information. Finally, for some
verbs, representations of blind and sighted are highly similar. The study
presents a formal approach for studying interindividual differences in word
meaning, and the first demonstration of how blindness impacts conceptual
representation of everyday verbs.",0
Are You Being Tracked? Discover the Power of Zero-Shot Trajectory Tracing with LLMs!,2403.06201v1,http://arxiv.org/abs/2403.06201v1,2024-03-10 12:50:35+00:00,"There is a burgeoning discussion around the capabilities of Large Language
Models (LLMs) in acting as fundamental components that can be seamlessly
incorporated into Artificial Intelligence of Things (AIoT) to interpret complex
trajectories. This study introduces LLMTrack, a model that illustrates how LLMs
can be leveraged for Zero-Shot Trajectory Recognition by employing a novel
single-prompt technique that combines role-play and think step-by-step
methodologies with unprocessed Inertial Measurement Unit (IMU) data. We
evaluate the model using real-world datasets designed to challenge it with
distinct trajectories characterized by indoor and outdoor scenarios. In both
test scenarios, LLMTrack not only meets but exceeds the performance benchmarks
set by traditional machine learning approaches and even contemporary
state-of-the-art deep learning models, all without the requirement of training
on specialized datasets. The results of our research suggest that, with
strategically designed prompts, LLMs can tap into their extensive knowledge
base and are well-equipped to analyze raw sensor data with remarkable
effectiveness.",0
DrFuse: Learning Disentangled Representation for Clinical Multi-Modal Fusion with Missing Modality and Modal Inconsistency,2403.06197v1,http://arxiv.org/abs/2403.06197v1,2024-03-10 12:41:34+00:00,"The combination of electronic health records (EHR) and medical images is
crucial for clinicians in making diagnoses and forecasting prognosis.
Strategically fusing these two data modalities has great potential to improve
the accuracy of machine learning models in clinical prediction tasks. However,
the asynchronous and complementary nature of EHR and medical images presents
unique challenges. Missing modalities due to clinical and administrative
factors are inevitable in practice, and the significance of each data modality
varies depending on the patient and the prediction target, resulting in
inconsistent predictions and suboptimal model performance. To address these
challenges, we propose DrFuse to achieve effective clinical multi-modal fusion.
It tackles the missing modality issue by disentangling the features shared
across modalities and those unique within each modality. Furthermore, we
address the modal inconsistency issue via a disease-wise attention layer that
produces the patient- and disease-wise weighting for each modality to make the
final prediction. We validate the proposed method using real-world large-scale
datasets, MIMIC-IV and MIMIC-CXR. Experimental results show that the proposed
method significantly outperforms the state-of-the-art models. Our
implementation is publicly available at https://github.com/dorothy-yao/drfuse.",0
On depth prediction for autonomous driving using self-supervised learning,2403.06194v1,http://arxiv.org/abs/2403.06194v1,2024-03-10 12:33:12+00:00,"Perception of the environment is a critical component for enabling autonomous
driving. It provides the vehicle with the ability to comprehend its
surroundings and make informed decisions. Depth prediction plays a pivotal role
in this process, as it helps the understanding of the geometry and motion of
the environment. This thesis focuses on the challenge of depth prediction using
monocular self-supervised learning techniques. The problem is approached from a
broader perspective first, exploring conditional generative adversarial
networks (cGANs) as a potential technique to achieve better generalization was
performed. In doing so, a fundamental contribution to the conditional GANs, the
acontrario cGAN was proposed. The second contribution entails a single
image-to-depth self-supervised method, proposing a solution for the rigid-scene
assumption using a novel transformer-based method that outputs a pose for each
dynamic object. The third significant aspect involves the introduction of a
video-to-depth map forecasting approach. This method serves as an extension of
self-supervised techniques to predict future depths. This involves the creation
of a novel transformer model capable of predicting the future depth of a given
scene. Moreover, the various limitations of the aforementioned methods were
addressed and a video-to-video depth maps model was proposed. This model
leverages the spatio-temporal consistency of the input and output sequence to
predict a more accurate depth sequence output. These methods have significant
applications in autonomous driving (AD) and advanced driver assistance systems
(ADAS).",0
An Improved Analysis of Langevin Algorithms with Prior Diffusion for Non-Log-Concave Sampling,2403.06183v1,http://arxiv.org/abs/2403.06183v1,2024-03-10 11:50:34+00:00,"Understanding the dimension dependency of computational complexity in
high-dimensional sampling problem is a fundamental problem, both from a
practical and theoretical perspective. Compared with samplers with unbiased
stationary distribution, e.g., Metropolis-adjusted Langevin algorithm (MALA),
biased samplers, e.g., Underdamped Langevin Dynamics (ULD), perform better in
low-accuracy cases just because a lower dimension dependency in their
complexities. Along this line, Freund et al. (2022) suggest that the modified
Langevin algorithm with prior diffusion is able to converge dimension
independently for strongly log-concave target distributions. Nonetheless, it
remains open whether such property establishes for more general cases. In this
paper, we investigate the prior diffusion technique for the target
distributions satisfying log-Sobolev inequality (LSI), which covers a much
broader class of distributions compared to the strongly log-concave ones. In
particular, we prove that the modified Langevin algorithm can also obtain the
dimension-independent convergence of KL divergence with different step size
schedules. The core of our proof technique is a novel construction of an
interpolating SDE, which significantly helps to conduct a more accurate
characterization of the discrete updates of the overdamped Langevin dynamics.
Our theoretical analysis demonstrates the benefits of prior diffusion for a
broader class of target distributions and provides new insights into developing
faster sampling algorithms.",0
Estimating the mass of galactic components using machine learning algorithms,2403.06178v1,http://arxiv.org/abs/2403.06178v1,2024-03-10 11:30:38+00:00,"The estimation of the bulge and disk massses, the main baryonic components of
a galaxy, can be performed using various approaches, but their implementation
tend to be challenging as they often rely on strong assumptions about either
the baryon dynamics or the dark matter model. In this work, we present an
alternative method for predicting the masses of galactic components, including
the disk, bulge, stellar and total mass, using a set of machine learning
algorithms: KNN-neighbours (KNN), Linear Regression (LR), Random Forest (RF)
and Neural Network (NN). The rest-frame absolute magnitudes in the
ugriz-photometric system were selected as input features, and the training was
performed using a sample of spiral galaxies hosting a bulge from Guo's mock
catalogue \citep{Guo-Catalog} derived from the Millennium simulation. In
general, all the algorithms provide good predictions for the galaxy's mass
components ranging from $10^9\,M_\odot$ to $10^{11}\,M_\odot$, corresponding to
the central region of the training mass domain; however, the NN give rise to
the most precise predictions in comparison to other methods. Additionally, to
test the performance of the NN architecture, we used a sample of observed
galaxies from the SDSS survey whose mass components are known. We found that
the NN can predict the luminous masses of disk-dominant galaxies within the
same range of magnitudes that for the synthetic sample up to a $99\%$ level of
confidence, while mass components of galaxies hosting larger bulges are well
predicted up to $95\%$ level of confidence. The NN algorithm can also bring up
scaling relations between masses of different components and magnitudes.",0
Domain Adversarial Active Learning for Domain Generalization Classification,2403.06174v1,http://arxiv.org/abs/2403.06174v1,2024-03-10 10:59:22+00:00,"Domain generalization models aim to learn cross-domain knowledge from source
domain data, to improve performance on unknown target domains. Recent research
has demonstrated that diverse and rich source domain samples can enhance domain
generalization capability. This paper argues that the impact of each sample on
the model's generalization ability varies. Despite its small scale, a
high-quality dataset can still attain a certain level of generalization
ability. Motivated by this, we propose a domain-adversarial active learning
(DAAL) algorithm for classification tasks in domain generalization. First, we
analyze that the objective of tasks is to maximize the inter-class distance
within the same domain and minimize the intra-class distance across different
domains. To achieve this objective, we design a domain adversarial selection
method that prioritizes challenging samples. Second, we posit that even in a
converged model, there are subsets of features that lack discriminatory power
within each domain. We attempt to identify these feature subsets and optimize
them by a constraint loss. We validate and analyze our DAAL algorithm on
multiple domain generalization datasets, comparing it with various domain
generalization algorithms and active learning algorithms. Our results
demonstrate that the DAAL algorithm can achieve strong generalization ability
with fewer data resources, thereby reducing data annotation costs in domain
generalization tasks.",0
When Crypto Economics Meet Graph Analytics and Learning,2403.06454v1,http://arxiv.org/abs/2403.06454v1,2024-03-11 06:15:50+00:00,"Utilizing graph analytics and learning has proven to be an effective method
for exploring aspects of crypto economics such as network effects,
decentralization, tokenomics, and fraud detection. However, the majority of
existing research predominantly focuses on leading cryptocurrencies, namely
Bitcoin (BTC) and Ethereum (ETH), overlooking the vast diversity among the more
than 10,000 cryptocurrency projects. This oversight may result in skewed
insights. In our paper, we aim to broaden the scope of investigation to
encompass the entire spectrum of cryptocurrencies, examining various coins
across their entire life cycles. Furthermore, we intend to pioneer advanced
methodologies, including graph transfer learning and the innovative concept of
""graph of graphs"". By extending our research beyond the confines of BTC and
ETH, our goal is to enhance the depth of our understanding of crypto economics
and to advance the development of more intricate graph-based techniques.",0
Prediction of Wort Density with LSTM Network,2403.06458v1,http://arxiv.org/abs/2403.06458v1,2024-03-11 06:36:33+00:00,"Many physical target values in technical processes are error-prone,
cumbersome, or expensive to measure automatically. One example of a physical
target value is the wort density, which is an important value needed for beer
production. This article introduces a system that helps the brewer measure wort
density through sensors in order to reduce errors in manual data collection.
Instead of a direct measurement of wort density, a method is developed that
calculates the density from measured values acquired by inexpensive standard
sensors such as pressure or temperature. The model behind the calculation is a
neural network, known as LSTM.",0
Real-Time Simulated Avatar from Head-Mounted Sensors,2403.06862v1,http://arxiv.org/abs/2403.06862v1,2024-03-11 16:15:51+00:00,"We present SimXR, a method for controlling a simulated avatar from
information (headset pose and cameras) obtained from AR / VR headsets. Due to
the challenging viewpoint of head-mounted cameras, the human body is often
clipped out of view, making traditional image-based egocentric pose estimation
challenging. On the other hand, headset poses provide valuable information
about overall body motion, but lack fine-grained details about the hands and
feet. To synergize headset poses with cameras, we control a humanoid to track
headset movement while analyzing input images to decide body movement. When
body parts are seen, the movements of hands and feet will be guided by the
images; when unseen, the laws of physics guide the controller to generate
plausible motion. We design an end-to-end method that does not rely on any
intermediate representations and learns to directly map from images and headset
poses to humanoid control signals. To train our method, we also propose a
large-scale synthetic dataset created using camera configurations compatible
with a commercially available VR headset (Quest 2) and show promising results
on real-world captures. To demonstrate the applicability of our framework, we
also test it on an AR headset with a forward-facing camera.",0
Distribution-Aware Data Expansion with Diffusion Models,2403.06741v1,http://arxiv.org/abs/2403.06741v1,2024-03-11 14:07:53+00:00,"The scale and quality of a dataset significantly impact the performance of
deep models. However, acquiring large-scale annotated datasets is both a costly
and time-consuming endeavor. To address this challenge, dataset expansion
technologies aim to automatically augment datasets, unlocking the full
potential of deep models. Current data expansion methods encompass image
transformation-based and synthesis-based methods. The transformation-based
methods introduce only local variations, resulting in poor diversity. While
image synthesis-based methods can create entirely new content, significantly
enhancing informativeness. However, existing synthesis methods carry the risk
of distribution deviations, potentially degrading model performance with
out-of-distribution samples. In this paper, we propose DistDiff, an effective
data expansion framework based on the distribution-aware diffusion model.
DistDiff constructs hierarchical prototypes to approximate the real data
distribution, optimizing latent data points within diffusion models with
hierarchical energy guidance. We demonstrate its ability to generate
distribution-consistent samples, achieving substantial improvements in data
expansion tasks. Specifically, without additional training, DistDiff achieves a
30.7% improvement in accuracy across six image datasets compared to the model
trained on original datasets and a 9.8% improvement compared to the
state-of-the-art diffusion-based method. Our code is available at
https://github.com/haoweiz23/DistDiff",0
FaceChain-SuDe: Building Derived Class to Inherit Category Attributes for One-shot Subject-Driven Generation,2403.06775v1,http://arxiv.org/abs/2403.06775v1,2024-03-11 14:43:40+00:00,"Subject-driven generation has garnered significant interest recently due to
its ability to personalize text-to-image generation. Typical works focus on
learning the new subject's private attributes. However, an important fact has
not been taken seriously that a subject is not an isolated new concept but
should be a specialization of a certain category in the pre-trained model. This
results in the subject failing to comprehensively inherit the attributes in its
category, causing poor attribute-related generations. In this paper, motivated
by object-oriented programming, we model the subject as a derived class whose
base class is its semantic category. This modeling enables the subject to
inherit public attributes from its category while learning its private
attributes from the user-provided example. Specifically, we propose a
plug-and-play method, Subject-Derived regularization (SuDe). It constructs the
base-derived class modeling by constraining the subject-driven generated images
to semantically belong to the subject's category. Extensive experiments under
three baselines and two backbones on various subjects show that our SuDe
enables imaginative attribute-related generations while maintaining subject
fidelity. Codes will be open sourced soon at FaceChain
(https://github.com/modelscope/facechain).",0
Redefining Event Types and Group Evolution in Temporal Data,2403.06771v1,http://arxiv.org/abs/2403.06771v1,2024-03-11 14:39:24+00:00,"Groups -- such as clusters of points or communities of nodes -- are
fundamental when addressing various data mining tasks. In temporal data, the
predominant approach for characterizing group evolution has been through the
identification of ``events"". However, the events usually described in the
literature, e.g., shrinks/growths, splits/merges, are often arbitrarily
defined, creating a gap between such theoretical/predefined types and real-data
group observations. Moving beyond existing taxonomies, we think of events as
``archetypes"" characterized by a unique combination of quantitative dimensions
that we call ``facets"". Group dynamics are defined by their position within the
facet space, where archetypal events occupy extremities. Thus, rather than
enforcing strict event types, our approach can allow for hybrid descriptions of
dynamics involving group proximity to multiple archetypes. We apply our
framework to evolving groups from several face-to-face interaction datasets,
showing it enables richer, more reliable characterization of group dynamics
with respect to state-of-the-art methods, especially when the groups are
subject to complex relationships. Our approach also offers intuitive solutions
to common tasks related to dynamic group analysis, such as choosing an
appropriate aggregation scale, quantifying partition stability, and evaluating
event quality.",0
XB-MAML: Learning Expandable Basis Parameters for Effective Meta-Learning with Wide Task Coverage,2403.06768v1,http://arxiv.org/abs/2403.06768v1,2024-03-11 14:37:57+00:00,"Meta-learning, which pursues an effective initialization model, has emerged
as a promising approach to handling unseen tasks. However, a limitation remains
to be evident when a meta-learner tries to encompass a wide range of task
distribution, e.g., learning across distinctive datasets or domains. Recently,
a group of works has attempted to employ multiple model initializations to
cover widely-ranging tasks, but they are limited in adaptively expanding
initializations. We introduce XB-MAML, which learns expandable basis
parameters, where they are linearly combined to form an effective
initialization to a given task. XB-MAML observes the discrepancy between the
vector space spanned by the basis and fine-tuned parameters to decide whether
to expand the basis. Our method surpasses the existing works in the
multi-domain meta-learning benchmarks and opens up new chances of meta-learning
for obtaining the diverse inductive bias that can be combined to stretch toward
the effective initialization for diverse unseen tasks.",0
An Image is Worth 1/2 Tokens After Layer 2: Plug-and-Play Inference Acceleration for Large Vision-Language Models,2403.06764v1,http://arxiv.org/abs/2403.06764v1,2024-03-11 14:35:32+00:00,"In this study, we identify the inefficient attention phenomena in Large
Vision-Language Models (LVLMs), notably within prominent models like LLaVA-1.5,
QwenVL-Chat and Video-LLaVA. We find out that the attention computation over
visual tokens is of extreme inefficiency in the deep layers of popular LVLMs,
suggesting a need for a sparser approach compared to textual data handling. To
this end, we introduce FastV, a versatile plug-and-play method designed to
optimize computational efficiency by learning adaptive attention patterns in
early layers and pruning visual tokens in subsequent ones. Our evaluations
demonstrate FastV's ability to dramatically reduce computational costs (e.g., a
45 reduction in FLOPs for LLaVA-1.5-13B) without sacrificing performance in a
wide range of image and video understanding tasks. The computational efficiency
and performance trade-off of FastV are highly customizable and
pareto-efficient. It can compress the FLOPs of a 13B-parameter model to achieve
a lower budget than that of a 7B-parameter model, while still maintaining
superior performance. We believe FastV has practical values for deployment of
LVLMs in edge devices and commercial models. Code is released at
https://github.com/pkunlp-icler/FastV.",0
Coupled geophysical and thermal constraints link between Mars basal molten layer and the planet viscosity profile,2403.06763v1,http://arxiv.org/abs/2403.06763v1,2024-03-11 14:34:26+00:00,"Computing the tidal deformations of Mars, we explored various Mars internal
structures by examining profiles that include or exclude a basal molten layer
within the mantle and a solid inner core. By assessing their compatibility with
a diverse set of geophysical observations we show that despite the very short
periods of excitation, tidal deformation is very efficient to constraint the
Mars interior. We calculated densities and thicknesses for Martian lithosphere,
mantle, {core-mantle boundary} layer and core and {found them} coherent with
preexisting results from other methods. We also estimated new viscosities for
these layers. We demonstrated that the geodetic record associated with thermal
constraint is very sensitive to the {presence} of a basal molten layer in deep
Martian mantle, and less sensitive to the solid core. Our results also indicate
that the existence of the basal molten layer necessarily comes together with an
inversion of viscosity between the lithosphere and the mantle. In this case, we
could attribute this reverse viscosity contrast to the poor hydration state of
Martian mantle and we underlined that this result prevents a strict Earth-like
plate tectonics on Mars. The existence of the basal molten layer is also
associated with a non-inversion of viscosity between the core-mantle boundary
layer and the liquid core. Finally, in our results, a basal molten layer is
incompatible with the existence of a solid inner core. Efforts to detect basal
molten layer are then of prime importance to decipher the Martian interior.
{Inversely, viscosity profiles appear to be very good tools for probing the
existence of such molten layer at the base of the Mars mantle.",0
Performance of SK-Gd's Upgraded Real-time Supernova Monitoring System,2403.06760v2,http://arxiv.org/abs/2403.06760v2,2024-03-11 14:31:21+00:00,"Among multi-messenger observations of the next galactic core-collapse
supernova, Super-Kamiokande (SK) plays a critical role in detecting the emitted
supernova neutrinos, determining the direction to the supernova (SN), and
notifying the astronomical community of these observations in advance of the
optical signal. On 2022, SK has increased the gadolinium dissolved in its water
target (SK-Gd) and has achieved a Gd concentration of 0.033%, resulting in
enhanced neutron detection capability, which in turn enables more accurate
determination of the supernova direction. Accordingly, SK-Gd's real-time
supernova monitoring system (Abe te al. 2016b) has been upgraded. SK_SN Notice,
a warning system that works together with this monitoring system, was released
on December 13, 2021, and is available through GCN Notices (Barthelmy et al.
2000). When the monitoring system detects an SN-like burst of events, SK_SN
Notice will automatically distribute an alarm with the reconstructed direction
to the supernova candidate within a few minutes. In this paper, we present a
systematic study of SK-Gd's response to a simulated galactic SN. Assuming a
supernova situated at 10 kpc, neutrino fluxes from six supernova models are
used to characterize SK-Gd's pointing accuracy using the same tools as the
online monitoring system. The pointing accuracy is found to vary from
3-7$^\circ$ depending on the models. However, if the supernova is closer than
10 kpc, SK_SN Notice can issue an alarm with three-degree accuracy, which will
benefit follow-up observations by optical telescopes with large fields of view.",0
Average Calibration Error: A Differentiable Loss for Improved Reliability in Image Segmentation,2403.06759v1,http://arxiv.org/abs/2403.06759v1,2024-03-11 14:31:03+00:00,"Deep neural networks for medical image segmentation often produce
overconfident results misaligned with empirical observations. Such
miscalibration, challenges their clinical translation. We propose to use
marginal L1 average calibration error (mL1-ACE) as a novel auxiliary loss
function to improve pixel-wise calibration without compromising segmentation
quality. We show that this loss, despite using hard binning, is directly
differentiable, bypassing the need for approximate but differentiable surrogate
or soft binning approaches. Our work also introduces the concept of dataset
reliability histograms which generalises standard reliability diagrams for
refined visual assessment of calibration in semantic segmentation aggregated at
the dataset level. Using mL1-ACE, we reduce average and maximum calibration
error by 45% and 55% respectively, maintaining a Dice score of 87% on the BraTS
2021 dataset. We share our code here: https://github.com/cai4cai/ACE-DLIRIS",0
Koopman Ensembles for Probabilistic Time Series Forecasting,2403.06757v2,http://arxiv.org/abs/2403.06757v2,2024-03-11 14:29:56+00:00,"In the context of an increasing popularity of data-driven models to represent
dynamical systems, many machine learning-based implementations of the Koopman
operator have recently been proposed. However, the vast majority of those works
are limited to deterministic predictions, while the knowledge of uncertainty is
critical in fields like meteorology and climatology. In this work, we
investigate the training of ensembles of models to produce stochastic outputs.
We show through experiments on real remote sensing image time series that
ensembles of independently trained models are highly overconfident and that
using a training criterion that explicitly encourages the members to produce
predictions with high inter-model variances greatly improves the uncertainty
quantification of the ensembles.",0
ALaRM: Align Language Models via Hierarchical Rewards Modeling,2403.06754v1,http://arxiv.org/abs/2403.06754v1,2024-03-11 14:28:40+00:00,"We introduce ALaRM, the first framework modeling hierarchical rewards in
reinforcement learning from human feedback (RLHF), which is designed to enhance
the alignment of large language models (LLMs) with human preferences. The
framework addresses the limitations of current alignment approaches, which
often struggle with the inconsistency and sparsity of human supervision
signals, by integrating holistic rewards with aspect-specific rewards. This
integration enables more precise and consistent guidance of language models
towards desired outcomes, particularly in complex and open text generation
tasks. By employing a methodology that filters and combines multiple rewards
based on their consistency, the framework provides a reliable mechanism for
improving model alignment. We validate our approach through applications in
long-form question answering and machine translation tasks, employing
gpt-3.5-turbo for pairwise comparisons, and demonstrate improvements over
existing baselines. Our work underscores the effectiveness of hierarchical
rewards modeling in refining LLM training processes for better human preference
alignment. We release our code at https://ALaRM-fdu.github.io.",0
Generalising Multi-Agent Cooperation through Task-Agnostic Communication,2403.06750v1,http://arxiv.org/abs/2403.06750v1,2024-03-11 14:20:13+00:00,"Existing communication methods for multi-agent reinforcement learning (MARL)
in cooperative multi-robot problems are almost exclusively task-specific,
training new communication strategies for each unique task. We address this
inefficiency by introducing a communication strategy applicable to any task
within a given environment. We pre-train the communication strategy without
task-specific reward guidance in a self-supervised manner using a set
autoencoder. Our objective is to learn a fixed-size latent Markov state from a
variable number of agent observations. Under mild assumptions, we prove that
policies using our latent representations are guaranteed to converge, and upper
bound the value error introduced by our Markov state approximation. Our method
enables seamless adaptation to novel tasks without fine-tuning the
communication strategy, gracefully supports scaling to more agents than present
during training, and detects out-of-distribution events in an environment.
Empirical results on diverse MARL scenarios validate the effectiveness of our
approach, surpassing task-specific communication strategies in unseen tasks.
Our implementation of this work is available at
https://github.com/proroklab/task-agnostic-comms.",0
Shortcut Learning in Medical Image Segmentation,2403.06748v1,http://arxiv.org/abs/2403.06748v1,2024-03-11 14:14:52+00:00,"Shortcut learning is a phenomenon where machine learning models prioritize
learning simple, potentially misleading cues from data that do not generalize
well beyond the training set. While existing research primarily investigates
this in the realm of image classification, this study extends the exploration
of shortcut learning into medical image segmentation. We demonstrate that
clinical annotations such as calipers, and the combination of zero-padded
convolutions and center-cropped training sets in the dataset can inadvertently
serve as shortcuts, impacting segmentation accuracy. We identify and evaluate
the shortcut learning on two different but common medical image segmentation
tasks. In addition, we suggest strategies to mitigate the influence of shortcut
learning and improve the generalizability of the segmentation models. By
uncovering the presence and implications of shortcuts in medical image
segmentation, we provide insights and methodologies for evaluating and
overcoming this pervasive challenge and call for attention in the community for
shortcuts in segmentation.",0
MetaSplit: Meta-Split Network for Limited-Stock Product Recommendation,2403.06747v3,http://arxiv.org/abs/2403.06747v3,2024-03-11 14:13:41+00:00,"Compared to business-to-consumer (B2C) e-commerce systems,
consumer-to-consumer (C2C) e-commerce platforms usually encounter the
limited-stock problem, that is, a product can only be sold one time in a C2C
system. This poses several unique challenges for click-through rate (CTR)
prediction. Due to limited user interactions for each product (i.e. item), the
corresponding item embedding in the CTR model may not easily converge. This
makes the conventional sequence modeling based approaches cannot effectively
utilize user history information since historical user behaviors contain a
mixture of items with different volume of stocks. Particularly, the attention
mechanism in a sequence model tends to assign higher score to products with
more accumulated user interactions, making limited-stock products being ignored
and contribute less to the final output. To this end, we propose the Meta-Split
Network (MSNet) to split user history sequence regarding to the volume of stock
for each product, and adopt differentiated modeling approaches for different
sequences. As for the limited-stock products, a meta-learning approach is
applied to address the problem of inconvergence, which is achieved by designing
meta scaling and shifting networks with ID and side information. In addition,
traditional approach can hardly update item embedding once the product is
consumed. Thereby, we propose an auxiliary loss that makes the parameters
updatable even when the product is no longer in distribution. To the best of
our knowledge, this is the first solution addressing the recommendation of
limited-stock product. Experimental results on the production dataset and
online A/B testing demonstrate the effectiveness of our proposed method.",0
Integration of Physics-Derived Memristor Models with Machine Learning Frameworks,2403.06746v1,http://arxiv.org/abs/2403.06746v1,2024-03-11 14:12:21+00:00,"Simulation frameworks such MemTorch, DNN+NeuroSim, and aihwkit are commonly
used to facilitate the end-to-end co-design of memristive machine learning (ML)
accelerators. These simulators can take device nonidealities into account and
are integrated with modern ML frameworks. However, memristors in these
simulators are modeled with either lookup tables or simple analytic models with
basic nonlinearities. These simple models are unable to capture certain
performance-critical aspects of device nonidealities. For example, they ignore
the physical cause of switching, which induces errors in switching timings and
thus incorrect estimations of conductance states. This work aims at bringing
physical dynamics into consideration to model nonidealities while being
compatible with GPU accelerators. We focus on Valence Change Memory (VCM)
cells, where the switching nonlinearity and SET/RESET asymmetry relate tightly
with the thermal resistance, ion mobility, Schottky barrier height, parasitic
resistance, and other effects. The resulting dynamics require solving an ODE
that captures changes in oxygen vacancies. We modified a physics-derived
SPICE-level VCM model, integrated it with the aihwkit simulator and tested the
performance with the MNIST dataset. Results show that noise that disrupts the
SET/RESET matching affects network performance the most. This work serves as a
tool for evaluating how physical dynamics in memristive devices affect neural
network accuracy and can be used to guide the development of future integrated
devices.",0
Enhancing Image Caption Generation Using Reinforcement Learning with Human Feedback,2403.06735v1,http://arxiv.org/abs/2403.06735v1,2024-03-11 13:57:05+00:00,"Research on generative models to produce human-aligned / human-preferred
outputs has seen significant recent contributions. Between text and
image-generative models, we narrowed our focus to text-based generative models,
particularly to produce captions for images that align with human preferences.
In this research, we explored a potential method to amplify the performance of
the Deep Neural Network Model to generate captions that are preferred by
humans. This was achieved by integrating Supervised Learning and Reinforcement
Learning with Human Feedback (RLHF) using the Flickr8k dataset. Also, a novel
loss function that is capable of optimizing the model based on human feedback
is introduced. In this paper, we provide a concise sketch of our approach and
results, hoping to contribute to the ongoing advances in the field of
human-aligned generative AI models.",0
Reliable Spatial-Temporal Voxels For Multi-Modal Test-Time Adaptation,2403.06461v1,http://arxiv.org/abs/2403.06461v1,2024-03-11 06:56:08+00:00,"Multi-modal test-time adaptation (MM-TTA) is proposed to adapt models to an
unlabeled target domain by leveraging the complementary multi-modal inputs in
an online manner. Previous MM-TTA methods rely on predictions of cross-modal
information in each input frame, while they ignore the fact that predictions of
geometric neighborhoods within consecutive frames are highly correlated,
leading to unstable predictions across time. To fulfill this gap, we propose
ReLiable Spatial-temporal Voxels (Latte), an MM-TTA method that leverages
reliable cross-modal spatial-temporal correspondences for multi-modal 3D
segmentation. Motivated by the fact that reliable predictions should be
consistent with their spatial-temporal correspondences, Latte aggregates
consecutive frames in a slide window manner and constructs ST voxel to capture
temporally local prediction consistency for each modality. After filtering out
ST voxels with high ST entropy, Latte conducts cross-modal learning for each
point and pixel by attending to those with reliable and consistent predictions
among both spatial and temporal neighborhoods. Experimental results show that
Latte achieves state-of-the-art performance on three different MM-TTA
benchmarks compared to previous MM-TTA or TTA methods.",0
On the Approximation of Kernel functions,2403.06731v1,http://arxiv.org/abs/2403.06731v1,2024-03-11 13:50:07+00:00,"Various methods in statistical learning build on kernels considered in
reproducing kernel Hilbert spaces. In applications, the kernel is often
selected based on characteristics of the problem and the data. This kernel is
then employed to infer response variables at points, where no explanatory data
were observed. The data considered here are located in compact sets in higher
dimensions and the paper addresses approximations of the kernel itself. The new
approach considers Taylor series approximations of radial kernel functions. For
the Gauss kernel on the unit cube, the paper establishes an upper bound of the
associated eigenfunctions, which grows only polynomially with respect to the
index. The novel approach substantiates smaller regularization parameters than
considered in the literature, overall leading to better approximations. This
improvement confirms low rank approximation methods such as the Nystr\""om
method.",0
Large Model driven Radiology Report Generation with Clinical Quality Reinforcement Learning,2403.06728v1,http://arxiv.org/abs/2403.06728v1,2024-03-11 13:47:11+00:00,"Radiology report generation (RRG) has attracted significant attention due to
its potential to reduce the workload of radiologists. Current RRG approaches
are still unsatisfactory against clinical standards. This paper introduces a
novel RRG method, \textbf{LM-RRG}, that integrates large models (LMs) with
clinical quality reinforcement learning to generate accurate and comprehensive
chest X-ray radiology reports. Our method first designs a large language model
driven feature extractor to analyze and interpret different regions of the
chest X-ray image, emphasizing specific regions with medical significance.
Next, based on the large model's decoder, we develop a multimodal report
generator that leverages multimodal prompts from visual features and textual
instruction to produce the radiology report in an auto-regressive way. Finally,
to better reflect the clinical significant and insignificant errors that
radiologists would normally assign in the report, we introduce a novel clinical
quality reinforcement learning strategy. It utilizes the radiology report
clinical quality (RadCliQ) metric as a reward function in the learning process.
Extensive experiments on the MIMIC-CXR and IU-Xray datasets demonstrate the
superiority of our method over the state of the art.",0
Probabilistic Contrastive Learning for Long-Tailed Visual Recognition,2403.06726v2,http://arxiv.org/abs/2403.06726v2,2024-03-11 13:44:49+00:00,"Long-tailed distributions frequently emerge in real-world data, where a large
number of minority categories contain a limited number of samples. Such
imbalance issue considerably impairs the performance of standard supervised
learning algorithms, which are mainly designed for balanced training sets.
Recent investigations have revealed that supervised contrastive learning
exhibits promising potential in alleviating the data imbalance. However, the
performance of supervised contrastive learning is plagued by an inherent
challenge: it necessitates sufficiently large batches of training data to
construct contrastive pairs that cover all categories, yet this requirement is
difficult to meet in the context of class-imbalanced data. To overcome this
obstacle, we propose a novel probabilistic contrastive (ProCo) learning
algorithm that estimates the data distribution of the samples from each class
in the feature space, and samples contrastive pairs accordingly. In fact,
estimating the distributions of all classes using features in a small batch,
particularly for imbalanced data, is not feasible. Our key idea is to introduce
a reasonable and simple assumption that the normalized features in contrastive
learning follow a mixture of von Mises-Fisher (vMF) distributions on unit
space, which brings two-fold benefits. First, the distribution parameters can
be estimated using only the first sample moment, which can be efficiently
computed in an online manner across different batches. Second, based on the
estimated distribution, the vMF distribution allows us to sample an infinite
number of contrastive pairs and derive a closed form of the expected
contrastive loss for efficient optimization. Our code is available at
https://github.com/LeapLabTHU/ProCo.",0
Improving Low-Resource Knowledge Tracing Tasks by Supervised Pre-training and Importance Mechanism Fine-tuning,2403.06725v1,http://arxiv.org/abs/2403.06725v1,2024-03-11 13:44:43+00:00,"Knowledge tracing (KT) aims to estimate student's knowledge mastery based on
their historical interactions. Recently, the deep learning based KT (DLKT)
approaches have achieved impressive performance in the KT task. These DLKT
models heavily rely on the large number of available student interactions.
However, due to various reasons such as budget constraints and privacy
concerns, observed interactions are very limited in many real-world scenarios,
a.k.a, low-resource KT datasets. Directly training a DLKT model on a
low-resource KT dataset may lead to overfitting and it is difficult to choose
the appropriate deep neural architecture. Therefore, in this paper, we propose
a low-resource KT framework called LoReKT to address above challenges. Inspired
by the prevalent ""pre-training and fine-tuning"" paradigm, we aim to learn
transferable parameters and representations from rich-resource KT datasets
during the pre-training stage and subsequently facilitate effective adaptation
to low-resource KT datasets. Specifically, we simplify existing sophisticated
DLKT model architectures with purely a stack of transformer decoders. We design
an encoding mechanism to incorporate student interactions from multiple KT data
sources and develop an importance mechanism to prioritize updating parameters
with high importance while constraining less important ones during the
fine-tuning stage. We evaluate LoReKT on six public KT datasets and
experimental results demonstrate the superiority of our approach in terms of
AUC and Accuracy. To encourage reproducible research, we make our data and code
publicly available at https://anonymous.4open.science/r/LoReKT-C619.",0
The Ouroboros of Memristors: Neural Networks Facilitating Memristor Programming,2403.06712v1,http://arxiv.org/abs/2403.06712v1,2024-03-11 13:37:18+00:00,"Memristive devices hold promise to improve the scale and efficiency of
machine learning and neuromorphic hardware, thanks to their compact size, low
power consumption, and the ability to perform matrix multiplications in
constant time. However, on-chip training with memristor arrays still faces
challenges, including device-to-device and cycle-to-cycle variations, switching
non-linearity, and especially SET and RESET asymmetry. To combat device
non-linearity and asymmetry, we propose to program memristors by harnessing
neural networks that map desired conductance updates to the required pulse
times. With our method, approximately 95% of devices can be programmed within a
relative percentage difference of +-50% from the target conductance after just
one attempt. Our approach substantially reduces memristor programming delays
compared to traditional write-and-verify methods, presenting an advantageous
solution for on-chip training scenarios. Furthermore, our proposed neural
network can be accelerated by memristor arrays upon deployment, providing
assistance while reducing hardware overhead compared with previous works.
  This work contributes significantly to the practical application of
memristors, particularly in reducing delays in memristor programming. It also
envisions the future development of memristor-based machine learning
accelerators.",0
Fast Text-to-3D-Aware Face Generation and Manipulation via Direct Cross-modal Mapping and Geometric Regularization,2403.06702v1,http://arxiv.org/abs/2403.06702v1,2024-03-11 13:17:55+00:00,"Text-to-3D-aware face (T3D Face) generation and manipulation is an emerging
research hot spot in machine learning, which still suffers from low efficiency
and poor quality. In this paper, we propose an End-to-End Efficient and
Effective network for fast and accurate T3D face generation and manipulation,
termed $E^3$-FaceNet. Different from existing complex generation paradigms,
$E^3$-FaceNet resorts to a direct mapping from text instructions to 3D-aware
visual space. We introduce a novel Style Code Enhancer to enhance cross-modal
semantic alignment, alongside an innovative Geometric Regularization objective
to maintain consistency across multi-view generations. Extensive experiments on
three benchmark datasets demonstrate that $E^3$-FaceNet can not only achieve
picture-like 3D face generation and manipulation, but also improve inference
speed by orders of magnitudes. For instance, compared with Latent3D,
$E^3$-FaceNet speeds up the five-view generations by almost 470 times, while
still exceeding in generation quality. Our code are released at
https://github.com/Aria-Zhangjl/E3-FaceNet.",0
Enhancing Adversarial Training with Prior Knowledge Distillation for Robust Image Compression,2403.06700v1,http://arxiv.org/abs/2403.06700v1,2024-03-11 13:16:44+00:00,"Deep neural network-based image compression (NIC) has achieved excellent
performance, but NIC method models have been shown to be susceptible to
backdoor attacks. Adversarial training has been validated in image compression
models as a common method to enhance model robustness. However, the improvement
effect of adversarial training on model robustness is limited. In this paper,
we propose a prior knowledge-guided adversarial training framework for image
compression models. Specifically, first, we propose a gradient regularization
constraint for training robust teacher models. Subsequently, we design a
knowledge distillation based strategy to generate a priori knowledge from the
teacher model to the student model for guiding adversarial training.
Experimental results show that our method improves the reconstruction quality
by about 9dB when the Kodak dataset is elected as the backdoor attack object
for psnr attack. Compared with Ma2023, our method has a 5dB higher PSNR output
at high bitrate points.",0
Advancing Graph Neural Networks with HL-HGAT: A Hodge-Laplacian and Attention Mechanism Approach for Heterogeneous Graph-Structured Data,2403.06687v1,http://arxiv.org/abs/2403.06687v1,2024-03-11 13:04:21+00:00,"Graph neural networks (GNNs) have proven effective in capturing relationships
among nodes in a graph. This study introduces a novel perspective by
considering a graph as a simplicial complex, encompassing nodes, edges,
triangles, and $k$-simplices, enabling the definition of graph-structured data
on any $k$-simplices. Our contribution is the Hodge-Laplacian heterogeneous
graph attention network (HL-HGAT), designed to learn heterogeneous signal
representations across $k$-simplices. The HL-HGAT incorporates three key
components: HL convolutional filters (HL-filters), simplicial projection (SP),
and simplicial attention pooling (SAP) operators, applied to $k$-simplices.
HL-filters leverage the unique topology of $k$-simplices encoded by the
Hodge-Laplacian (HL) operator, operating within the spectral domain of the
$k$-th HL operator. To address computation challenges, we introduce a
polynomial approximation for HL-filters, exhibiting spatial localization
properties. Additionally, we propose a pooling operator to coarsen
$k$-simplices, combining features through simplicial attention mechanisms of
self-attention and cross-attention via transformers and SP operators, capturing
topological interconnections across multiple dimensions of simplices. The
HL-HGAT is comprehensively evaluated across diverse graph applications,
including NP-hard problems, graph multi-label and classification challenges,
and graph regression tasks in logistics, computer vision, biology, chemistry,
and neuroscience. The results demonstrate the model's efficacy and versatility
in handling a wide range of graph-based scenarios.",0
Transferring Relative Monocular Depth to Surgical Vision with Temporal Consistency,2403.06683v1,http://arxiv.org/abs/2403.06683v1,2024-03-11 12:57:51+00:00,"Relative monocular depth, inferring depth up to shift and scale from a single
image, is an active research topic. Recent deep learning models, trained on
large and varied meta-datasets, now provide excellent performance in the domain
of natural images. However, few datasets exist which provide ground truth depth
for endoscopic images, making training such models from scratch unfeasible.
This work investigates the transfer of these models into the surgical domain,
and presents an effective and simple way to improve on standard supervision
through the use of temporal consistency self-supervision. We show temporal
consistency significantly improves supervised training alone when transferring
to the low-data regime of endoscopy, and outperforms the prevalent
self-supervision technique for this task. In addition we show our method
drastically outperforms the state-of-the-art method from within the domain of
endoscopy. We also release our code, model and ensembled meta-dataset,
Meta-MED, establishing a strong benchmark for future work.",0
Restoring Ancient Ideograph: A Multimodal Multitask Neural Network Approach,2403.06682v1,http://arxiv.org/abs/2403.06682v1,2024-03-11 12:57:28+00:00,"Cultural heritage serves as the enduring record of human thought and history.
Despite significant efforts dedicated to the preservation of cultural relics,
many ancient artefacts have been ravaged irreversibly by natural deterioration
and human actions. Deep learning technology has emerged as a valuable tool for
restoring various kinds of cultural heritages, including ancient text
restoration. Previous research has approached ancient text restoration from
either visual or textual perspectives, often overlooking the potential of
synergizing multimodal information. This paper proposes a novel Multimodal
Multitask Restoring Model (MMRM) to restore ancient texts, particularly
emphasising the ideograph. This model combines context understanding with
residual visual information from damaged ancient artefacts, enabling it to
predict damaged characters and generate restored images simultaneously. We
tested the MMRM model through experiments conducted on both simulated datasets
and authentic ancient inscriptions. The results show that the proposed method
gives insightful restoration suggestions in both simulation experiments and
real-world scenarios. To the best of our knowledge, this work represents the
pioneering application of multimodal deep learning in ancient text restoration,
which will contribute to the understanding of ancient society and culture in
digital humanities fields.",0
Trustworthy Partial Label Learning with Out-of-distribution Detection,2403.06681v1,http://arxiv.org/abs/2403.06681v1,2024-03-11 12:56:36+00:00,"Partial Label Learning (PLL) grapples with learning from ambiguously labelled
data, and it has been successfully applied in fields such as image recognition.
Nevertheless, traditional PLL methods rely on the closed-world assumption,
which can be limiting in open-world scenarios and negatively impact model
performance and generalization. To tackle these challenges, our study
introduces a novel method called PLL-OOD, which is the first to incorporate
Out-of-Distribution (OOD) detection into the PLL framework. PLL-OOD
significantly enhances model adaptability and accuracy by merging
self-supervised learning with partial label loss and pioneering the
Partial-Energy (PE) score for OOD detection. This approach improves data
feature representation and effectively disambiguates candidate labels, using a
dynamic label confidence matrix to refine predictions. The PE score, adjusted
by label confidence, precisely identifies OOD instances, optimizing model
training towards in-distribution data. This innovative method markedly boosts
PLL model robustness and performance in open-world settings. To validate our
approach, we conducted a comprehensive comparative experiment combining the
existing state-of-the-art PLL model with multiple OOD scores on the CIFAR-10
and CIFAR-100 datasets with various OOD datasets. The results demonstrate that
the proposed PLL-OOD framework is highly effective and effectiveness
outperforms existing models, showcasing its superiority and effectiveness.",0
Answering Diverse Questions via Text Attached with Key Audio-Visual Clues,2403.06679v1,http://arxiv.org/abs/2403.06679v1,2024-03-11 12:51:37+00:00,"Audio-visual question answering (AVQA) requires reference to video content
and auditory information, followed by correlating the question to predict the
most precise answer. Although mining deeper layers of audio-visual information
to interact with questions facilitates the multimodal fusion process, the
redundancy of audio-visual parameters tends to reduce the generalization of the
inference engine to multiple question-answer pairs in a single video. Indeed,
the natural heterogeneous relationship between audiovisuals and text makes the
perfect fusion challenging, to prevent high-level audio-visual semantics from
weakening the network's adaptability to diverse question types, we propose a
framework for performing mutual correlation distillation (MCD) to aid question
inference. MCD is divided into three main steps: 1) firstly, the residual
structure is utilized to enhance the audio-visual soft associations based on
self-attention, then key local audio-visual features relevant to the question
context are captured hierarchically by shared aggregators and coupled in the
form of clues with specific question vectors. 2) Secondly, knowledge
distillation is enforced to align audio-visual-text pairs in a shared latent
space to narrow the cross-modal semantic gap. 3) And finally, the audio-visual
dependencies are decoupled by discarding the decision-level integrations. We
evaluate the proposed method on two publicly available datasets containing
multiple question-and-answer pairs, i.e., Music-AVQA and AVQA. Experiments show
that our method outperforms other state-of-the-art methods, and one interesting
finding behind is that removing deep audio-visual features during inference can
effectively mitigate overfitting. The source code is released at
http://github.com/rikeilong/MCD-forAVQA.",0
Streamlining in the Riemannian Realm: Efficient Riemannian Optimization with Loopless Variance Reduction,2403.06677v1,http://arxiv.org/abs/2403.06677v1,2024-03-11 12:49:37+00:00,"In this study, we investigate stochastic optimization on Riemannian
manifolds, focusing on the crucial variance reduction mechanism used in both
Euclidean and Riemannian settings. Riemannian variance-reduced methods usually
involve a double-loop structure, computing a full gradient at the start of each
loop. Determining the optimal inner loop length is challenging in practice, as
it depends on strong convexity or smoothness constants, which are often unknown
or hard to estimate. Motivated by Euclidean methods, we introduce the
Riemannian Loopless SVRG (R-LSVRG) and PAGE (R-PAGE) methods. These methods
replace the outer loop with probabilistic gradient computation triggered by a
coin flip in each iteration, ensuring simpler proofs, efficient hyperparameter
selection, and sharp convergence guarantees. Using R-PAGE as a framework for
non-convex Riemannian optimization, we demonstrate its applicability to various
important settings. For example, we derive Riemannian MARINA (R-MARINA) for
distributed settings with communication compression, providing the best
theoretical communication complexity guarantees for non-convex distributed
optimization over Riemannian manifolds. Experimental results support our
theoretical findings.",0
From Factor Models to Deep Learning: Machine Learning in Reshaping Empirical Asset Pricing,2403.06779v1,http://arxiv.org/abs/2403.06779v1,2024-03-11 14:48:57+00:00,"This paper comprehensively reviews the application of machine learning (ML)
and AI in finance, specifically in the context of asset pricing. It starts by
summarizing the traditional asset pricing models and examining their
limitations in capturing the complexities of financial markets. It explores how
1) ML models, including supervised, unsupervised, semi-supervised, and
reinforcement learning, provide versatile frameworks to address these
complexities, and 2) the incorporation of advanced ML algorithms into
traditional financial models enhances return prediction and portfolio
optimization. These methods can adapt to changing market dynamics by modeling
structural changes and incorporating heterogeneous data sources, such as text
and images. In addition, this paper explores challenges in applying ML in asset
pricing, addressing the growing demand for explainability in decision-making
and mitigating overfitting in complex models. This paper aims to provide
insights into novel methodologies showcasing the potential of ML to reshape the
future of quantitative finance.",0
Genetic Learning for Designing Sim-to-Real Data Augmentations,2403.06786v1,http://arxiv.org/abs/2403.06786v1,2024-03-11 15:00:56+00:00,"Data augmentations are useful in closing the sim-to-real domain gap when
training on synthetic data. This is because they widen the training data
distribution, thus encouraging the model to generalize better to other domains.
Many image augmentation techniques exist, parametrized by different settings,
such as strength and probability. This leads to a large space of different
possible augmentation policies. Some policies work better than others for
overcoming the sim-to-real gap for specific datasets, and it is unclear why.
This paper presents two different interpretable metrics that can be combined to
predict how well a certain augmentation policy will work for a specific
sim-to-real setting, focusing on object detection. We validate our metrics by
training many models with different augmentation policies and showing a strong
correlation with performance on real data. Additionally, we introduce
GeneticAugment, a genetic programming method that can leverage these metrics to
automatically design an augmentation policy for a specific dataset without
needing to train a model.",0
The evaluation of a code-switched Sepedi-English automatic speech recognition system,2403.07947v1,http://arxiv.org/abs/2403.07947v1,2024-03-11 15:11:28+00:00,"Speech technology is a field that encompasses various techniques and tools
used to enable machines to interact with speech, such as automatic speech
recognition (ASR), spoken dialog systems, and others, allowing a device to
capture spoken words through a microphone from a human speaker. End-to-end
approaches such as Connectionist Temporal Classification (CTC) and
attention-based methods are the most used for the development of ASR systems.
However, these techniques were commonly used for research and development for
many high-resourced languages with large amounts of speech data for training
and evaluation, leaving low-resource languages relatively underdeveloped. While
the CTC method has been successfully used for other languages, its
effectiveness for the Sepedi language remains uncertain. In this study, we
present the evaluation of the Sepedi-English code-switched automatic speech
recognition system. This end-to-end system was developed using the Sepedi
Prompted Code Switching corpus and the CTC approach. The performance of the
system was evaluated using both the NCHLT Sepedi test corpus and the Sepedi
Prompted Code Switching corpus. The model produced the lowest WER of 41.9%,
however, the model faced challenges in recognizing the Sepedi only text.",0
Boosting Image Restoration via Priors from Pre-trained Models,2403.06793v1,http://arxiv.org/abs/2403.06793v1,2024-03-11 15:11:57+00:00,"Pre-trained models with large-scale training data, such as CLIP and Stable
Diffusion, have demonstrated remarkable performance in various high-level
computer vision tasks such as image understanding and generation from language
descriptions. Yet, their potential for low-level tasks such as image
restoration remains relatively unexplored. In this paper, we explore such
models to enhance image restoration. As off-the-shelf features (OSF) from
pre-trained models do not directly serve image restoration, we propose to learn
an additional lightweight module called Pre-Train-Guided Refinement Module
(PTG-RM) to refine restoration results of a target restoration network with
OSF. PTG-RM consists of two components, Pre-Train-Guided Spatial-Varying
Enhancement (PTG-SVE), and Pre-Train-Guided Channel-Spatial Attention
(PTG-CSA). PTG-SVE enables optimal short- and long-range neural operations,
while PTG-CSA enhances spatial-channel attention for restoration-related
learning. Extensive experiments demonstrate that PTG-RM, with its compact size
($<$1M parameters), effectively enhances restoration performance of various
models across different tasks, including low-light enhancement, deraining,
deblurring, and denoising.",0
A Geospatial Approach to Predicting Desert Locust Breeding Grounds in Africa,2403.06860v1,http://arxiv.org/abs/2403.06860v1,2024-03-11 16:13:58+00:00,"Desert locust swarms present a major threat to agriculture and food security.
Addressing this challenge, our study develops an operationally-ready model for
predicting locust breeding grounds, which has the potential to enhance early
warning systems and targeted control measures. We curated a dataset from the
United Nations Food and Agriculture Organization's (UN-FAO) locust observation
records and analyzed it using two types of spatio-temporal input features:
remotely-sensed environmental and climate data as well as multi-spectral earth
observation images. Our approach employed custom deep learning models
(three-dimensional and LSTM-based recurrent convolutional networks), along with
the geospatial foundational model Prithvi recently released by Jakubik et al.,
2023. These models notably outperformed existing baselines, with the
Prithvi-based model, fine-tuned on multi-spectral images from NASA's Harmonized
Landsat and Sentinel-2 (HLS) dataset, achieving the highest accuracy, F1 and
ROC-AUC scores (83.03%, 81.53% and 87.69%, respectively). A significant finding
from our research is that multi-spectral earth observation images alone are
sufficient for effective locust breeding ground prediction without the need to
explicitly incorporate climatic or environmental features.",0
Concurrent Speaker Detection: A multi-microphone Transformer-Based Approach,2403.06856v1,http://arxiv.org/abs/2403.06856v1,2024-03-11 16:12:08+00:00,"We present a deep-learning approach for the task of Concurrent Speaker
Detection (CSD) using a modified transformer model. Our model is designed to
handle multi-microphone data but can also work in the single-microphone case.
The method can classify audio segments into one of three classes: 1) no speech
activity (noise only), 2) only a single speaker is active, and 3) more than one
speaker is active. We incorporate a Cost-Sensitive (CS) loss and a confidence
calibration to the training procedure. The approach is evaluated using three
real-world databases: AMI, AliMeeting, and CHiME 5, demonstrating an
improvement over existing approaches.",0
Surface-aware Mesh Texture Synthesis with Pre-trained 2D CNNs,2403.06855v1,http://arxiv.org/abs/2403.06855v1,2024-03-11 16:11:57+00:00,"Mesh texture synthesis is a key component in the automatic generation of 3D
content. Existing learning-based methods have drawbacks -- either by
disregarding the shape manifold during texture generation or by requiring a
large number of different views to mitigate occlusion-related inconsistencies.
In this paper, we present a novel surface-aware approach for mesh texture
synthesis that overcomes these drawbacks by leveraging the pre-trained weights
of 2D Convolutional Neural Networks (CNNs) with the same architecture, but with
convolutions designed for 3D meshes. Our proposed network keeps track of the
oriented patches surrounding each texel, enabling seamless texture synthesis
and retaining local similarity to classical 2D convolutions with square
kernels. Our approach allows us to synthesize textures that account for the
geometric content of mesh surfaces, eliminating discontinuities and achieving
comparable quality to 2D image synthesis algorithms. We compare our approach
with state-of-the-art methods where, through qualitative and quantitative
evaluations, we demonstrate that our approach is more effective for a variety
of meshes and styles, while also producing visually appealing and consistent
textures on meshes.",0
Quantifying the Sensitivity of Inverse Reinforcement Learning to Misspecification,2403.06854v1,http://arxiv.org/abs/2403.06854v1,2024-03-11 16:09:39+00:00,"Inverse reinforcement learning (IRL) aims to infer an agent's preferences
(represented as a reward function $R$) from their behaviour (represented as a
policy $\pi$). To do this, we need a behavioural model of how $\pi$ relates to
$R$. In the current literature, the most common behavioural models are
optimality, Boltzmann-rationality, and causal entropy maximisation. However,
the true relationship between a human's preferences and their behaviour is much
more complex than any of these behavioural models. This means that the
behavioural models are misspecified, which raises the concern that they may
lead to systematic errors if applied to real data. In this paper, we analyse
how sensitive the IRL problem is to misspecification of the behavioural model.
Specifically, we provide necessary and sufficient conditions that completely
characterise how the observed data may differ from the assumed behavioural
model without incurring an error above a given threshold. In addition to this,
we also characterise the conditions under which a behavioural model is robust
to small perturbations of the observed policy, and we analyse how robust many
behavioural models are to misspecification of their parameter values (such as
e.g.\ the discount rate). Our analysis suggests that the IRL problem is highly
sensitive to misspecification, in the sense that very mild misspecification can
lead to very large errors in the inferred reward function.",0
Human-Exoskeleton Interaction Portrait,2403.06851v1,http://arxiv.org/abs/2403.06851v1,2024-03-11 16:08:22+00:00,"Human-robot physical interaction contains crucial information for optimizing
user experience, enhancing robot performance, and objectively assessing user
adaptation. This study introduces a new method to evaluate human-robot
co-adaptation in lower limb exoskeletons by analyzing muscle activity and
interaction torque as a two-dimensional random variable. We introduce the
Interaction Portrait (IP), which visualizes this variable's distribution in
polar coordinates. We applied this metric to compare a recent torque controller
(HTC) based on kinematic state feedback and a novel feedforward controller
(AMTC) with online learning, proposed herein, against a time-based controller
(TBC) during treadmill walking at varying speeds. Compared to TBC, both HTC and
AMTC significantly lower users' normalized oxygen uptake, suggesting enhanced
user-exoskeleton coordination. IP analysis reveals this improvement stems from
two distinct co-adaptation strategies, unidentifiable by traditional muscle
activity or interaction torque analyses alone. HTC encourages users to yield
control to the exoskeleton, decreasing muscular effort but increasing
interaction torque, as the exoskeleton compensates for user dynamics.
Conversely, AMTC promotes user engagement through increased muscular effort and
reduced interaction torques, aligning it more closely with rehabilitation and
gait training applications. IP phase evolution provides insight into each
user's interaction strategy development, showcasing IP analysis's potential in
comparing and designing novel controllers to optimize human-robot interaction
in wearable robots.",0
All in One: Multi-Task Prompting for Graph Neural Networks (Extended Abstract),2403.07040v1,http://arxiv.org/abs/2403.07040v1,2024-03-11 16:04:58+00:00,"This paper is an extended abstract of our original work published in KDD23,
where we won the best research paper award (Xiangguo Sun, Hong Cheng, Jia Li,
Bo Liu, and Jihong Guan. All in one: Multi-task prompting for graph neural
networks. KDD 23) The paper introduces a novel approach to bridging the gap
between pre-trained graph models and the diverse tasks they're applied to,
inspired by the success of prompt learning in NLP. Recognizing the challenge of
aligning pre-trained models with varied graph tasks (node level, edge level,
and graph level), which can lead to negative transfer and poor performance, we
propose a multi-task prompting method for graphs. This method involves unifying
graph and language prompt formats, enabling NLP's prompting strategies to be
adapted for graph tasks. By analyzing the task space of graph applications, we
reformulate problems to fit graph-level tasks and apply meta-learning to
improve prompt initialization for multiple tasks. Experiments show our method's
effectiveness in enhancing model performance across different graph tasks.
  Beyond the original work, in this extended abstract, we further discuss the
graph prompt from a bigger picture and provide some of the latest work toward
this area.",0
DiaLoc: An Iterative Approach to Embodied Dialog Localization,2403.06846v1,http://arxiv.org/abs/2403.06846v1,2024-03-11 16:03:43+00:00,"Multimodal learning has advanced the performance for many vision-language
tasks. However, most existing works in embodied dialog research focus on
navigation and leave the localization task understudied. The few existing
dialog-based localization approaches assume the availability of entire dialog
prior to localizaiton, which is impractical for deployed dialog-based
localization. In this paper, we propose DiaLoc, a new dialog-based localization
framework which aligns with a real human operator behavior. Specifically, we
produce an iterative refinement of location predictions which can visualize
current pose believes after each dialog turn. DiaLoc effectively utilizes the
multimodal data for multi-shot localization, where a fusion encoder fuses
vision and dialog information iteratively. We achieve state-of-the-art results
on embodied dialog-based localization task, in single-shot (+7.08% in
Acc5@valUnseen) and multi-shot settings (+10.85% in Acc5@valUnseen). DiaLoc
narrows the gap between simulation and real-world applications, opening doors
for future research on collaborative localization and navigation.",0
Towards an educational tool for supporting neonatologists in the delivery room,2403.06843v1,http://arxiv.org/abs/2403.06843v1,2024-03-11 16:03:21+00:00,"Nowadays, there is evidence that several factors may increase the risk, for
an infant, to require stabilisation or resuscitation manoeuvres at birth.
However, this risk factors are not completely known, and a universally
applicable model for predicting high-risk situations is not available yet.
Considering both these limitations and the fact that the need for resuscitation
at birth is a rare event, periodic training of the healthcare personnel
responsible for newborn caring in the delivery room is mandatory.
  In this paper, we propose a machine learning approach for identifying risk
factors and their impact on the birth event from real data, which can be used
by personnel to progressively increase and update their knowledge. Our final
goal will be the one of designing a user-friendly mobile application, able to
improve the recognition rate and the planning of the appropriate interventions
on high-risk patients.",0
RA-ISF: Learning to Answer and Understand from Retrieval Augmentation via Iterative Self-Feedback,2403.06840v1,http://arxiv.org/abs/2403.06840v1,2024-03-11 16:01:05+00:00,"Large language models (LLMs) demonstrate exceptional performance in numerous
tasks but still heavily rely on knowledge stored in their parameters. Moreover,
updating this knowledge incurs high training costs. Retrieval-augmented
generation (RAG) methods address this issue by integrating external knowledge.
The model can answer questions it couldn't previously by retrieving knowledge
relevant to the query. This approach improves performance in certain scenarios
for specific tasks. However, if irrelevant texts are retrieved, it may impair
model performance. In this paper, we propose Retrieval Augmented Iterative
Self-Feedback (RA-ISF), a framework that iteratively decomposes tasks and
processes them in three submodules to enhance the model's problem-solving
capabilities. Experiments show that our method outperforms existing benchmarks,
performing well on models like GPT3.5, Llama2, significantly enhancing factual
reasoning capabilities and reducing hallucinations.",0
DiPrompT: Disentangled Prompt Tuning for Multiple Latent Domain Generalization in Federated Learning,2403.08506v1,http://arxiv.org/abs/2403.08506v1,2024-03-11 15:58:15+00:00,"Federated learning (FL) has emerged as a powerful paradigm for learning from
decentralized data, and federated domain generalization further considers the
test dataset (target domain) is absent from the decentralized training data
(source domains). However, most existing FL methods assume that domain labels
are provided during training, and their evaluation imposes explicit constraints
on the number of domains, which must strictly match the number of clients.
Because of the underutilization of numerous edge devices and additional
cross-client domain annotations in the real world, such restrictions may be
impractical and involve potential privacy leaks. In this paper, we propose an
efficient and novel approach, called Disentangled Prompt Tuning (DiPrompT), a
method that tackles the above restrictions by learning adaptive prompts for
domain generalization in a distributed manner. Specifically, we first design
two types of prompts, i.e., global prompt to capture general knowledge across
all clients and domain prompts to capture domain-specific knowledge. They
eliminate the restriction on the one-to-one mapping between source domains and
local clients. Furthermore, a dynamic query metric is introduced to
automatically search the suitable domain label for each sample, which includes
two-substep text-image alignments based on prompt tuning without
labor-intensive annotation. Extensive experiments on multiple datasets
demonstrate that our DiPrompT achieves superior domain generalization
performance over state-of-the-art FL methods when domain labels are not
provided, and even outperforms many centralized learning methods using domain
labels.",0
Can LLMs Separate Instructions From Data? And What Do We Even Mean By That?,2403.06833v1,http://arxiv.org/abs/2403.06833v1,2024-03-11 15:48:56+00:00,"Instruction-tuned Large Language Models (LLMs) have achieved breakthrough
results, opening countless new possibilities for many practical applications.
However, LLMs lack elementary safety features that are established norms in
other areas of computer science, such as the separation between instructions
and data, causing them to malfunction or rendering them vulnerable to
manipulation and interference by third parties e.g., via indirect
prompt/command injection. Even worse, so far, there is not even an established
definition of what precisely such a separation would mean and how its violation
could be tested. In this work, we aim to close this gap. We introduce a formal
measure to quantify the phenomenon of instruction-data separation as well as an
empirical variant of the measure that can be computed from a model`s black-box
outputs. We also introduce a new dataset, SEP (Should it be Executed or
Processed?), which allows estimating the measure, and we report results on
several state-of-the-art open-source and closed LLMs. Finally, we
quantitatively demonstrate that all evaluated LLMs fail to achieve a high
amount of separation, according to our measure. The source code and SEP dataset
are openly accessible at
https://github.com/egozverev/Shold-It-Be-Executed-Or-Processed.",0
The Power of Noise: Toward a Unified Multi-modal Knowledge Graph Representation Framework,2403.06832v1,http://arxiv.org/abs/2403.06832v1,2024-03-11 15:48:43+00:00,"The advancement of Multi-modal Pre-training highlights the necessity for a
robust Multi-Modal Knowledge Graph (MMKG) representation learning framework.
This framework is crucial for integrating structured knowledge into multi-modal
Large Language Models (LLMs) at scale, aiming to alleviate issues like
knowledge misconceptions and multi-modal hallucinations. In this work, to
evaluate models' ability to accurately embed entities within MMKGs, we focus on
two widely researched tasks: Multi-modal Knowledge Graph Completion (MKGC) and
Multi-modal Entity Alignment (MMEA). Building on this foundation, we propose a
novel SNAG method that utilizes a Transformer-based architecture equipped with
modality-level noise masking for the robust integration of multi-modal entity
features in KGs. By incorporating specific training objectives for both MKGC
and MMEA, our approach achieves SOTA performance across a total of ten datasets
(three for MKGC and seven for MEMA), demonstrating its robustness and
versatility. Besides, SNAG can not only function as a standalone model but also
enhance other existing methods, providing stable performance improvements. Our
code and data are available at: https://github.com/zjukg/SNAG.",0
Constructing Variables Using Classifiers as an Aid to Regression: An Empirical Assessment,2403.06829v2,http://arxiv.org/abs/2403.06829v2,2024-03-11 15:44:40+00:00,"This paper proposes a method for the automatic creation of variables (in the
case of regression) that complement the information contained in the initial
input vector. The method works as a pre-processing step in which the continuous
values of the variable to be regressed are discretized into a set of intervals
which are then used to define value thresholds. Then classifiers are trained to
predict whether the value to be regressed is less than or equal to each of
these thresholds. The different outputs of the classifiers are then
concatenated in the form of an additional vector of variables that enriches the
initial vector of the regression problem. The implemented system can thus be
considered as a generic pre-processing tool. We tested the proposed enrichment
method with 5 types of regressors and evaluated it in 33 regression datasets.
Our experimental results confirm the interest of the approach.",0
NeuPAN: Direct Point Robot Navigation with End-to-End Model-based Learning,2403.06828v1,http://arxiv.org/abs/2403.06828v1,2024-03-11 15:44:38+00:00,"Navigating a nonholonomic robot in a cluttered environment requires extremely
accurate perception and locomotion for collision avoidance. This paper presents
NeuPAN: a real-time, highly-accurate, map-free, robot-agnostic, and
environment-invariant robot navigation solution. Leveraging a tightly-coupled
perception-locomotion framework, NeuPAN has two key innovations compared to
existing approaches: 1) it directly maps raw points to a learned multi-frame
distance space, avoiding error propagation from perception to control; 2) it is
interpretable from an end-to-end model-based learning perspective, enabling
provable convergence. The crux of NeuPAN is to solve a high-dimensional
end-to-end mathematical model with various point-level constraints using the
plug-and-play (PnP) proximal alternating-minimization network (PAN) with
neurons in the loop. This allows NeuPAN to generate real-time, end-to-end,
physically-interpretable motions directly from point clouds, which seamlessly
integrates data- and knowledge-engines, where its network parameters are
adjusted via back propagation. We evaluate NeuPAN on car-like robot,
wheel-legged robot, and passenger autonomous vehicle, in both simulated and
real-world environments. Experiments demonstrate that NeuPAN outperforms
various benchmarks, in terms of accuracy, efficiency, robustness, and
generalization capability across various environments, including the cluttered
sandbox, office, corridor, and parking lot. We show that NeuPAN works well in
unstructured environments with arbitrary-shape undetectable objects, making
impassable ways passable.",0
In-context Exploration-Exploitation for Reinforcement Learning,2403.06826v1,http://arxiv.org/abs/2403.06826v1,2024-03-11 15:43:14+00:00,"In-context learning is a promising approach for online policy learning of
offline reinforcement learning (RL) methods, which can be achieved at inference
time without gradient optimization. However, this method is hindered by
significant computational costs resulting from the gathering of large training
trajectory sets and the need to train large Transformer models. We address this
challenge by introducing an In-context Exploration-Exploitation (ICEE)
algorithm, designed to optimize the efficiency of in-context policy learning.
Unlike existing models, ICEE performs an exploration-exploitation trade-off at
inference time within a Transformer model, without the need for explicit
Bayesian inference. Consequently, ICEE can solve Bayesian optimization problems
as efficiently as Gaussian process biased methods do, but in significantly less
time. Through experiments in grid world environments, we demonstrate that ICEE
can learn to solve new RL tasks using only tens of episodes, marking a
substantial improvement over the hundreds of episodes needed by the previous
in-context learning method.",0
Are Targeted Messages More Effective?,2403.06817v1,http://arxiv.org/abs/2403.06817v1,2024-03-11 15:34:57+00:00,"Graph neural networks (GNN) are deep learning architectures for graphs.
Essentially, a GNN is a distributed message passing algorithm, which is
controlled by parameters learned from data. It operates on the vertices of a
graph: in each iteration, vertices receive a message on each incoming edge,
aggregate these messages, and then update their state based on their current
state and the aggregated messages. The expressivity of GNNs can be
characterised in terms of certain fragments of first-order logic with counting
and the Weisfeiler-Lehman algorithm.
  The core GNN architecture comes in two different versions. In the first
version, a message only depends on the state of the source vertex, whereas in
the second version it depends on the states of the source and target vertices.
In practice, both of these versions are used, but the theory of GNNs so far
mostly focused on the first one. On the logical side, the two versions
correspond to two fragments of first-order logic with counting that we call
modal and guarded.
  The question whether the two versions differ in their expressivity has been
mostly overlooked in the GNN literature and has only been asked recently
(Grohe, LICS'23). We answer this question here. It turns out that the answer is
not as straightforward as one might expect. By proving that the modal and
guarded fragment of first-order logic with counting have the same expressivity
over labelled undirected graphs, we show that in a non-uniform setting the two
GNN versions have the same expressivity. However, we also prove that in a
uniform setting the second version is strictly more expressive.",0
"Efficient first-order algorithms for large-scale, non-smooth maximum entropy models with application to wildfire science",2403.06816v1,http://arxiv.org/abs/2403.06816v1,2024-03-11 15:33:55+00:00,"Maximum entropy (Maxent) models are a class of statistical models that use
the maximum entropy principle to estimate probability distributions from data.
Due to the size of modern data sets, Maxent models need efficient optimization
algorithms to scale well for big data applications. State-of-the-art algorithms
for Maxent models, however, were not originally designed to handle big data
sets; these algorithms either rely on technical devices that may yield
unreliable numerical results, scale poorly, or require smoothness assumptions
that many practical Maxent models lack. In this paper, we present novel
optimization algorithms that overcome the shortcomings of state-of-the-art
algorithms for training large-scale, non-smooth Maxent models. Our proposed
first-order algorithms leverage the Kullback-Leibler divergence to train
large-scale and non-smooth Maxent models efficiently. For Maxent models with
discrete probability distribution of $n$ elements built from samples, each
containing $m$ features, the stepsize parameters estimation and iterations in
our algorithms scale on the order of $O(mn)$ operations and can be trivially
parallelized. Moreover, the strong $\ell_{1}$ convexity of the
Kullback--Leibler divergence allows for larger stepsize parameters, thereby
speeding up the convergence rate of our algorithms. To illustrate the
efficiency of our novel algorithms, we consider the problem of estimating
probabilities of fire occurrences as a function of ecological features in the
Western US MTBS-Interagency wildfire data set. Our numerical results show that
our algorithms outperform the state of the arts by one order of magnitude and
yield results that agree with physical models of wildfire occurrence and
previous statistical analyses of wildfire drivers.",0
ε-Neural Thompson Sampling of Deep Brain Stimulation for Parkinson Disease Treatment,2403.06814v1,http://arxiv.org/abs/2403.06814v1,2024-03-11 15:33:40+00:00,"Deep Brain Stimulation (DBS) stands as an effective intervention for
alleviating the motor symptoms of Parkinson's disease (PD). Traditional
commercial DBS devices are only able to deliver fixed-frequency periodic pulses
to the basal ganglia (BG) regions of the brain, i.e., continuous DBS (cDBS).
However, they in general suffer from energy inefficiency and side effects, such
as speech impairment. Recent research has focused on adaptive DBS (aDBS) to
resolve the limitations of cDBS. Specifically, reinforcement learning (RL)
based approaches have been developed to adapt the frequencies of the stimuli in
order to achieve both energy efficiency and treatment efficacy. However, RL
approaches in general require significant amount of training data and
computational resources, making it intractable to integrate RL policies into
real-time embedded systems as needed in aDBS. In contrast, contextual
multi-armed bandits (CMAB) in general lead to better sample efficiency compared
to RL. In this study, we propose a CMAB solution for aDBS. Specifically, we
define the context as the signals capturing irregular neuronal firing
activities in the BG regions (i.e., beta-band power spectral density), while
each arm signifies the (discretized) pulse frequency of the stimulation.
Moreover, an {\epsilon}-exploring strategy is introduced on top of the classic
Thompson sampling method, leading to an algorithm called {\epsilon}-Neural
Thompson sampling ({\epsilon}-NeuralTS), such that the learned CMAB policy can
better balance exploration and exploitation of the BG environment. The
{\epsilon}-NeuralTS algorithm is evaluated using a computation BG model that
captures the neuronal activities in PD patients' brains. The results show that
our method outperforms both existing cDBS methods and CMAB baselines.",0
LeOCLR: Leveraging Original Images for Contrastive Learning of Visual Representations,2403.06813v1,http://arxiv.org/abs/2403.06813v1,2024-03-11 15:33:32+00:00,"Contrastive instance discrimination outperforms supervised learning in
downstream tasks like image classification and object detection. However, this
approach heavily relies on data augmentation during representation learning,
which may result in inferior results if not properly implemented. Random
cropping followed by resizing is a common form of data augmentation used in
contrastive learning, but it can lead to degraded representation learning if
the two random crops contain distinct semantic content. To address this issue,
this paper introduces LeOCLR (Leveraging Original Images for Contrastive
Learning of Visual Representations), a framework that employs a new instance
discrimination approach and an adapted loss function that ensures the shared
region between positive pairs is semantically correct. The experimental results
show that our approach consistently improves representation learning across
different datasets compared to baseline models. For example, our approach
outperforms MoCo-v2 by 5.1% on ImageNet-1K in linear evaluation and several
other methods on transfer learning tasks.",0
Monotone Individual Fairness,2403.06812v1,http://arxiv.org/abs/2403.06812v1,2024-03-11 15:32:56+00:00,"We revisit the problem of online learning with individual fairness, where an
online learner strives to maximize predictive accuracy while ensuring that
similar individuals are treated similarly. We first extend the frameworks of
Gillen et al. (2018); Bechavod et al. (2020), which rely on feedback from human
auditors regarding fairness violations, as we consider auditing schemes that
are capable of aggregating feedback from any number of auditors, using a rich
class we term monotone aggregation functions. We then prove a characterization
for such auditing schemes, practically reducing the analysis of auditing for
individual fairness by multiple auditors to that of auditing by
(instance-specific) single auditors. Using our generalized framework, we
present an oracle-efficient algorithm achieving an upper bound frontier of
$(\mathcal{O}(T^{1/2+2b}),\mathcal{O}(T^{3/4-b}))$ respectively for regret,
number of fairness violations, for $0\leq b \leq 1/4$. We then study an online
classification setting where label feedback is available for
positively-predicted individuals only, and present an oracle-efficient
algorithm achieving an upper bound frontier of
$(\mathcal{O}(T^{2/3+2b}),\mathcal{O}(T^{5/6-b}))$ for regret, number of
fairness violations, for $0\leq b \leq 1/6$. In both settings, our algorithms
improve on the best known bounds for oracle-efficient algorithms. Furthermore,
our algorithms offer significant improvements in computational efficiency,
greatly reducing the number of required calls to an (offline) optimization
oracle per round, to $\tilde{\mathcal{O}}(\alpha^{-2})$ in the full information
setting, and $\tilde{\mathcal{O}}(\alpha^{-2} + k^2T^{1/3})$ in the partial
information setting, where $\alpha$ is the sensitivity for reporting fairness
violations, and $k$ is the number of individuals in a round.",0
Deep Learning Approaches for Human Action Recognition in Video Data,2403.06810v1,http://arxiv.org/abs/2403.06810v1,2024-03-11 15:31:25+00:00,"Human action recognition in videos is a critical task with significant
implications for numerous applications, including surveillance, sports
analytics, and healthcare. The challenge lies in creating models that are both
precise in their recognition capabilities and efficient enough for practical
use. This study conducts an in-depth analysis of various deep learning models
to address this challenge. Utilizing a subset of the UCF101 Videos dataset, we
focus on Convolutional Neural Networks (CNNs), Recurrent Neural Networks
(RNNs), and Two-Stream ConvNets. The research reveals that while CNNs
effectively capture spatial features and RNNs encode temporal sequences,
Two-Stream ConvNets exhibit superior performance by integrating spatial and
temporal dimensions. These insights are distilled from the evaluation metrics
of accuracy, precision, recall, and F1-score. The results of this study
underscore the potential of composite models in achieving robust human action
recognition and suggest avenues for future research in optimizing these models
for real-world deployment.",0
Multistep Consistency Models,2403.06807v1,http://arxiv.org/abs/2403.06807v1,2024-03-11 15:26:34+00:00,"Diffusion models are relatively easy to train but require many steps to
generate samples. Consistency models are far more difficult to train, but
generate samples in a single step.
  In this paper we propose Multistep Consistency Models: A unification between
Consistency Models (Song et al., 2023) and TRACT (Berthelot et al., 2023) that
can interpolate between a consistency model and a diffusion model: a trade-off
between sampling speed and sampling quality. Specifically, a 1-step consistency
model is a conventional consistency model whereas we show that a $\infty$-step
consistency model is a diffusion model.
  Multistep Consistency Models work really well in practice. By increasing the
sample budget from a single step to 2-8 steps, we can train models more easily
that generate higher quality samples, while retaining much of the sampling
speed benefits. Notable results are 1.4 FID on Imagenet 64 in 8 step and 2.1
FID on Imagenet128 in 8 steps with consistency distillation. We also show that
our method scales to a text-to-image diffusion model, generating samples that
are very close to the quality of the original model.",0
On the Global Convergence of Policy Gradient in Average Reward Markov Decision Processes,2403.06806v1,http://arxiv.org/abs/2403.06806v1,2024-03-11 15:25:03+00:00,"We present the first finite time global convergence analysis of policy
gradient in the context of infinite horizon average reward Markov decision
processes (MDPs). Specifically, we focus on ergodic tabular MDPs with finite
state and action spaces. Our analysis shows that the policy gradient iterates
converge to the optimal policy at a sublinear rate of
$O\left({\frac{1}{T}}\right),$ which translates to $O\left({\log(T)}\right)$
regret, where $T$ represents the number of iterations. Prior work on
performance bounds for discounted reward MDPs cannot be extended to average
reward MDPs because the bounds grow proportional to the fifth power of the
effective horizon. Thus, our primary contribution is in proving that the policy
gradient algorithm converges for average-reward MDPs and in obtaining
finite-time performance guarantees. In contrast to the existing discounted
reward performance bounds, our performance bounds have an explicit dependence
on constants that capture the complexity of the underlying MDP. Motivated by
this observation, we reexamine and improve the existing performance bounds for
discounted reward MDPs. We also present simulations to empirically evaluate the
performance of average reward policy gradient algorithm.",0
CT2Rep: Automated Radiology Report Generation for 3D Medical Imaging,2403.06801v1,http://arxiv.org/abs/2403.06801v1,2024-03-11 15:17:45+00:00,"Medical imaging plays a crucial role in diagnosis, with radiology reports
serving as vital documentation. Automating report generation has emerged as a
critical need to alleviate the workload of radiologists. While machine learning
has facilitated report generation for 2D medical imaging, extending this to 3D
has been unexplored due to computational complexity and data scarcity. We
introduce the first method to generate radiology reports for 3D medical
imaging, specifically targeting chest CT volumes. Given the absence of
comparable methods, we establish a baseline using an advanced 3D vision encoder
in medical imaging to demonstrate our method's effectiveness, which leverages a
novel auto-regressive causal transformer. Furthermore, recognizing the benefits
of leveraging information from previous visits, we augment CT2Rep with a
cross-attention-based multi-modal fusion module and hierarchical memory,
enabling the incorporation of longitudinal multimodal data. Access our code at:
https://github.com/ibrahimethemhamamci/CT2Rep",0
MambaMIL: Enhancing Long Sequence Modeling with Sequence Reordering in Computational Pathology,2403.06800v1,http://arxiv.org/abs/2403.06800v1,2024-03-11 15:17:25+00:00,"Multiple Instance Learning (MIL) has emerged as a dominant paradigm to
extract discriminative feature representations within Whole Slide Images (WSIs)
in computational pathology. Despite driving notable progress, existing MIL
approaches suffer from limitations in facilitating comprehensive and efficient
interactions among instances, as well as challenges related to time-consuming
computations and overfitting. In this paper, we incorporate the Selective Scan
Space State Sequential Model (Mamba) in Multiple Instance Learning (MIL) for
long sequence modeling with linear complexity, termed as MambaMIL. By
inheriting the capability of vanilla Mamba, MambaMIL demonstrates the ability
to comprehensively understand and perceive long sequences of instances.
Furthermore, we propose the Sequence Reordering Mamba (SR-Mamba) aware of the
order and distribution of instances, which exploits the inherent valuable
information embedded within the long sequences. With the SR-Mamba as the core
component, MambaMIL can effectively capture more discriminative features and
mitigate the challenges associated with overfitting and high computational
overhead. Extensive experiments on two public challenging tasks across nine
diverse datasets demonstrate that our proposed framework performs favorably
against state-of-the-art MIL methods. The code is released at
https://github.com/isyangshu/MambaMIL.",0
Dynamic Perturbation-Adaptive Adversarial Training on Medical Image Classification,2403.06798v1,http://arxiv.org/abs/2403.06798v1,2024-03-11 15:16:20+00:00,"Remarkable successes were made in Medical Image Classification (MIC)
recently, mainly due to wide applications of convolutional neural networks
(CNNs). However, adversarial examples (AEs) exhibited imperceptible similarity
with raw data, raising serious concerns on network robustness. Although
adversarial training (AT), in responding to malevolent AEs, was recognized as
an effective approach to improve robustness, it was challenging to overcome
generalization decline of networks caused by the AT. In this paper, in order to
reserve high generalization while improving robustness, we proposed a dynamic
perturbation-adaptive adversarial training (DPAAT) method, which placed AT in a
dynamic learning environment to generate adaptive data-level perturbations and
provided a dynamically updated criterion by loss information collections to
handle the disadvantage of fixed perturbation sizes in conventional AT methods
and the dependence on external transference. Comprehensive testing on
dermatology HAM10000 dataset showed that the DPAAT not only achieved better
robustness improvement and generalization preservation but also significantly
enhanced mean average precision and interpretability on various CNNs,
indicating its great potential as a generic adversarial training method on the
MIC.",0
Leveraging Internal Representations of Model for Magnetic Image Classification,2403.06797v1,http://arxiv.org/abs/2403.06797v1,2024-03-11 15:15:50+00:00,"Data generated by edge devices has the potential to train intelligent
autonomous systems across various domains. Despite the emergence of diverse
machine learning approaches addressing privacy concerns and utilizing
distributed data, security issues persist due to the sensitive storage of data
shards in disparate locations. This paper introduces a potentially
groundbreaking paradigm for machine learning model training, specifically
designed for scenarios with only a single magnetic image and its corresponding
label image available. We harness the capabilities of Deep Learning to generate
concise yet informative samples, aiming to overcome data scarcity. Through the
utilization of deep learning's internal representations, our objective is to
efficiently address data scarcity issues and produce meaningful results. This
methodology presents a promising avenue for training machine learning models
with minimal data.",0
Provable Mutual Benefits from Federated Learning in Privacy-Sensitive Domains,2403.06672v1,http://arxiv.org/abs/2403.06672v1,2024-03-11 12:43:44+00:00,"Cross-silo federated learning (FL) allows data owners to train accurate
machine learning models by benefiting from each others private datasets.
Unfortunately, the model accuracy benefits of collaboration are often
undermined by privacy defenses. Therefore, to incentivize client participation
in privacy-sensitive domains, a FL protocol should strike a delicate balance
between privacy guarantees and end-model accuracy. In this paper, we study the
question of when and how a server could design a FL protocol provably
beneficial for all participants. First, we provide necessary and sufficient
conditions for the existence of mutually beneficial protocols in the context of
mean estimation and convex stochastic optimization. We also derive protocols
that maximize the total clients' utility, given symmetric privacy preferences.
Finally, we design protocols maximizing end-model accuracy and demonstrate
their benefits in synthetic experiments.",0
Untangling Gaussian Mixtures,2403.06671v1,http://arxiv.org/abs/2403.06671v1,2024-03-11 12:42:31+00:00,"Tangles were originally introduced as a concept to formalize regions of high
connectivity in graphs. In recent years, they have also been discovered as a
link between structural graph theory and data science: when interpreting
similarity in data sets as connectivity between points, finding clusters in the
data essentially amounts to finding tangles in the underlying graphs. This
paper further explores the potential of tangles in data sets as a means for a
formal study of clusters. Real-world data often follow a normal distribution.
Accounting for this, we develop a quantitative theory of tangles in data sets
drawn from Gaussian mixtures. To this end, we equip the data with a graph
structure that models similarity between the points and allows us to apply
tangle theory to the data. We provide explicit conditions under which tangles
associated with the marginal Gaussian distributions exist asymptotically almost
surely. This can be considered as a sufficient formal criterion for the
separabability of clusters in the data.",0
CEAT: Continual Expansion and Absorption Transformer for Non-Exemplar Class-Incremental Learning,2403.06670v2,http://arxiv.org/abs/2403.06670v2,2024-03-11 12:40:12+00:00,"In real-world applications, dynamic scenarios require the models to possess
the capability to learn new tasks continuously without forgetting the old
knowledge. Experience-Replay methods store a subset of the old images for joint
training. In the scenario of more strict privacy protection, storing the old
images becomes infeasible, which leads to a more severe plasticity-stability
dilemma and classifier bias. To meet the above challenges, we propose a new
architecture, named continual expansion and absorption transformer~(CEAT). The
model can learn the novel knowledge by extending the expanded-fusion layers in
parallel with the frozen previous parameters. After the task ends, we
losslessly absorb the extended parameters into the backbone to ensure that the
number of parameters remains constant. To improve the learning ability of the
model, we designed a novel prototype contrastive loss to reduce the overlap
between old and new classes in the feature space. Besides, to address the
classifier bias towards the new classes, we propose a novel approach to
generate the pseudo-features to correct the classifier. We experiment with our
methods on three standard Non-Exemplar Class-Incremental Learning~(NECIL)
benchmarks. Extensive experiments demonstrate that our model gets a significant
improvement compared with the previous works and achieves 5.38%, 5.20%, and
4.92% improvement on CIFAR-100, TinyImageNet, and ImageNet-Subset.",0
Unconditional deep-water limit of the intermediate long wave equation in low-regularity,2403.06554v1,http://arxiv.org/abs/2403.06554v1,2024-03-11 09:57:56+00:00,"In this paper, we establish the unconditional deep-water limit of the
intermediate long wave equation (ILW) to the Benjamin-Ono equation (BO) in
low-regularity Sobolev spaces on both the real line and the circle. Our main
tool is new unconditional uniqueness results for ILW in $H^s$ when $s_0<s\leq
\frac 14$ on the line and $s_0<s< \frac 12$ on the circle, where $s_0 =
3-\sqrt{33/4}\approx 0.1277$. Here, we adapt the strategy of Mo\c{s}incat-Pilod
(2023) for BO to the setting of ILW by viewing ILW as a perturbation of BO and
making use of the smoothing property of the perturbation term.",0
Detection of Object Throwing Behavior in Surveillance Videos,2403.06552v1,http://arxiv.org/abs/2403.06552v1,2024-03-11 09:53:19+00:00,"Anomalous behavior detection is a challenging research area within computer
vision. Progress in this area enables automated detection of dangerous behavior
using surveillance camera feeds. A dangerous behavior that is often overlooked
in other research is the throwing action in traffic flow, which is one of the
unique requirements of our Smart City project to enhance public safety. This
paper proposes a solution for throwing action detection in surveillance videos
using deep learning. At present, datasets for throwing actions are not publicly
available. To address the use-case of our Smart City project, we first generate
the novel public 'Throwing Action' dataset, consisting of 271 videos of
throwing actions performed by traffic participants, such as pedestrians,
bicyclists, and car drivers, and 130 normal videos without throwing actions.
Second, we compare the performance of different feature extractors for our
anomaly detection method on the UCF-Crime and Throwing-Action datasets. The
explored feature extractors are the Convolutional 3D (C3D) network, the
Inflated 3D ConvNet (I3D) network, and the Multi-Fiber Network (MFNet).
Finally, the performance of the anomaly detection algorithm is improved by
applying the Adam optimizer instead of Adadelta, and proposing a mean normal
loss function that covers the multitude of normal situations in traffic. Both
aspects yield better anomaly detection performance. Besides this, the proposed
mean normal loss function lowers the false alarm rate on the combined dataset.
The experimental results reach an area under the ROC curve of 86.10 for the
Throwing-Action dataset, and 80.13 on the combined dataset, respectively.",0
ToolRerank: Adaptive and Hierarchy-Aware Reranking for Tool Retrieval,2403.06551v1,http://arxiv.org/abs/2403.06551v1,2024-03-11 09:52:32+00:00,"Tool learning aims to extend the capabilities of large language models (LLMs)
with external tools. A major challenge in tool learning is how to support a
large number of tools, including unseen tools. To address this challenge,
previous studies have proposed retrieving suitable tools for the LLM based on
the user query. However, previously proposed methods do not consider the
differences between seen and unseen tools, nor do they take the hierarchy of
the tool library into account, which may lead to suboptimal performance for
tool retrieval. Therefore, to address the aforementioned issues, we propose
ToolRerank, an adaptive and hierarchy-aware reranking method for tool retrieval
to further refine the retrieval results. Specifically, our proposed ToolRerank
includes Adaptive Truncation, which truncates the retrieval results related to
seen and unseen tools at different positions, and Hierarchy-Aware Reranking,
which makes retrieval results more concentrated for single-tool queries and
more diverse for multi-tool queries. Experimental results show that ToolRerank
can improve the quality of the retrieval results, leading to better execution
results generated by the LLM.",0
OMH: Structured Sparsity via Optimally Matched Hierarchy for Unsupervised Semantic Segmentation,2403.06546v1,http://arxiv.org/abs/2403.06546v1,2024-03-11 09:46:41+00:00,"Unsupervised Semantic Segmentation (USS) involves segmenting images without
relying on predefined labels, aiming to alleviate the burden of extensive human
labeling. Existing methods utilize features generated by self-supervised models
and specific priors for clustering. However, their clustering objectives are
not involved in the optimization of the features during training. Additionally,
due to the lack of clear class definitions in USS, the resulting segments may
not align well with the clustering objective. In this paper, we introduce a
novel approach called Optimally Matched Hierarchy (OMH) to simultaneously
address the above issues. The core of our method lies in imposing structured
sparsity on the feature space, which allows the features to encode information
with different levels of granularity. The structure of this sparsity stems from
our hierarchy (OMH). To achieve this, we learn a soft but sparse hierarchy
among parallel clusters through Optimal Transport. Our OMH yields better
unsupervised segmentation performance compared to existing USS methods. Our
extensive experiments demonstrate the benefits of OMH when utilizing our
differentiable paradigm. We will make our code publicly available.",0
ReStainGAN: Leveraging IHC to IF Stain Domain Translation for in-silico Data Generation,2403.06545v1,http://arxiv.org/abs/2403.06545v1,2024-03-11 09:45:34+00:00,"The creation of in-silico datasets can expand the utility of existing
annotations to new domains with different staining patterns in computational
pathology. As such, it has the potential to significantly lower the cost
associated with building large and pixel precise datasets needed to train
supervised deep learning models. We propose a novel approach for the generation
of in-silico immunohistochemistry (IHC) images by disentangling morphology
specific IHC stains into separate image channels in immunofluorescence (IF)
images. The proposed approach qualitatively and quantitatively outperforms
baseline methods as proven by training nucleus segmentation models on the
created in-silico datasets.",0
Multi-Scale Implicit Transformer with Re-parameterize for Arbitrary-Scale Super-Resolution,2403.06536v1,http://arxiv.org/abs/2403.06536v1,2024-03-11 09:23:20+00:00,"Recently, the methods based on implicit neural representations have shown
excellent capabilities for arbitrary-scale super-resolution (ASSR). Although
these methods represent the features of an image by generating latent codes,
these latent codes are difficult to adapt for different magnification factors
of super-resolution, which seriously affects their performance. Addressing
this, we design Multi-Scale Implicit Transformer (MSIT), consisting of an
Multi-scale Neural Operator (MSNO) and Multi-Scale Self-Attention (MSSA). Among
them, MSNO obtains multi-scale latent codes through feature enhancement,
multi-scale characteristics extraction, and multi-scale characteristics
merging. MSSA further enhances the multi-scale characteristics of latent codes,
resulting in better performance. Furthermore, to improve the performance of
network, we propose the Re-Interaction Module (RIM) combined with the
cumulative training strategy to improve the diversity of learned information
for the network. We have systematically introduced multi-scale characteristics
for the first time in ASSR, extensive experiments are performed to validate the
effectiveness of MSIT, and our method achieves state-of-the-art performance in
arbitrary super-resolution tasks.",0
Decentralized and Lifelong-Adaptive Multi-Agent Collaborative Learning,2403.06535v1,http://arxiv.org/abs/2403.06535v1,2024-03-11 09:21:11+00:00,"Decentralized and lifelong-adaptive multi-agent collaborative learning aims
to enhance collaboration among multiple agents without a central server, with
each agent solving varied tasks over time. To achieve efficient collaboration,
agents should: i) autonomously identify beneficial collaborative relationships
in a decentralized manner; and ii) adapt to dynamically changing task
observations. In this paper, we propose DeLAMA, a decentralized multi-agent
lifelong collaborative learning algorithm with dynamic collaboration graphs. To
promote autonomous collaboration relationship learning, we propose a
decentralized graph structure learning algorithm, eliminating the need for
external priors. To facilitate adaptation to dynamic tasks, we design a memory
unit to capture the agents' accumulated learning history and knowledge, while
preserving finite storage consumption. To further augment the system's
expressive capabilities and computational efficiency, we apply algorithm
unrolling, leveraging the advantages of both mathematical optimization and
neural networks. This allows the agents to `learn to collaborate' through the
supervision of training tasks. Our theoretical analysis verifies that
inter-agent collaboration is communication efficient under a small number of
communication rounds. The experimental results verify its ability to facilitate
the discovery of collaboration strategies and adaptation to dynamic learning
scenarios, achieving a 98.80% reduction in MSE and a 188.87% improvement in
classification accuracy. We expect our work can serve as a foundational
technique to facilitate future works towards an intelligent, decentralized, and
dynamic multi-agent system. Code is available at
https://github.com/ShuoTang123/DeLAMA.",0
SARDet-100K: Towards Open-Source Benchmark and ToolKit for Large-Scale SAR Object Detection,2403.06534v1,http://arxiv.org/abs/2403.06534v1,2024-03-11 09:20:40+00:00,"Synthetic Aperture Radar (SAR) object detection has gained significant
attention recently due to its irreplaceable all-weather imaging capabilities.
However, this research field suffers from both limited public datasets (mostly
comprising <2K images with only mono-category objects) and inaccessible source
code. To tackle these challenges, we establish a new benchmark dataset and an
open-source method for large-scale SAR object detection. Our dataset,
SARDet-100K, is a result of intense surveying, collecting, and standardizing 10
existing SAR detection datasets, providing a large-scale and diverse dataset
for research purposes. To the best of our knowledge, SARDet-100K is the first
COCO-level large-scale multi-class SAR object detection dataset ever created.
With this high-quality dataset, we conducted comprehensive experiments and
uncovered a crucial challenge in SAR object detection: the substantial
disparities between the pretraining on RGB datasets and finetuning on SAR
datasets in terms of both data domain and model structure. To bridge these
gaps, we propose a novel Multi-Stage with Filter Augmentation (MSFA)
pretraining framework that tackles the problems from the perspective of data
input, domain transition, and model migration. The proposed MSFA method
significantly enhances the performance of SAR object detection models while
demonstrating exceptional generalizability and flexibility across diverse
models. This work aims to pave the way for further advancements in SAR object
detection. The dataset and code is available at
https://github.com/zcablii/SARDet_100K.",0
Reconstructing Visual Stimulus Images from EEG Signals Based on Deep Visual Representation Model,2403.06532v1,http://arxiv.org/abs/2403.06532v1,2024-03-11 09:19:09+00:00,"Reconstructing visual stimulus images is a significant task in neural
decoding, and up to now, most studies consider the functional magnetic
resonance imaging (fMRI) as the signal source. However, the fMRI-based image
reconstruction methods are difficult to widely applied because of the
complexity and high cost of the acquisition equipments. Considering the
advantages of low cost and easy portability of the electroencephalogram (EEG)
acquisition equipments, we propose a novel image reconstruction method based on
EEG signals in this paper. Firstly, to satisfy the high recognizability of
visual stimulus images in fast switching manner, we build a visual stimuli
image dataset, and obtain the EEG dataset by a corresponding EEG signals
collection experiment. Secondly, the deep visual representation model(DVRM)
consisting of a primary encoder and a subordinate decoder is proposed to
reconstruct visual stimuli. The encoder is designed based on the
residual-in-residual dense blocks to learn the distribution characteristics
between EEG signals and visual stimulus images, while the decoder is designed
based on the deep neural network to reconstruct the visual stimulus image from
the learned deep visual representation. The DVRM can fit the deep and multiview
visual features of human natural state and make the reconstructed images more
precise. Finally, we evaluate the DVRM in the quality of the generated images
on our EEG dataset. The results show that the DVRM have good performance in the
task of learning deep visual representation from EEG signals and generating
reconstructed images that are realistic and highly resemble the original
images.",0
Ultrafast switching of sliding ferroelectricity and dynamical magnetic field in van der Waals bilayer induced by light,2403.06531v2,http://arxiv.org/abs/2403.06531v2,2024-03-11 09:15:12+00:00,"Sliding ferroelectricity is a unique type of polarity recently observed in a
properly stacked van der Waals bilayer. However, electric-field control of
sliding ferroelectricity is hard and could induce large coercive electric
fields and serious leakage currents which corrode the ferroelectricity and
electronic properties, which are essential for modern two-dimensional
electronics and optoelectronics. Here, we proposed laser-pulse deterministic
control of sliding ferroelectricity in bilayer h-BN by first principles and
molecular dynamics simulation with machine-learned force fields. The laser
pulses excite shear modes which exhibit certain directional movements of
lateral sliding between bilayers. The vibration of excited modes under laser
pulses is predicted to overcome the energy barrier and achieve the switching of
sliding ferroelectricity. Furthermore, it is found that three possible sliding
transitions - between AB (BA) and BA (AB) stacking - can lead to the occurrence
of dynamical magnetic fields along three different directions. Remarkably, the
magnetic fields are generated by the simple linear motion of nonmagnetic
species, without any need for more exotic (circular, spiral) pathways. Such
predictions of deterministic control of sliding ferroelectricity and
multi-states of dynamical magnetic field thus expand the potential applications
of sliding ferroelectricity in memory and electronic devices.",0
Confidence-Aware RGB-D Face Recognition via Virtual Depth Synthesis,2403.06529v1,http://arxiv.org/abs/2403.06529v1,2024-03-11 09:12:24+00:00,"2D face recognition encounters challenges in unconstrained environments due
to varying illumination, occlusion, and pose. Recent studies focus on RGB-D
face recognition to improve robustness by incorporating depth information.
However, collecting sufficient paired RGB-D training data is expensive and
time-consuming, hindering wide deployment. In this work, we first construct a
diverse depth dataset generated by 3D Morphable Models for depth model
pre-training. Then, we propose a domain-independent pre-training framework that
utilizes readily available pre-trained RGB and depth models to separately
perform face recognition without needing additional paired data for retraining.
To seamlessly integrate the two distinct networks and harness the complementary
benefits of RGB and depth information for improved accuracy, we propose an
innovative Adaptive Confidence Weighting (ACW). This mechanism is designed to
learn confidence estimates for each modality to achieve modality fusion at the
score level. Our method is simple and lightweight, only requiring ACW training
beyond the backbone models. Experiments on multiple public RGB-D face
recognition benchmarks demonstrate state-of-the-art performance surpassing
previous methods based on depth estimation and feature fusion, validating the
efficacy of our approach.",0
Adaptive Federated Learning Over the Air,2403.06528v1,http://arxiv.org/abs/2403.06528v1,2024-03-11 09:10:37+00:00,"We propose a federated version of adaptive gradient methods, particularly
AdaGrad and Adam, within the framework of over-the-air model training. This
approach capitalizes on the inherent superposition property of wireless
channels, facilitating fast and scalable parameter aggregation. Meanwhile, it
enhances the robustness of the model training process by dynamically adjusting
the stepsize in accordance with the global gradient update. We derive the
convergence rate of the training algorithms, encompassing the effects of
channel fading and interference, for a broad spectrum of nonconvex loss
functions. Our analysis shows that the AdaGrad-based algorithm converges to a
stationary point at the rate of $\mathcal{O}( \ln{(T)} /{ T^{ 1 -
\frac{1}{\alpha} } } )$, where $\alpha$ represents the tail index of the
electromagnetic interference. This result indicates that the level of
heavy-tailedness in interference distribution plays a crucial role in the
training efficiency: the heavier the tail, the slower the algorithm converges.
In contrast, an Adam-like algorithm converges at the $\mathcal{O}( 1/T )$ rate,
demonstrating its advantage in expediting the model training process. We
conduct extensive experiments that corroborate our theoretical findings and
affirm the practical efficacy of our proposed federated adaptive gradient
methods.",0
Tactical Decision Making for Autonomous Trucks by Deep Reinforcement Learning with Total Cost of Operation Based Reward,2403.06524v1,http://arxiv.org/abs/2403.06524v1,2024-03-11 08:58:42+00:00,"We develop a deep reinforcement learning framework for tactical decision
making in an autonomous truck, specifically for Adaptive Cruise Control (ACC)
and lane change maneuvers in a highway scenario. Our results demonstrate that
it is beneficial to separate high-level decision-making processes and low-level
control actions between the reinforcement learning agent and the low-level
controllers based on physical models. In the following, we study optimizing the
performance with a realistic and multi-objective reward function based on Total
Cost of Operation (TCOP) of the truck using different approaches; by adding
weights to reward components, by normalizing the reward components and by using
curriculum learning techniques.",0
Active Generation for Image Classification,2403.06517v1,http://arxiv.org/abs/2403.06517v1,2024-03-11 08:45:31+00:00,"Recently, the growing capabilities of deep generative models have underscored
their potential in enhancing image classification accuracy. However, existing
methods often demand the generation of a disproportionately large number of
images compared to the original dataset, while having only marginal
improvements in accuracy. This computationally expensive and time-consuming
process hampers the practicality of such approaches. In this paper, we propose
to address the efficiency of image generation by focusing on the specific needs
and characteristics of the model. With a central tenet of active learning, our
method, named ActGen, takes a training-aware approach to image generation. It
aims to create images akin to the challenging or misclassified samples
encountered by the current model and incorporates these generated images into
the training set to augment model performance. ActGen introduces an attentive
image guidance technique, using real images as guides during the denoising
process of a diffusion model. The model's attention on class prompt is
leveraged to ensure the preservation of similar foreground object while
diversifying the background. Furthermore, we introduce a gradient-based
generation guidance method, which employs two losses to generate more
challenging samples and prevent the generated images from being too similar to
previously generated ones. Experimental results on the CIFAR and ImageNet
datasets demonstrate that our method achieves better performance with a
significantly reduced number of generated images.",0
Advancing Text-Driven Chest X-Ray Generation with Policy-Based Reinforcement Learning,2403.06516v1,http://arxiv.org/abs/2403.06516v1,2024-03-11 08:43:57+00:00,"Recent advances in text-conditioned image generation diffusion models have
begun paving the way for new opportunities in modern medical domain, in
particular, generating Chest X-rays (CXRs) from diagnostic reports.
Nonetheless, to further drive the diffusion models to generate CXRs that
faithfully reflect the complexity and diversity of real data, it has become
evident that a nontrivial learning approach is needed. In light of this, we
propose CXRL, a framework motivated by the potential of reinforcement learning
(RL). Specifically, we integrate a policy gradient RL approach with
well-designed multiple distinctive CXR-domain specific reward models. This
approach guides the diffusion denoising trajectory, achieving precise CXR
posture and pathological details. Here, considering the complex medical image
environment, we present ""RL with Comparative Feedback"" (RLCF) for the reward
mechanism, a human-like comparative evaluation that is known to be more
effective and reliable in complex scenarios compared to direct evaluation. Our
CXRL framework includes jointly optimizing learnable adaptive condition
embeddings (ACE) and the image generator, enabling the model to produce more
accurate and higher perceptual CXR quality. Our extensive evaluation of the
MIMIC-CXR-JPG dataset demonstrates the effectiveness of our RL-based tuning
approach. Consequently, our CXRL generates pathologically realistic CXRs,
establishing a new standard for generating CXRs with high fidelity to
real-world clinical scenarios.",0
Skeleton Supervised Airway Segmentation,2403.06510v1,http://arxiv.org/abs/2403.06510v1,2024-03-11 08:37:03+00:00,"Fully-supervised airway segmentation has accomplished significant triumphs
over the years in aiding pre-operative diagnosis and intra-operative
navigation. However, full voxel-level annotation constitutes a labor-intensive
and time-consuming task, often plagued by issues such as missing branches,
branch annotation discontinuity, or erroneous edge delineation. label-efficient
solutions for airway extraction are rarely explored yet primarily demanding in
medical practice. To this end, we introduce a novel skeleton-level annotation
(SkA) tailored to the airway, which simplifies the annotation workflow while
enhancing annotation consistency and accuracy, preserving the complete
topology. Furthermore, we propose a skeleton-supervised learning framework to
achieve accurate airway segmentation. Firstly, a dual-stream buffer inference
is introduced to realize initial label propagation from SkA, avoiding the
collapse of direct learning from SkA. Then, we construct a geometry-aware
dual-path propagation framework (GDP) to further promote complementary
propagation learning, composed of hard geometry-aware propagation learning and
soft geometry-aware propagation guidance. Experiments reveal that our proposed
framework outperforms the competing methods with SKA, which amounts to only
1.96% airways, and achieves comparable performance with the baseline model that
is fully supervised with 100% airways, demonstrating its significant potential
in achieving label-efficient segmentation for other tubular structures, such as
vessels.",0
Automatic Generation of Python Programs Using Context-Free Grammars,2403.06503v1,http://arxiv.org/abs/2403.06503v1,2024-03-11 08:25:52+00:00,"In recent years, data has emerged as the new gold, serving as a powerful tool
for creating intelligent systems. However, procuring high-quality data remains
challenging, especially for code. To address this, we developed TinyPy
Generator, a tool that generates random Python programs using a context-free
grammar. The generated programs are guaranteed to be correct by construction.
Our system uses custom production rules (in the Backus-Naur Form (BNF) format)
to recursively generate code. This allows us to generate code with different
levels of complexity, ranging from code containing only assignments to more
complex code containing conditionals and loops. Our proposed tool enables
effortless large-scale Python code generation, beneficial for a wide range of
applications. TinyPy Generator is particularly useful in the field of machine
learning, where it can generate substantial amounts of Python code for training
Python language models. Additionally, researchers who are studying programming
languages can utilize this tool to create datasets for their experiments, which
can help validate the robustness of code interpreters or compilers. Unlike
existing research, we have open-sourced our implementation. This allows
customization according to user needs and extends potential usage to other
languages.",0
A Converting Autoencoder Toward Low-latency and Energy-efficient DNN Inference at the Edge,2403.07036v1,http://arxiv.org/abs/2403.07036v1,2024-03-11 08:13:42+00:00,"Reducing inference time and energy usage while maintaining prediction
accuracy has become a significant concern for deep neural networks (DNN)
inference on resource-constrained edge devices. To address this problem, we
propose a novel approach based on ""converting"" autoencoder and lightweight
DNNs. This improves upon recent work such as early-exiting framework and DNN
partitioning. Early-exiting frameworks spend different amounts of computation
power for different input data depending upon their complexity. However, they
can be inefficient in real-world scenarios that deal with many hard image
samples. On the other hand, DNN partitioning algorithms that utilize the
computation power of both the cloud and edge devices can be affected by network
delays and intermittent connections between the cloud and the edge. We present
CBNet, a low-latency and energy-efficient DNN inference framework tailored for
edge devices. It utilizes a ""converting"" autoencoder to efficiently transform
hard images into easy ones, which are subsequently processed by a lightweight
DNN for inference. To the best of our knowledge, such autoencoder has not been
proposed earlier. Our experimental results using three popular
image-classification datasets on a Raspberry Pi 4, a Google Cloud instance, and
an instance with Nvidia Tesla K80 GPU show that CBNet achieves up to 4.8x
speedup in inference latency and 79% reduction in energy usage compared to
competing techniques while maintaining similar or higher accuracy.",0
"Detection of Unobserved Common Causes based on NML Code in Discrete, Mixed, and Continuous Variables",2403.06499v1,http://arxiv.org/abs/2403.06499v1,2024-03-11 08:11:52+00:00,"Causal discovery in the presence of unobserved common causes from
observational data only is a crucial but challenging problem. We categorize all
possible causal relationships between two random variables into the following
four categories and aim to identify one from observed data: two cases in which
either of the direct causality exists, a case that variables are independent,
and a case that variables are confounded by latent confounders. Although
existing methods have been proposed to tackle this problem, they require
unobserved variables to satisfy assumptions on the form of their equation
models. In our previous study (Kobayashi et al., 2022), the first causal
discovery method without such assumptions is proposed for discrete data and
named CLOUD. Using Normalized Maximum Likelihood (NML) Code, CLOUD selects a
model that yields the minimum codelength of the observed data from a set of
model candidates. This paper extends CLOUD to apply for various data types
across discrete, mixed, and continuous. We not only performed theoretical
analysis to show the consistency of CLOUD in terms of the model selection, but
also demonstrated that CLOUD is more effective than existing methods in
inferring causal relationships by extensive experiments on both synthetic and
real-world data.",0
Toward Generalist Anomaly Detection via In-context Residual Learning with Few-shot Sample Prompts,2403.06495v2,http://arxiv.org/abs/2403.06495v2,2024-03-11 08:07:46+00:00,"This paper explores the problem of Generalist Anomaly Detection (GAD), aiming
to train one single detection model that can generalize to detect anomalies in
diverse datasets from different application domains without any further
training on the target data. Some recent studies have shown that large
pre-trained Visual-Language Models (VLMs) like CLIP have strong generalization
capabilities on detecting industrial defects from various datasets, but their
methods rely heavily on handcrafted text prompts about defects, making them
difficult to generalize to anomalies in other applications, e.g., medical image
anomalies or semantic anomalies in natural images. In this work, we propose to
train a GAD model with few-shot normal images as sample prompts for AD on
diverse datasets on the fly. To this end, we introduce a novel approach that
learns an in-context residual learning model for GAD, termed InCTRL. It is
trained on an auxiliary dataset to discriminate anomalies from normal samples
based on a holistic evaluation of the residuals between query images and
few-shot normal sample prompts. Regardless of the datasets, per definition of
anomaly, larger residuals are expected for anomalies than normal samples,
thereby enabling InCTRL to generalize across different domains without further
training. Comprehensive experiments on nine AD datasets are performed to
establish a GAD benchmark that encapsulate the detection of industrial defect
anomalies, medical anomalies, and semantic anomalies in both one-vs-all and
multi-class setting, on which InCTRL is the best performer and significantly
outperforms state-of-the-art competing methods.",0
Multiple Population Alternate Evolution Neural Architecture Search,2403.07035v1,http://arxiv.org/abs/2403.07035v1,2024-03-11 08:05:01+00:00,"The effectiveness of Evolutionary Neural Architecture Search (ENAS) is
influenced by the design of the search space. Nevertheless, common methods
including the global search space, scalable search space and hierarchical
search space have certain limitations. Specifically, the global search space
requires a significant amount of computational resources and time, the scalable
search space sacrifices the diversity of network structures and the
hierarchical search space increases the search cost in exchange for network
diversity. To address above limitation, we propose a novel paradigm of
searching neural network architectures and design the Multiple Population
Alternate Evolution Neural Architecture Search (MPAE), which can achieve module
diversity with a smaller search cost. MPAE converts the search space into L
interconnected units and sequentially searches the units, then the above search
of the entire network be cycled several times to reduce the impact of previous
units on subsequent units. To accelerate the population evolution process, we
also propose the the population migration mechanism establishes an excellent
migration archive and transfers the excellent knowledge and experience in the
migration archive to new populations. The proposed method requires only 0.3 GPU
days to search a neural network on the CIFAR dataset and achieves the
state-of-the-art results.",0
Graph Neural Network with Two Uplift Estimators for Label-Scarcity Individual Uplift Modeling,2403.06489v1,http://arxiv.org/abs/2403.06489v1,2024-03-11 07:51:27+00:00,"Uplift modeling aims to measure the incremental effect, which we call uplift,
of a strategy or action on the users from randomized experiments or
observational data. Most existing uplift methods only use individual data,
which are usually not informative enough to capture the unobserved and complex
hidden factors regarding the uplift. Furthermore, uplift modeling scenario
usually has scarce labeled data, especially for the treatment group, which also
poses a great challenge for model training. Considering that the neighbors'
features and the social relationships are very informative to characterize a
user's uplift, we propose a graph neural network-based framework with two
uplift estimators, called GNUM, to learn from the social graph for uplift
estimation. Specifically, we design the first estimator based on a
class-transformed target. The estimator is general for all types of outcomes,
and is able to comprehensively model the treatment and control group data
together to approach the uplift. When the outcome is discrete, we further
design the other uplift estimator based on our defined partial labels, which is
able to utilize more labeled data from both the treatment and control groups,
to further alleviate the label scarcity problem. Comprehensive experiments on a
public dataset and two industrial datasets show a superior performance of our
proposed framework over state-of-the-art methods under various evaluation
metrics. The proposed algorithms have been deployed online to serve real-world
uplift estimation scenarios.",0
Multilingual Turn-taking Prediction Using Voice Activity Projection,2403.06487v2,http://arxiv.org/abs/2403.06487v2,2024-03-11 07:50:29+00:00,"This paper investigates the application of voice activity projection (VAP), a
predictive turn-taking model for spoken dialogue, on multilingual data,
encompassing English, Mandarin, and Japanese. The VAP model continuously
predicts the upcoming voice activities of participants in dyadic dialogue,
leveraging a cross-attention Transformer to capture the dynamic interplay
between participants. The results show that a monolingual VAP model trained on
one language does not make good predictions when applied to other languages.
However, a multilingual model, trained on all three languages, demonstrates
predictive performance on par with monolingual models across all languages.
Further analyses show that the multilingual model has learned to discern the
language of the input signal. We also analyze the sensitivity to pitch, a
prosodic cue that is thought to be important for turn-taking. Finally, we
compare two different audio encoders, contrastive predictive coding (CPC)
pre-trained on English, with a recent model based on multilingual wav2vec 2.0
(MMS).",0
Knowledge-aware Alert Aggregation in Large-scale Cloud Systems: a Hybrid Approach,2403.06485v1,http://arxiv.org/abs/2403.06485v1,2024-03-11 07:48:35+00:00,"Due to the scale and complexity of cloud systems, a system failure would
trigger an ""alert storm"", i.e., massive correlated alerts. Although these
alerts can be traced back to a few root causes, the overwhelming number makes
it infeasible for manual handling. Alert aggregation is thus critical to help
engineers concentrate on the root cause and facilitate failure resolution.
Existing methods typically utilize semantic similarity-based methods or
statistical methods to aggregate alerts. However, semantic similarity-based
methods overlook the causal rationale of alerts, while statistical methods can
hardly handle infrequent alerts.
  To tackle these limitations, we introduce leveraging external knowledge,
i.e., Standard Operation Procedure (SOP) of alerts as a supplement. We propose
COLA, a novel hybrid approach based on correlation mining and LLM (Large
Language Model) reasoning for online alert aggregation. The correlation mining
module effectively captures the temporal and spatial relations between alerts,
measuring their correlations in an efficient manner. Subsequently, only
uncertain pairs with low confidence are forwarded to the LLM reasoning module
for detailed analysis. This hybrid design harnesses both statistical evidence
for frequent alerts and the reasoning capabilities of computationally intensive
LLMs, ensuring the overall efficiency of COLA in handling large volumes of
alerts in practical scenarios. We evaluate COLA on three datasets collected
from the production environment of a large-scale cloud platform. The
experimental results show COLA achieves F1-scores from 0.901 to 0.930,
outperforming state-of-the-art methods and achieving comparable efficiency. We
also share our experience in deploying COLA in our real-world cloud system,
Cloud X.",0
Financial Default Prediction via Motif-preserving Graph Neural Network with Curriculum Learning,2403.06482v1,http://arxiv.org/abs/2403.06482v1,2024-03-11 07:44:56+00:00,"User financial default prediction plays a critical role in credit risk
forecasting and management. It aims at predicting the probability that the user
will fail to make the repayments in the future. Previous methods mainly extract
a set of user individual features regarding his own profiles and behaviors and
build a binary-classification model to make default predictions. However, these
methods cannot get satisfied results, especially for users with limited
information. Although recent efforts suggest that default prediction can be
improved by social relations, they fail to capture the higher-order topology
structure at the level of small subgraph patterns. In this paper, we fill in
this gap by proposing a motif-preserving Graph Neural Network with curriculum
learning (MotifGNN) to jointly learn the lower-order structures from the
original graph and higherorder structures from multi-view motif-based graphs
for financial default prediction. Specifically, to solve the problem of weak
connectivity in motif-based graphs, we design the motif-based gating mechanism.
It utilizes the information learned from the original graph with good
connectivity to strengthen the learning of the higher-order structure. And
considering that the motif patterns of different samples are highly unbalanced,
we propose a curriculum learning mechanism on the whole learning process to
more focus on the samples with uncommon motif distributions. Extensive
experiments on one public dataset and two industrial datasets all demonstrate
the effectiveness of our proposed method.",0
Toward Robust Canine Cardiac Diagnosis: Deep Prototype Alignment Network-Based Few-Shot Segmentation in Veterinary Medicine,2403.06471v1,http://arxiv.org/abs/2403.06471v1,2024-03-11 07:19:29+00:00,"In the cutting-edge domain of medical artificial intelligence (AI),
remarkable advances have been achieved in areas such as diagnosis, prediction,
and therapeutic interventions. Despite these advances, the technology for image
segmentation faces the significant barrier of having to produce extensively
annotated datasets. To address this challenge, few-shot segmentation (FSS) has
been recognized as one of the innovative solutions. Although most of the FSS
research has focused on human health care, its application in veterinary
medicine, particularly for pet care, remains largely limited. This study has
focused on accurate segmentation of the heart and left atrial enlargement on
canine chest radiographs using the proposed deep prototype alignment network
(DPANet). The PANet architecture is adopted as the backbone model, and
experiments are conducted using various encoders based on VGG-19, ResNet-18,
and ResNet-50 to extract features. Experimental results demonstrate that the
proposed DPANet achieves the highest performance. In the 2way-1shot scenario,
it achieves the highest intersection over union (IoU) value of 0.6966, and in
the 2way-5shot scenario, it achieves the highest IoU value of 0.797. The DPANet
not only signifies a performance improvement, but also shows an improved
training speed in the 2way-5shot scenario. These results highlight our model's
exceptional capability as a trailblazing solution for segmenting the heart and
left atrial enlargement in veterinary applications through FSS, setting a new
benchmark in veterinary AI research, and demonstrating its superior potential
to veterinary medicine advances.",0
RL-MSA: a Reinforcement Learning-based Multi-line bus Scheduling Approach,2403.06466v1,http://arxiv.org/abs/2403.06466v1,2024-03-11 07:07:05+00:00,"Multiple Line Bus Scheduling Problem (MLBSP) is vital to save operational
cost of bus company and guarantee service quality for passengers. Existing
approaches typically generate a bus scheduling scheme in an offline manner and
then schedule buses according to the scheme. In practice, uncertain events such
as traffic congestion occur frequently, which may make the pre-determined bus
scheduling scheme infeasible. In this paper, MLBSP is modeled as a Markov
Decision Process (MDP). A Reinforcement Learning-based Multi-line bus
Scheduling Approach (RL-MSA) is proposed for bus scheduling at both the offline
and online phases. At the offline phase, deadhead decision is integrated into
bus selection decision for the first time to simplify the learning problem. At
the online phase, deadhead decision is made through a time window mechanism
based on the policy learned at the offline phase. We develop several new and
useful state features including the features for control points, bus lines and
buses. A bus priority screening mechanism is invented to construct bus-related
features. Considering the interests of both the bus company and passengers, a
reward function combining the final reward and the step-wise reward is devised.
Experiments at the offline phase demonstrate that the number of buses used of
RL-MSA is decreased compared with offline optimization approaches. At the
online phase, RL-MSA can cover all departure times in a timetable (i.e.,
service quality) without increasing the number of buses used (i.e., operational
cost).",0
Towards the Uncharted: Density-Descending Feature Perturbation for Semi-supervised Semantic Segmentation,2403.06462v2,http://arxiv.org/abs/2403.06462v2,2024-03-11 06:59:05+00:00,"Semi-supervised semantic segmentation allows model to mine effective
supervision from unlabeled data to complement label-guided training. Recent
research has primarily focused on consistency regularization techniques,
exploring perturbation-invariant training at both the image and feature levels.
In this work, we proposed a novel feature-level consistency learning framework
named Density-Descending Feature Perturbation (DDFP). Inspired by the
low-density separation assumption in semi-supervised learning, our key insight
is that feature density can shed a light on the most promising direction for
the segmentation classifier to explore, which is the regions with lower
density. We propose to shift features with confident predictions towards
lower-density regions by perturbation injection. The perturbed features are
then supervised by the predictions on the original features, thereby compelling
the classifier to explore less dense regions to effectively regularize the
decision boundary. Central to our method is the estimation of feature density.
To this end, we introduce a lightweight density estimator based on normalizing
flow, allowing for efficient capture of the feature density distribution in an
online manner. By extracting gradients from the density estimator, we can
determine the direction towards less dense regions for each feature. The
proposed DDFP outperforms other designs on feature-level perturbations and
shows state of the art performances on both Pascal VOC and Cityscapes dataset
under various partition protocols. The project is available at
https://github.com/Gavinwxy/DDFP.",0
Leveraging graph neural networks for supporting Automatic Triage of Patients,2403.07038v1,http://arxiv.org/abs/2403.07038v1,2024-03-11 09:54:35+00:00,"Patient triage plays a crucial role in emergency departments, ensuring timely
and appropriate care based on correctly evaluating the emergency grade of
patient conditions.
  Triage methods are generally performed by human operator based on her own
experience and information that are gathered from the patient management
process.
  Thus, it is a process that can generate errors in emergency level
associations. Recently, Traditional triage methods heavily rely on human
decisions, which can be subjective and prone to errors.
  Recently, a growing interest has been focused on leveraging artificial
intelligence (AI) to develop algorithms able to maximize information gathering
and minimize errors in patient triage processing.
  We define and implement an AI based module to manage patients emergency code
assignments in emergency departments. It uses emergency department historical
data to train the medical decision process. Data containing relevant patient
information, such as vital signs, symptoms, and medical history, are used to
accurately classify patients into triage categories. Experimental results
demonstrate that the proposed algorithm achieved high accuracy outperforming
traditional triage methods. By using the proposed method we claim that
healthcare professionals can predict severity index to guide patient management
processing and resource allocation.",0
Data-driven architecture to encode information in the kinematics of robots and artificial avatars,2403.06557v1,http://arxiv.org/abs/2403.06557v1,2024-03-11 10:00:26+00:00,"We present a data-driven control architecture for modifying the kinematics of
robots and artificial avatars to encode specific information such as the
presence or not of an emotion in the movements of an avatar or robot driven by
a human operator. We validate our approach on an experimental dataset obtained
during the reach-to-grasp phase of a pick-and-place task.",0
PeerAiD: Improving Adversarial Distillation from a Specialized Peer Tutor,2403.06668v1,http://arxiv.org/abs/2403.06668v1,2024-03-11 12:36:14+00:00,"Adversarial robustness of the neural network is a significant concern when it
is applied to security-critical domains. In this situation, adversarial
distillation is a promising option which aims to distill the robustness of the
teacher network to improve the robustness of a small student network. Previous
works pretrain the teacher network to make it robust to the adversarial
examples aimed at itself. However, the adversarial examples are dependent on
the parameters of the target network. The fixed teacher network inevitably
degrades its robustness against the unseen transferred adversarial examples
which targets the parameters of the student network in the adversarial
distillation process. We propose PeerAiD to make a peer network learn the
adversarial examples of the student network instead of adversarial examples
aimed at itself. PeerAiD is an adversarial distillation that trains the peer
network and the student network simultaneously in order to make the peer
network specialized for defending the student network. We observe that such
peer networks surpass the robustness of pretrained robust teacher network
against student-attacked adversarial samples. With this peer network and
adversarial distillation, PeerAiD achieves significantly higher robustness of
the student network with AutoAttack (AA) accuracy up to 1.66%p and improves the
natural accuracy of the student network up to 4.72%p with ResNet-18 and
TinyImageNet dataset.",0
Nonlinear spatial evolution of degenerate quartets of water waves,2403.06558v1,http://arxiv.org/abs/2403.06558v1,2024-03-11 10:00:42+00:00,"In this manuscript we investigate the Benjamin-Feir (or modulation)
instability for the spatial evolution of water waves from the perspective of
the discrete, spatial Zakharov equation, which captures cubically nonlinear and
resonant wave interactions in deep water without restrictions on spectral
bandwidth. Spatial evolution, with measurements at discrete locations, is
pertinent for laboratory hydrodynamic experiments, such as in wave flumes,
which rely on time-series measurements at a series of fixed gauges installed
along the facility. This setting is likewise appropriate for experiments in
electromagnetic and plasma waves. Through a reformulation of the problem for a
degenerate quartet, we bring to bear techniques of phase-plane analysis which
elucidate the full dynamics without recourse to linear stability analysis. In
particular we find hitherto unexplored breather solutions and discuss the
optimal transfer of energy from carrier to sidebands. Finally, we discuss the
observability of such discrete solutions in light of numerical simulations.",0
Smart-Infinity: Fast Large Language Model Training using Near-Storage Processing on a Real System,2403.06664v1,http://arxiv.org/abs/2403.06664v1,2024-03-11 12:32:14+00:00,"The recent huge advance of Large Language Models (LLMs) is mainly driven by
the increase in the number of parameters. This has led to substantial memory
capacity requirements, necessitating the use of dozens of GPUs just to meet the
capacity. One popular solution to this is storage-offloaded training, which
uses host memory and storage as an extended memory hierarchy. However, this
obviously comes at the cost of storage bandwidth bottleneck because storage
devices have orders of magnitude lower bandwidth compared to that of GPU device
memories. Our work, Smart-Infinity, addresses the storage bandwidth bottleneck
of storage-offloaded LLM training using near-storage processing devices on a
real system. The main component of Smart-Infinity is SmartUpdate, which
performs parameter updates on custom near-storage accelerators. We identify
that moving parameter updates to the storage side removes most of the storage
traffic. In addition, we propose an efficient data transfer handler structure
to address the system integration issues for Smart-Infinity. The handler allows
overlapping data transfers with fixed memory consumption by reusing the device
buffer. Lastly, we propose accelerator-assisted gradient
compression/decompression to enhance the scalability of Smart-Infinity. When
scaling to multiple near-storage processing devices, the write traffic on the
shared channel becomes the bottleneck. To alleviate this, we compress the
gradients on the GPU and decompress them on the accelerators. It provides
further acceleration from reduced traffic. As a result, Smart-Infinity achieves
a significant speedup compared to the baseline. Notably, Smart-Infinity is a
ready-to-use approach that is fully integrated into PyTorch on a real system.
We will open-source Smart-Infinity to facilitate its use.",0
epsilon-Mesh Attack: A Surface-based Adversarial Point Cloud Attack for Facial Expression Recognition,2403.06661v1,http://arxiv.org/abs/2403.06661v1,2024-03-11 12:29:55+00:00,"Point clouds and meshes are widely used 3D data structures for many computer
vision applications. While the meshes represent the surfaces of an object,
point cloud represents sampled points from the surface which is also the output
of modern sensors such as LiDAR and RGB-D cameras. Due to the wide application
area of point clouds and the recent advancements in deep neural networks,
studies focusing on robust classification of the 3D point cloud data emerged.
To evaluate the robustness of deep classifier networks, a common method is to
use adversarial attacks where the gradient direction is followed to change the
input slightly. The previous studies on adversarial attacks are generally
evaluated on point clouds of daily objects. However, considering 3D faces,
these adversarial attacks tend to affect the person's facial structure more
than the desired amount and cause malformation. Specifically for facial
expressions, even a small adversarial attack can have a significant effect on
the face structure. In this paper, we suggest an adversarial attack called
$\epsilon$-Mesh Attack, which operates on point cloud data via limiting
perturbations to be on the mesh surface. We also parameterize our attack by
$\epsilon$ to scale the perturbation mesh. Our surface-based attack has tighter
perturbation bounds compared to $L_2$ and $L_\infty$ norm bounded attacks that
operate on unit-ball. Even though our method has additional constraints, our
experiments on CoMA, Bosphorus and FaceWarehouse datasets show that
$\epsilon$-Mesh Attack (Perpendicular) successfully confuses trained DGCNN and
PointNet models $99.72\%$ and $97.06\%$ of the time, with indistinguishable
facial deformations. The code is available at
https://github.com/batuceng/e-mesh-attack.",0
Zero-Shot ECG Classification with Multimodal Learning and Test-time Clinical Knowledge Enhancement,2403.06659v1,http://arxiv.org/abs/2403.06659v1,2024-03-11 12:28:55+00:00,"Electrocardiograms (ECGs) are non-invasive diagnostic tools crucial for
detecting cardiac arrhythmic diseases in clinical practice. While ECG
Self-supervised Learning (eSSL) methods show promise in representation learning
from unannotated ECG data, they often overlook the clinical knowledge that can
be found in reports. This oversight and the requirement for annotated samples
for downstream tasks limit eSSL's versatility. In this work, we address these
issues with the Multimodal ECG Representation Learning (MERL}) framework.
Through multimodal learning on ECG records and associated reports, MERL is
capable of performing zero-shot ECG classification with text prompts,
eliminating the need for training data in downstream tasks. At test time, we
propose the Clinical Knowledge Enhanced Prompt Engineering (CKEPE) approach,
which uses Large Language Models (LLMs) to exploit external expert-verified
clinical knowledge databases, generating more descriptive prompts and reducing
hallucinations in LLM-generated content to boost zero-shot classification.
Based on MERL, we perform the first benchmark across six public ECG datasets,
showing the superior performance of MERL compared against eSSL methods.
Notably, MERL achieves an average AUC score of 75.2% in zero-shot
classification (without training data), 3.2% higher than linear probed eSSL
methods with 10\% annotated training data, averaged across all six datasets.",0
Towards Zero-Shot Interpretable Human Recognition: A 2D-3D Registration Framework,2403.06658v1,http://arxiv.org/abs/2403.06658v1,2024-03-11 12:27:20+00:00,"Large vision models based in deep learning architectures have been
consistently advancing the state-of-the-art in biometric recognition. However,
three weaknesses are commonly reported for such kind of approaches: 1) their
extreme demands in terms of learning data; 2) the difficulties in generalising
between different domains; and 3) the lack of interpretability/explainability,
with biometrics being of particular interest, as it is important to provide
evidence able to be used for forensics/legal purposes (e.g., in courts). To the
best of our knowledge, this paper describes the first recognition
framework/strategy that aims at addressing the three weaknesses simultaneously.
At first, it relies exclusively in synthetic samples for learning purposes.
Instead of requiring a large amount and variety of samples for each subject,
the idea is to exclusively enroll a 3D point cloud per identity. Then, using
generative strategies, we synthesize a very large (potentially infinite) number
of samples, containing all the desired covariates (poses, clothing, distances,
perspectives, lighting, occlusions,...). Upon the synthesizing method used, it
is possible to adapt precisely to different kind of domains, which accounts for
generalization purposes. Such data are then used to learn a model that performs
local registration between image pairs, establishing positive correspondences
between body parts that are the key, not only to recognition (according to
cardinality and distribution), but also to provide an interpretable description
of the response (e.g.: ""both samples are from the same person, as they have
similar facial shape, hair color and legs thickness"").",0
UAV-Enabled Asynchronous Federated Learning,2403.06653v1,http://arxiv.org/abs/2403.06653v1,2024-03-11 12:19:11+00:00,"To exploit unprecedented data generation in mobile edge networks, federated
learning (FL) has emerged as a promising alternative to the conventional
centralized machine learning (ML).
  However, there are some critical challenges for FL deployment.
  One major challenge called straggler issue severely limits FL's coverage
where the device with the weakest channel condition becomes the bottleneck of
the model aggregation performance.
  Besides, the huge uplink communication overhead compromises the effectiveness
of FL, which is particularly pronounced in large-scale systems.
  To address the straggler issue, we propose the integration of an unmanned
aerial vehicle (UAV) as the parameter server (UAV-PS) to coordinate the FL
implementation.
  We further employ over-the-air computation technique that leverages the
superposition property of wireless channels for efficient uplink communication.
  Specifically, in this paper, we develop a novel UAV-enabled over-the-air
asynchronous FL (UAV-AFL) framework which supports the UAV-PS in updating the
model continuously to enhance the learning performance. Moreover, we conduct a
convergence analysis to quantitatively capture the impact of model asynchrony,
device selection and communication errors on the UAV-AFL learning performance.
Based on this, a unified communication-learning problem is formulated to
maximize asymptotical learning performance by optimizing the UAV-PS trajectory,
device selection and over-the-air transceiver design. Simulation results
demonstrate that the proposed scheme achieves substantially learning efficiency
improvement compared with the state-of-the-art approaches.",0
Mitigating Biases in Collective Decision-Making: Enhancing Performance in the Face of Fake News,2403.08829v1,http://arxiv.org/abs/2403.08829v1,2024-03-11 12:08:08+00:00,"Individual and social biases undermine the effectiveness of human advisers by
inducing judgment errors which can disadvantage protected groups. In this
paper, we study the influence these biases can have in the pervasive problem of
fake news by evaluating human participants' capacity to identify false
headlines. By focusing on headlines involving sensitive characteristics, we
gather a comprehensive dataset to explore how human responses are shaped by
their biases. Our analysis reveals recurring individual biases and their
permeation into collective decisions. We show that demographic factors,
headline categories, and the manner in which information is presented
significantly influence errors in human judgment. We then use our collected
data as a benchmark problem on which we evaluate the efficacy of adaptive
aggregation algorithms. In addition to their improved accuracy, our results
highlight the interactions between the emergence of collective intelligence and
the mitigation of participant biases.",0
Ricci flow-based brain surface covariance descriptors for Alzheimer disease,2403.06645v1,http://arxiv.org/abs/2403.06645v1,2024-03-11 12:07:33+00:00,"Automated feature extraction from MRI brain scans and diagnosis of
Alzheimer's disease are ongoing challenges. With advances in 3D imaging
technology, 3D data acquisition is becoming more viable and efficient than its
2D counterpart. Rather than using feature-based vectors, in this paper, for the
first time, we suggest a pipeline to extract novel covariance-based descriptors
from the cortical surface using the Ricci energy optimization. The covariance
descriptors are components of the nonlinear manifold of symmetric
positive-definite matrices, thus we focus on using the Gaussian radial basis
function to apply manifold-based classification to the 3D shape problem.
Applying this novel signature to the analysis of abnormal cortical brain
morphometry allows for diagnosing Alzheimer's disease. Experimental studies
performed on about two hundred 3D MRI brain models, gathered from Alzheimer's
Disease Neuroimaging Initiative (ADNI) dataset demonstrate the effectiveness of
our descriptors in achieving remarkable classification accuracy.",0
Elephants Never Forget: Testing Language Models for Memorization of Tabular Data,2403.06644v1,http://arxiv.org/abs/2403.06644v1,2024-03-11 12:07:13+00:00,"While many have shown how Large Language Models (LLMs) can be applied to a
diverse set of tasks, the critical issues of data contamination and
memorization are often glossed over. In this work, we address this concern for
tabular data. Starting with simple qualitative tests for whether an LLM knows
the names and values of features, we introduce a variety of different
techniques to assess the degrees of contamination, including statistical tests
for conditional distribution modeling and four tests that identify
memorization. Our investigation reveals that LLMs are pre-trained on many
popular tabular datasets. This exposure can lead to invalid performance
evaluation on downstream tasks because the LLMs have, in effect, been fit to
the test set. Interestingly, we also identify a regime where the language model
reproduces important statistics of the data, but fails to reproduce the dataset
verbatim. On these datasets, although seen during training, good performance on
downstream tasks might not be due to overfitting. Our findings underscore the
need for ensuring data integrity in machine learning tasks with LLMs. To
facilitate future research, we release an open-source tool that can perform
various tests for memorization
\url{https://github.com/interpretml/LLM-Tabular-Memorization-Checker}.",0
Spatial features of CO2 for occupancy detection in a naturally ventilated school building,2403.06643v1,http://arxiv.org/abs/2403.06643v1,2024-03-11 12:04:28+00:00,"Accurate occupancy information helps to improve building energy efficiency
and occupant comfort. Occupancy detection methods based on CO2 sensors have
received attention due to their low cost and low intrusiveness. In naturally
ventilated buildings, the accuracy of CO2-based occupancy detection is
generally low in related studies due to the complex ventilation behavior and
the difficulty in measuring the actual air exchange through windows. In this
study, we present two novel features for occupancy detection based on the
spatial distribution of the CO2 concentration. After a quantitative analysis
with Support Vector Machine (SVM) as classifier, it was found that the accuracy
of occupancy state detection in naturally ventilated rooms could be improved by
up to 14.8 percentage points compared to the baseline, reaching 83.2 % (F1
score 0.84) without any ventilation information. With ventilation information,
the accuracy reached 87.6 % (F1 score 0.89). The performance of occupancy
quantity detection was significantly improved by up to 25.3 percentage points
versus baseline, reaching 56 %, with root mean square error (RMSE) of 11.44
occupants, using only CO2-related features. Additional ventilation information
further enhanced the performance to 61.8 % (RMSE 9.02 occupants). By
incorporating spatial features, the model using only CO2-related features
revealed similar performance as the model containing additional ventilation
information, resulting in a better low-cost occupancy detection method for
naturally ventilated buildings.",0
KELLMRec: Knowledge-Enhanced Large Language Models for Recommendation,2403.06642v1,http://arxiv.org/abs/2403.06642v1,2024-03-11 12:04:20+00:00,"The utilization of semantic information is an important research problem in
the field of recommender systems, which aims to complement the missing parts of
mainstream ID-based approaches. With the rise of LLM, its ability to act as a
knowledge base and its reasoning capability have opened up new possibilities
for this research area, making LLM-based recommendation an emerging research
direction. However, directly using LLM to process semantic information for
recommendation scenarios is unreliable and sub-optimal due to several problems
such as hallucination. A promising way to cope with this is to use external
knowledge to aid LLM in generating truthful and usable text. Inspired by the
above motivation, we propose a Knowledge-Enhanced LLMRec method. In addition to
using external knowledge in prompts, the proposed method also includes a
knowledge-based contrastive learning scheme for training. Experiments on public
datasets and in-enterprise datasets validate the effectiveness of the proposed
method.",0
Evaluating the Energy Efficiency of Few-Shot Learning for Object Detection in Industrial Settings,2403.06631v1,http://arxiv.org/abs/2403.06631v1,2024-03-11 11:41:30+00:00,"In the ever-evolving era of Artificial Intelligence (AI), model performance
has constituted a key metric driving innovation, leading to an exponential
growth in model size and complexity. However, sustainability and energy
efficiency have been critical requirements during deployment in contemporary
industrial settings, necessitating the use of data-efficient approaches such as
few-shot learning. In this paper, to alleviate the burden of lengthy model
training and minimize energy consumption, a finetuning approach to adapt
standard object detection models to downstream tasks is examined. Subsequently,
a thorough case study and evaluation of the energy demands of the developed
models, applied in object detection benchmark datasets from volatile industrial
environments is presented. Specifically, different finetuning strategies as
well as utilization of ancillary evaluation data during training are examined,
and the trade-off between performance and efficiency is highlighted in this
low-data regime. Finally, this paper introduces a novel way to quantify this
trade-off through a customized Efficiency Factor metric.",0
Forest Inspection Dataset for Aerial Semantic Segmentation and Depth Estimation,2403.06621v1,http://arxiv.org/abs/2403.06621v1,2024-03-11 11:26:44+00:00,"Humans use UAVs to monitor changes in forest environments since they are
lightweight and provide a large variety of surveillance data. However, their
information does not present enough details for understanding the scene which
is needed to assess the degree of deforestation. Deep learning algorithms must
be trained on large amounts of data to output accurate interpretations, but
ground truth recordings of annotated forest imagery are not available. To solve
this problem, we introduce a new large aerial dataset for forest inspection
which contains both real-world and virtual recordings of natural environments,
with densely annotated semantic segmentation labels and depth maps, taken in
different illumination conditions, at various altitudes and recording angles.
We test the performance of two multi-scale neural networks for solving the
semantic segmentation task (HRNet and PointFlow network), studying the impact
of the various acquisition conditions and the capabilities of transfer learning
from virtual to real data. Our results showcase that the best results are
obtained when the training is done on a dataset containing a large variety of
scenarios, rather than separating the data into specific categories. We also
develop a framework to assess the deforestation degree of an area.",0
Density-Guided Label Smoothing for Temporal Localization of Driving Actions,2403.06616v1,http://arxiv.org/abs/2403.06616v1,2024-03-11 11:06:41+00:00,"Temporal localization of driving actions plays a crucial role in advanced
driver-assistance systems and naturalistic driving studies. However, this is a
challenging task due to strict requirements for robustness, reliability and
accurate localization. In this work, we focus on improving the overall
performance by efficiently utilizing video action recognition networks and
adapting these to the problem of action localization. To this end, we first
develop a density-guided label smoothing technique based on label probability
distributions to facilitate better learning from boundary video-segments that
typically include multiple labels. Second, we design a post-processing step to
efficiently fuse information from video-segments and multiple camera views into
scene-level predictions, which facilitates elimination of false positives. Our
methodology yields a competitive performance on the A2 test set of the
naturalistic driving action recognition track of the 2022 NVIDIA AI City
Challenge with an F1 score of 0.271.",0
Pulling back symmetric Riemannian geometry for data analysis,2403.06612v1,http://arxiv.org/abs/2403.06612v1,2024-03-11 10:59:55+00:00,"Data sets tend to live in low-dimensional non-linear subspaces. Ideal data
analysis tools for such data sets should therefore account for such non-linear
geometry. The symmetric Riemannian geometry setting can be suitable for a
variety of reasons. First, it comes with a rich mathematical structure to
account for a wide range of non-linear geometries that has been shown to be
able to capture the data geometry through empirical evidence from classical
non-linear embedding. Second, many standard data analysis tools initially
developed for data in Euclidean space can also be generalised efficiently to
data on a symmetric Riemannian manifold. A conceptual challenge comes from the
lack of guidelines for constructing a symmetric Riemannian structure on the
data space itself and the lack of guidelines for modifying successful
algorithms on symmetric Riemannian manifolds for data analysis to this setting.
This work considers these challenges in the setting of pullback Riemannian
geometry through a diffeomorphism. The first part of the paper characterises
diffeomorphisms that result in proper, stable and efficient data analysis. The
second part then uses these best practices to guide construction of such
diffeomorphisms through deep learning. As a proof of concept, different types
of pullback geometries -- among which the proposed construction -- are tested
on several data analysis tasks and on several toy data sets. The numerical
experiments confirm the predictions from theory, i.e., that the diffeomorphisms
generating the pullback geometry need to map the data manifold into a geodesic
subspace of the pulled back Riemannian manifold while preserving local isometry
around the data manifold for proper, stable and efficient data analysis, and
that pulling back positive curvature can be problematic in terms of stability.",0
Distributionally Generative Augmentation for Fair Facial Attribute Classification,2403.06606v1,http://arxiv.org/abs/2403.06606v1,2024-03-11 10:50:53+00:00,"Facial Attribute Classification (FAC) holds substantial promise in widespread
applications. However, FAC models trained by traditional methodologies can be
unfair by exhibiting accuracy inconsistencies across varied data
subpopulations. This unfairness is largely attributed to bias in data, where
some spurious attributes (e.g., Male) statistically correlate with the target
attribute (e.g., Smiling). Most of existing fairness-aware methods rely on the
labels of spurious attributes, which may be unavailable in practice. This work
proposes a novel, generation-based two-stage framework to train a fair FAC
model on biased data without additional annotation. Initially, we identify the
potential spurious attributes based on generative models. Notably, it enhances
interpretability by explicitly showing the spurious attributes in image space.
Following this, for each image, we first edit the spurious attributes with a
random degree sampled from a uniform distribution, while keeping target
attribute unchanged. Then we train a fair FAC model by fostering model
invariance to these augmentation. Extensive experiments on three common
datasets demonstrate the effectiveness of our method in promoting fairness in
FAC without compromising accuracy. Codes are in
https://github.com/heqianpei/DiGA.",0
Cross-domain and Cross-dimension Learning for Image-to-Graph Transformers,2403.06601v1,http://arxiv.org/abs/2403.06601v1,2024-03-11 10:48:56+00:00,"Direct image-to-graph transformation is a challenging task that solves object
detection and relationship prediction in a single model. Due to the complexity
of this task, large training datasets are rare in many domains, which makes the
training of large networks challenging. This data sparsity necessitates the
establishment of pre-training strategies akin to the state-of-the-art in
computer vision. In this work, we introduce a set of methods enabling
cross-domain and cross-dimension transfer learning for image-to-graph
transformers. We propose (1) a regularized edge sampling loss for sampling the
optimal number of object relationships (edges) across domains, (2) a domain
adaptation framework for image-to-graph transformers that aligns features from
different domains, and (3) a simple projection function that allows us to
pretrain 3D transformers on 2D input data. We demonstrate our method's utility
in cross-domain and cross-dimension experiments, where we pretrain our models
on 2D satellite images before applying them to vastly different target domains
in 2D and 3D. Our method consistently outperforms a series of baselines on
challenging benchmarks, such as retinal or whole-brain vessel graph extraction.",0
BEV2PR: BEV-Enhanced Visual Place Recognition with Structural Cues,2403.06600v1,http://arxiv.org/abs/2403.06600v1,2024-03-11 10:46:43+00:00,"In this paper, we propose a new image-based visual place recognition (VPR)
framework by exploiting the structural cues in bird's-eye view (BEV) from a
single monocular camera. The motivation arises from two key observations about
VPR: 1) For the methods based on both camera and LiDAR sensors, the integration
of LiDAR in robotic systems has led to increased expenses, while the alignment
of data between different sensors is also a major challenge. 2) Other
image-/camera-based methods, involving integrating RGB images and their derived
variants (e.g., pseudo depth images, pseudo 3D point clouds), exhibit several
limitations, such as the failure to effectively exploit the explicit spatial
relationships between different objects. To tackle the above issues, we design
a new BEV-enhanced VPR framework, nemely BEV2PR, which can generate a composite
descriptor with both visual cues and spatial awareness solely based on a single
camera. For the visual cues, any popular aggregation module for RGB global
features can be integrated into our framework. The key points lie in: 1) We use
BEV segmentation features as an explicit source of structural knowledge in
constructing global features. 2) The lower layers of the pre-trained backbone
from BEV map generation are shared for visual and structural streams in VPR,
facilitating the learning of fine-grained local features in the visual stream.
3) The complementary visual features and structural features can jointly
enhance VPR performance. Our BEV2PR framework enables consistent performance
improvements over several popular camera-based VPR aggregation modules when
integrating them. The experiments on our collected VPR-NuScenes dataset
demonstrate an absolute gain of 2.47% on Recall@1 for the strong Conv-AP
baseline to achieve the best performance in our setting, and notably, a 18.06%
gain on the hard set.",0
Exploiting Style Latent Flows for Generalizing Deepfake Detection Video Detection,2403.06592v1,http://arxiv.org/abs/2403.06592v1,2024-03-11 10:35:58+00:00,"This paper presents a new approach for the detection of fake videos, based on
the analysis of style latent vectors and their abnormal behavior in temporal
changes in the generated videos. We discovered that the generated facial videos
suffer from the temporal distinctiveness in the temporal changes of style
latent vectors, which are inevitable during the generation of temporally stable
videos with various facial expressions and geometric transformations. Our
framework utilizes the StyleGRU module, trained by contrastive learning, to
represent the dynamic properties of style latent vectors. Additionally, we
introduce a style attention module that integrates StyleGRU-generated features
with content-based features, enabling the detection of visual and temporal
artifacts. We demonstrate our approach across various benchmark scenarios in
deepfake detection, showing its superiority in cross-dataset and
cross-manipulation scenarios. Through further analysis, we also validate the
importance of using temporal changes of style latent vectors to improve the
generality of deepfake video detection.",0
ContextGPT: Infusing LLMs Knowledge into Neuro-Symbolic Activity Recognition Models,2403.06586v1,http://arxiv.org/abs/2403.06586v1,2024-03-11 10:32:23+00:00,"Context-aware Human Activity Recognition (HAR) is a hot research area in
mobile computing, and the most effective solutions in the literature are based
on supervised deep learning models. However, the actual deployment of these
systems is limited by the scarcity of labeled data that is required for
training. Neuro-Symbolic AI (NeSy) provides an interesting research direction
to mitigate this issue, by infusing common-sense knowledge about human
activities and the contexts in which they can be performed into HAR deep
learning classifiers. Existing NeSy methods for context-aware HAR rely on
knowledge encoded in logic-based models (e.g., ontologies) whose design,
implementation, and maintenance to capture new activities and contexts require
significant human engineering efforts, technical knowledge, and domain
expertise. Recent works show that pre-trained Large Language Models (LLMs)
effectively encode common-sense knowledge about human activities. In this work,
we propose ContextGPT: a novel prompt engineering approach to retrieve from
LLMs common-sense knowledge about the relationship between human activities and
the context in which they are performed. Unlike ontologies, ContextGPT requires
limited human effort and expertise. An extensive evaluation carried out on two
public datasets shows how a NeSy model obtained by infusing common-sense
knowledge from ContextGPT is effective in data scarcity scenarios, leading to
similar (and sometimes better) recognition rates than logic-based approaches
with a fraction of the effort.",0
Data Poisoning Attacks in Gossip Learning,2403.06583v1,http://arxiv.org/abs/2403.06583v1,2024-03-11 10:29:50+00:00,"Traditional machine learning systems were designed in a centralized manner.
In such designs, the central entity maintains both the machine learning model
and the data used to adjust the model's parameters. As data centralization
yields privacy issues, Federated Learning was introduced to reduce data sharing
and have a central server coordinate the learning of multiple devices. While
Federated Learning is more decentralized, it still relies on a central entity
that may fail or be subject to attacks, provoking the failure of the whole
system. Then, Decentralized Federated Learning removes the need for a central
server entirely, letting participating processes handle the coordination of the
model construction. This distributed control urges studying the possibility of
malicious attacks by the participants themselves. While poisoning attacks on
Federated Learning have been extensively studied, their effects in
Decentralized Federated Learning did not get the same level of attention. Our
work is the first to propose a methodology to assess poisoning attacks in
Decentralized Federated Learning in both churn free and churn prone scenarios.
Furthermore, in order to evaluate our methodology on a case study
representative for gossip learning we extended the gossipy simulator with an
attack injector module.",0
DNNShield: Embedding Identifiers for Deep Neural Network Ownership Verification,2403.06581v1,http://arxiv.org/abs/2403.06581v1,2024-03-11 10:27:36+00:00,"The surge in popularity of machine learning (ML) has driven significant
investments in training Deep Neural Networks (DNNs). However, these models that
require resource-intensive training are vulnerable to theft and unauthorized
use. This paper addresses this challenge by introducing DNNShield, a novel
approach for DNN protection that integrates seamlessly before training.
DNNShield embeds unique identifiers within the model architecture using
specialized protection layers. These layers enable secure training and
deployment while offering high resilience against various attacks, including
fine-tuning, pruning, and adaptive adversarial attacks. Notably, our approach
achieves this security with minimal performance and computational overhead
(less than 5\% runtime increase). We validate the effectiveness and efficiency
of DNNShield through extensive evaluations across three datasets and four model
architectures. This practical solution empowers developers to protect their
DNNs and intellectual property rights.",0
FFAD: A Novel Metric for Assessing Generated Time Series Data Utilizing Fourier Transform and Auto-encoder,2403.06576v1,http://arxiv.org/abs/2403.06576v1,2024-03-11 10:26:04+00:00,"The success of deep learning-based generative models in producing realistic
images, videos, and audios has led to a crucial consideration: how to
effectively assess the quality of synthetic samples. While the Fr\'{e}chet
Inception Distance (FID) serves as the standard metric for evaluating
generative models in image synthesis, a comparable metric for time series data
is notably absent. This gap in assessment capabilities stems from the absence
of a widely accepted feature vector extractor pre-trained on benchmark time
series datasets. In addressing these challenges related to assessing the
quality of time series, particularly in the context of Fr\'echet Distance, this
work proposes a novel solution leveraging the Fourier transform and
Auto-encoder, termed the Fr\'{e}chet Fourier-transform Auto-encoder Distance
(FFAD). Through our experimental results, we showcase the potential of FFAD for
effectively distinguishing samples from different classes. This novel metric
emerges as a fundamental tool for the evaluation of generative time series
data, contributing to the ongoing efforts of enhancing assessment methodologies
in the realm of deep learning-based generative models.",0
Lander.AI: Adaptive Landing Behavior Agent for Expertise in 3D Dynamic Platform Landings,2403.06572v2,http://arxiv.org/abs/2403.06572v2,2024-03-11 10:20:44+00:00,"Mastering autonomous drone landing on dynamic platforms presents formidable
challenges due to unpredictable velocities and external disturbances caused by
the wind, ground effect, turbines or propellers of the docking platform. This
study introduces an advanced Deep Reinforcement Learning (DRL) agent,
Lander:AI, designed to navigate and land on platforms in the presence of windy
conditions, thereby enhancing drone autonomy and safety. Lander:AI is
rigorously trained within the gym-pybullet-drone simulation, an environment
that mirrors real-world complexities, including wind turbulence, to ensure the
agent's robustness and adaptability.
  The agent's capabilities were empirically validated with Crazyflie 2.1 drones
across various test scenarios, encompassing both simulated environments and
real-world conditions. The experimental results showcased Lander:AI's
high-precision landing and its ability to adapt to moving platforms, even under
wind-induced disturbances. Furthermore, the system performance was benchmarked
against a baseline PID controller augmented with an Extended Kalman Filter,
illustrating significant improvements in landing precision and error recovery.
Lander:AI leverages bio-inspired learning to adapt to external forces like
birds, enhancing drone adaptability without knowing force magnitudes.This
research not only advances drone landing technologies, essential for inspection
and emergency applications, but also highlights the potential of DRL in
addressing intricate aerodynamic challenges.",0
Scalable Online Exploration via Coverability,2403.06571v1,http://arxiv.org/abs/2403.06571v1,2024-03-11 10:14:06+00:00,"Exploration is a major challenge in reinforcement learning, especially for
high-dimensional domains that require function approximation. We propose
exploration objectives -- policy optimization objectives that enable downstream
maximization of any reward function -- as a conceptual framework to systematize
the study of exploration. Within this framework, we introduce a new objective,
$L_1$-Coverage, which generalizes previous exploration schemes and supports
three fundamental desiderata:
  1. Intrinsic complexity control. $L_1$-Coverage is associated with a
structural parameter, $L_1$-Coverability, which reflects the intrinsic
statistical difficulty of the underlying MDP, subsuming Block and Low-Rank
MDPs.
  2. Efficient planning. For a known MDP, optimizing $L_1$-Coverage efficiently
reduces to standard policy optimization, allowing flexible integration with
off-the-shelf methods such as policy gradient and Q-learning approaches.
  3. Efficient exploration. $L_1$-Coverage enables the first computationally
efficient model-based and model-free algorithms for online (reward-free or
reward-driven) reinforcement learning in MDPs with low coverability.
  Empirically, we find that $L_1$-Coverage effectively drives off-the-shelf
policy optimization algorithms to explore the state space.",0
Enhancing Joint Motion Prediction for Individuals with Limb Loss Through Model Reprogramming,2403.06569v2,http://arxiv.org/abs/2403.06569v2,2024-03-11 10:10:45+00:00,"Mobility impairment caused by limb loss is a significant challenge faced by
millions of individuals worldwide. The development of advanced assistive
technologies, such as prosthetic devices, has the potential to greatly improve
the quality of life for amputee patients. A critical component in the design of
such technologies is the accurate prediction of reference joint motion for the
missing limb. However, this task is hindered by the scarcity of joint motion
data available for amputee patients, in contrast to the substantial quantity of
data from able-bodied subjects. To overcome this, we leverage deep learning's
reprogramming property to repurpose well-trained models for a new goal without
altering the model parameters. With only data-level manipulation, we adapt
models originally designed for able-bodied people to forecast joint motion in
amputees. The findings in this study have significant implications for
advancing assistive tech and amputee mobility.",0
Unraveling the Mystery of Scaling Laws: Part I,2403.06563v1,http://arxiv.org/abs/2403.06563v1,2024-03-11 10:05:29+00:00,"Scaling law principles indicate a power-law correlation between loss and
variables such as model size, dataset size, and computational resources
utilized during training. These principles play a vital role in optimizing
various aspects of model pre-training, ultimately contributing to the success
of large language models such as GPT-4, Llama and Gemini. However, the original
scaling law paper by OpenAI did not disclose the complete details necessary to
derive the precise scaling law formulas, and their conclusions are only based
on models containing up to 1.5 billion parameters. Though some subsequent works
attempt to unveil these details and scale to larger models, they often neglect
the training dependency of important factors such as the learning rate, context
length and batch size, leading to their failure to establish a reliable formula
for predicting the test loss trajectory. In this technical report, we confirm
that the scaling law formulations proposed in the original OpenAI paper remain
valid when scaling the model size up to 33 billion, but the constant
coefficients in these formulas vary significantly with the experiment setup. We
meticulously identify influential factors and provide transparent, step-by-step
instructions to estimate all constant terms in scaling-law formulas by training
on models with only 1M~60M parameters. Using these estimated formulas, we
showcase the capability to accurately predict various attributes for models
with up to 33B parameters before their training, including (1) the minimum
possible test loss; (2) the minimum required training steps and processed
tokens to achieve a specific loss; (3) the critical batch size with an optimal
time/computation trade-off at any loss value; and (4) the complete test loss
trajectory with arbitrary batch size.",0
Sliced-Wasserstein Distances and Flows on Cartan-Hadamard Manifolds,2403.06560v1,http://arxiv.org/abs/2403.06560v1,2024-03-11 10:01:21+00:00,"While many Machine Learning methods were developed or transposed on
Riemannian manifolds to tackle data with known non Euclidean geometry, Optimal
Transport (OT) methods on such spaces have not received much attention. The
main OT tool on these spaces is the Wasserstein distance which suffers from a
heavy computational burden. On Euclidean spaces, a popular alternative is the
Sliced-Wasserstein distance, which leverages a closed-form solution of the
Wasserstein distance in one dimension, but which is not readily available on
manifolds. In this work, we derive general constructions of Sliced-Wasserstein
distances on Cartan-Hadamard manifolds, Riemannian manifolds with non-positive
curvature, which include among others Hyperbolic spaces or the space of
Symmetric Positive Definite matrices. Then, we propose different applications.
Additionally, we derive non-parametric schemes to minimize these new distances
by approximating their Wasserstein gradient flows.",0
Robust Synthetic-to-Real Transfer for Stereo Matching,2403.07705v1,http://arxiv.org/abs/2403.07705v1,2024-03-12 14:50:05+00:00,"With advancements in domain generalized stereo matching networks, models
pre-trained on synthetic data demonstrate strong robustness to unseen domains.
However, few studies have investigated the robustness after fine-tuning them in
real-world scenarios, during which the domain generalization ability can be
seriously degraded. In this paper, we explore fine-tuning stereo matching
networks without compromising their robustness to unseen domains. Our
motivation stems from comparing Ground Truth (GT) versus Pseudo Label (PL) for
fine-tuning: GT degrades, but PL preserves the domain generalization ability.
Empirically, we find the difference between GT and PL implies valuable
information that can regularize networks during fine-tuning. We also propose a
framework to utilize this difference for fine-tuning, consisting of a frozen
Teacher, an exponential moving average (EMA) Teacher, and a Student network.
The core idea is to utilize the EMA Teacher to measure what the Student has
learned and dynamically improve GT and PL for fine-tuning. We integrate our
framework with state-of-the-art networks and evaluate its effectiveness on
several real-world datasets. Extensive experiments show that our method
effectively preserves the domain generalization ability during fine-tuning.",0
Fast and Simple Explainability for Point Cloud Networks,2403.07706v1,http://arxiv.org/abs/2403.07706v1,2024-03-12 14:51:23+00:00,"We propose a fast and simple explainable AI (XAI) method for point cloud
data. It computes pointwise importance with respect to a trained network
downstream task. This allows better understanding of the network properties,
which is imperative for safety-critical applications. In addition to debugging
and visualization, our low computational complexity facilitates online feedback
to the network at inference. This can be used to reduce uncertainty and to
increase robustness. In this work, we introduce \emph{Feature Based
Interpretability} (FBI), where we compute the features' norm, per point, before
the bottleneck. We analyze the use of gradients and post- and pre-bottleneck
strategies, showing pre-bottleneck is preferred, in terms of smoothness and
ranking. We obtain at least three orders of magnitude speedup, compared to
current XAI methods, thus, scalable for big point clouds or large-scale
architectures. Our approach achieves SOTA results, in terms of classification
explainability. We demonstrate how the proposed measure is helpful in analyzing
and characterizing various aspects of 3D learning, such as rotation invariance,
robustness to out-of-distribution (OOD) outliers or domain shift and dataset
bias.",0
Improving Reinforcement Learning from Human Feedback Using Contrastive Rewards,2403.07708v2,http://arxiv.org/abs/2403.07708v2,2024-03-12 14:51:57+00:00,"Reinforcement learning from human feedback (RLHF) is the mainstream paradigm
used to align large language models (LLMs) with human preferences. Yet existing
RLHF heavily relies on accurate and informative reward models, which are
vulnerable and sensitive to noise from various sources, e.g. human labeling
errors, making the pipeline fragile. In this work, we improve the effectiveness
of the reward model by introducing a penalty term on the reward, named as
\textit{contrastive rewards}. %Contrastive rewards Our approach involves two
steps: (1) an offline sampling step to obtain responses to prompts that serve
as baseline calculation and (2) a contrastive reward calculated using the
baseline responses and used in the Proximal Policy Optimization (PPO) step. We
show that contrastive rewards enable the LLM to penalize reward uncertainty,
improve robustness, encourage improvement over baselines, calibrate according
to task difficulty, and reduce variance in PPO. We show empirically contrastive
rewards can improve RLHF substantially, evaluated by both GPTs and humans, and
our method consistently outperforms strong baselines.",0
Hyperparameters in Continual Learning: a Reality Check,2403.09066v1,http://arxiv.org/abs/2403.09066v1,2024-03-14 03:13:01+00:00,"Various algorithms for continual learning (CL) have been designed with the
goal of effectively alleviating the trade-off between stability and plasticity
during the CL process. To achieve this goal, tuning appropriate hyperparameters
for each algorithm is essential. As an evaluation protocol, it has been common
practice to train a CL algorithm using diverse hyperparameter values on a CL
scenario constructed with a benchmark dataset. Subsequently, the best
performance attained with the optimal hyperparameter value serves as the
criterion for evaluating the CL algorithm. In this paper, we contend that this
evaluation protocol is not only impractical but also incapable of effectively
assessing the CL capability of a CL algorithm. Returning to the fundamental
principles of model evaluation in machine learning, we propose an evaluation
protocol that involves Hyperparameter Tuning and Evaluation phases. Those
phases consist of different datasets but share the same CL scenario. In the
Hyperparameter Tuning phase, each algorithm is iteratively trained with
different hyperparameter values to find the optimal hyperparameter values.
Subsequently, in the Evaluation phase, the optimal hyperparameter values is
directly applied for training each algorithm, and their performance in the
Evaluation phase serves as the criterion for evaluating them. Through
experiments on CIFAR-100 and ImageNet-100 based on the proposed protocol in
class-incremental learning, we not only observed that the existing evaluation
method fail to properly assess the CL capability of each algorithm but also
observe that some recently proposed state-of-the-art algorithms, which reported
superior performance, actually exhibit inferior performance compared to the
previous algorithm.",0
CardioCaps: Attention-based Capsule Network for Class-Imbalanced Echocardiogram Classification,2403.09108v1,http://arxiv.org/abs/2403.09108v1,2024-03-14 05:01:31+00:00,"Capsule Neural Networks (CapsNets) is a novel architecture that utilizes
vector-wise representations formed by multiple neurons. Specifically, the
Dynamic Routing CapsNets (DR-CapsNets) employ an affine matrix and dynamic
routing mechanism to train capsules and acquire translation-equivariance
properties, enhancing its robustness compared to traditional Convolutional
Neural Networks (CNNs). Echocardiograms, which capture moving images of the
heart, present unique challenges for traditional image classification methods.
In this paper, we explore the potential of DR-CapsNets and propose CardioCaps,
a novel attention-based DR-CapsNet architecture for class-imbalanced
echocardiogram classification. CardioCaps comprises two key components: a
weighted margin loss incorporating a regression auxiliary loss and an attention
mechanism. First, the weighted margin loss prioritizes positive cases,
supplemented by an auxiliary loss function based on the Ejection Fraction (EF)
regression task, a crucial measure of cardiac function. This approach enhances
the model's resilience in the face of class imbalance. Second, recognizing the
quadratic complexity of dynamic routing leading to training inefficiencies, we
adopt the attention mechanism as a more computationally efficient alternative.
Our results demonstrate that CardioCaps surpasses traditional machine learning
baseline methods, including Logistic Regression, Random Forest, and XGBoost
with sampling methods and a class weight matrix. Furthermore, CardioCaps
outperforms other deep learning baseline methods such as CNNs, ResNets, U-Nets,
and ViTs, as well as advanced CapsNets methods such as EM-CapsNets and
Efficient-CapsNets. Notably, our model demonstrates robustness to class
imbalance, achieving high precision even in datasets with a substantial
proportion of negative cases.",0
S^2MVTC: a Simple yet Efficient Scalable Multi-View Tensor Clustering,2403.09107v1,http://arxiv.org/abs/2403.09107v1,2024-03-14 05:00:29+00:00,"Anchor-based large-scale multi-view clustering has attracted considerable
attention for its effectiveness in handling massive datasets. However, current
methods mainly seek the consensus embedding feature for clustering by exploring
global correlations between anchor graphs or projection matrices.In this paper,
we propose a simple yet efficient scalable multi-view tensor clustering
(S^2MVTC) approach, where our focus is on learning correlations of embedding
features within and across views. Specifically, we first construct the
embedding feature tensor by stacking the embedding features of different views
into a tensor and rotating it. Additionally, we build a novel tensor
low-frequency approximation (TLFA) operator, which incorporates graph
similarity into embedding feature learning, efficiently achieving smooth
representation of embedding features within different views. Furthermore,
consensus constraints are applied to embedding features to ensure inter-view
semantic consistency. Experimental results on six large-scale multi-view
datasets demonstrate that S^2MVTC significantly outperforms state-of-the-art
algorithms in terms of clustering performance and CPU execution time,
especially when handling massive data. The code of S^2MVTC is publicly
available at https://github.com/longzhen520/S2MVTC.",0
Soften to Defend: Towards Adversarial Robustness via Self-Guided Label Refinement,2403.09101v1,http://arxiv.org/abs/2403.09101v1,2024-03-14 04:48:31+00:00,"Adversarial training (AT) is currently one of the most effective ways to
obtain the robustness of deep neural networks against adversarial attacks.
However, most AT methods suffer from robust overfitting, i.e., a significant
generalization gap in adversarial robustness between the training and testing
curves. In this paper, we first identify a connection between robust
overfitting and the excessive memorization of noisy labels in AT from a view of
gradient norm. As such label noise is mainly caused by a distribution mismatch
and improper label assignments, we are motivated to propose a label refinement
approach for AT. Specifically, our Self-Guided Label Refinement first
self-refines a more accurate and informative label distribution from
over-confident hard labels, and then it calibrates the training by dynamically
incorporating knowledge from self-distilled models into the current model and
thus requiring no external teachers. Empirical results demonstrate that our
method can simultaneously boost the standard accuracy and robust performance
across multiple benchmark datasets, attack types, and architectures. In
addition, we also provide a set of analyses from the perspectives of
information theory to dive into our method and suggest the importance of soft
labels for robust generalization.",0
Virtual birefringence imaging and histological staining of amyloid deposits in label-free tissue using autofluorescence microscopy and deep learning,2403.09100v1,http://arxiv.org/abs/2403.09100v1,2024-03-14 04:48:06+00:00,"Systemic amyloidosis is a group of diseases characterized by the deposition
of misfolded proteins in various organs and tissues, leading to progressive
organ dysfunction and failure. Congo red stain is the gold standard chemical
stain for the visualization of amyloid deposits in tissue sections, as it forms
complexes with the misfolded proteins and shows a birefringence pattern under
polarized light microscopy. However, Congo red staining is tedious and costly
to perform, and prone to false diagnoses due to variations in the amount of
amyloid, staining quality and expert interpretation through manual examination
of tissue under a polarization microscope. Here, we report the first
demonstration of virtual birefringence imaging and virtual Congo red staining
of label-free human tissue to show that a single trained neural network can
rapidly transform autofluorescence images of label-free tissue sections into
brightfield and polarized light microscopy equivalent images, matching the
histochemically stained versions of the same samples. We demonstrate the
efficacy of our method with blind testing and pathologist evaluations on
cardiac tissue where the virtually stained images agreed well with the
histochemically stained ground truth images. Our virtually stained polarization
and brightfield images highlight amyloid birefringence patterns in a
consistent, reproducible manner while mitigating diagnostic challenges due to
variations in the quality of chemical staining and manual imaging processes as
part of the clinical workflow.",0
Deep unfolding Network for Hyperspectral Image Super-Resolution with Automatic Exposure Correction,2403.09096v1,http://arxiv.org/abs/2403.09096v1,2024-03-14 04:41:30+00:00,"In recent years, the fusion of high spatial resolution multispectral image
(HR-MSI) and low spatial resolution hyperspectral image (LR-HSI) has been
recognized as an effective method for HSI super-resolution (HSI-SR). However,
both HSI and MSI may be acquired under extreme conditions such as night or
poorly illuminating scenarios, which may cause different exposure levels,
thereby seriously downgrading the yielded HSISR. In contrast to most existing
methods based on respective low-light enhancements (LLIE) of MSI and HSI
followed by their fusion, a deep Unfolding HSI Super-Resolution with Automatic
Exposure Correction (UHSR-AEC) is proposed, that can effectively generate a
high-quality fused HSI-SR (in texture and features) even under very imbalanced
exposures, thanks to the correlation between LLIE and HSI-SR taken into
account. Extensive experiments are provided to demonstrate the state-of-the-art
overall performance of the proposed UHSR-AEC, including comparison with some
benchmark peer methods.",0
MCFEND: A Multi-source Benchmark Dataset for Chinese Fake News Detection,2403.09092v1,http://arxiv.org/abs/2403.09092v1,2024-03-14 04:32:13+00:00,"The prevalence of fake news across various online sources has had a
significant influence on the public. Existing Chinese fake news detection
datasets are limited to news sourced solely from Weibo. However, fake news
originating from multiple sources exhibits diversity in various aspects,
including its content and social context. Methods trained on purely one single
news source can hardly be applicable to real-world scenarios. Our pilot
experiment demonstrates that the F1 score of the state-of-the-art method that
learns from a large Chinese fake news detection dataset, Weibo-21, drops
significantly from 0.943 to 0.470 when the test data is changed to multi-source
news data, failing to identify more than one-third of the multi-source fake
news. To address this limitation, we constructed the first multi-source
benchmark dataset for Chinese fake news detection, termed MCFEND, which is
composed of news we collected from diverse sources such as social platforms,
messaging apps, and traditional online news outlets. Notably, such news has
been fact-checked by 14 authoritative fact-checking agencies worldwide. In
addition, various existing Chinese fake news detection methods are thoroughly
evaluated on our proposed dataset in cross-source, multi-source, and unseen
source ways. MCFEND, as a benchmark dataset, aims to advance Chinese fake news
detection approaches in real-world scenarios.",0
Dissipative Gradient Descent Ascent Method: A Control Theory Inspired Algorithm for Min-max Optimization,2403.09090v1,http://arxiv.org/abs/2403.09090v1,2024-03-14 04:26:00+00:00,"Gradient Descent Ascent (GDA) methods for min-max optimization problems
typically produce oscillatory behavior that can lead to instability, e.g., in
bilinear settings. To address this problem, we introduce a dissipation term
into the GDA updates to dampen these oscillations. The proposed Dissipative GDA
(DGDA) method can be seen as performing standard GDA on a state-augmented and
regularized saddle function that does not strictly introduce additional
convexity/concavity. We theoretically show the linear convergence of DGDA in
the bilinear and strongly convex-strongly concave settings and assess its
performance by comparing DGDA with other methods such as GDA, Extra-Gradient
(EG), and Optimistic GDA. Our findings demonstrate that DGDA surpasses these
methods, achieving superior convergence rates. We support our claims with two
numerical examples that showcase DGDA's effectiveness in solving saddle point
problems.",0
Learning from straggler clients in federated learning,2403.09086v1,http://arxiv.org/abs/2403.09086v1,2024-03-14 04:06:45+00:00,"How well do existing federated learning algorithms learn from client devices
that return model updates with a significant time delay? Is it even possible to
learn effectively from clients that report back minutes, hours, or days after
being scheduled? We answer these questions by developing Monte Carlo
simulations of client latency that are guided by real-world applications. We
study synchronous optimization algorithms like FedAvg and FedAdam as well as
the asynchronous FedBuff algorithm, and observe that all these existing
approaches struggle to learn from severely delayed clients. To improve upon
this situation, we experiment with modifications, including distillation
regularization and exponential moving averages of model weights. Finally, we
introduce two new algorithms, FARe-DUST and FeAST-on-MSG, based on distillation
and averaging, respectively. Experiments with the EMNIST, CIFAR-100, and
StackOverflow benchmark federated learning tasks demonstrate that our new
algorithms outperform existing ones in terms of accuracy for straggler clients,
while also providing better trade-offs between training time and total
accuracy.",0
Meaningful Learning: Advancing Abstract Reasoning in Large Language Models via Generic Fact Guidance,2403.09085v1,http://arxiv.org/abs/2403.09085v1,2024-03-14 04:06:13+00:00,"Large language models (LLMs) have developed impressive performance and strong
explainability across various reasoning scenarios, marking a significant stride
towards mimicking human-like intelligence. Despite this, when tasked with
simple questions supported by a generic fact, LLMs often fail to provide
consistent and precise answers, indicating a deficiency in abstract reasoning
abilities. This has sparked a vigorous debate about whether LLMs are genuinely
reasoning or merely memorizing. In light of this, we design a preliminary study
to quantify and delve into the abstract reasoning abilities of existing LLMs.
Our findings reveal a substantial discrepancy between their general reasoning
and abstract reasoning performances. To relieve this problem, we tailor an
abstract reasoning dataset (AbsR) together with a meaningful learning paradigm
to teach LLMs how to leverage generic facts for reasoning purposes. The results
show that our approach not only boosts the general reasoning performance of
LLMs but also makes considerable strides towards their capacity for abstract
reasoning, moving beyond simple memorization or imitation to a more nuanced
understanding and application of generic facts.",0
Information Extraction: An application to the domain of hyper-local financial data on developing countries,2403.09077v1,http://arxiv.org/abs/2403.09077v1,2024-03-14 03:49:36+00:00,"Despite the need for financial data on company activities in developing
countries for development research and economic analysis, such data does not
exist. In this project, we develop and evaluate two Natural Language Processing
(NLP) based techniques to address this issue. First, we curate a custom dataset
specific to the domain of financial text data on developing countries and
explore multiple approaches for information extraction. We then explore a
text-to-text approach with the transformer-based T5 model with the goal of
undertaking simultaneous NER and relation extraction. We find that this model
is able to learn the custom text structure output data corresponding to the
entities and their relations, resulting in an accuracy of 92.44\%, a precision
of 68.25\% and a recall of 54.20\% from our best T5 model on the combined task.
Secondly, we explore an approach with sequential NER and relation extration.
For the NER, we run pre-trained and fine-tuned models using SpaCy, and we
develop a custom relation extraction model using SpaCy's Dependency Parser
output and some heuristics to determine entity relationships \cite{spacy}. We
obtain an accuracy of 84.72\%, a precision of 6.06\% and a recall of 5.57\% on
this sequential task.",0
Large Language Models are Parallel Multilingual Learners,2403.09073v1,http://arxiv.org/abs/2403.09073v1,2024-03-14 03:33:46+00:00,"In this study, we reveal an in-context learning (ICL) capability of
multilingual large language models (LLMs): by translating the input to several
languages, we provide Parallel Input in Multiple Languages (PiM) to LLMs, which
significantly enhances their comprehension abilities. To test this capability,
we design extensive experiments encompassing 8 typical datasets, 7 languages
and 8 state-of-the-art multilingual LLMs. Experimental results show that (1)
incorporating more languages help PiM surpass the conventional ICL further; (2)
even combining with the translations that are inferior to baseline performance
can also help. Moreover, by examining the activated neurons in LLMs, we
discover a counterintuitive but interesting phenomenon. Contrary to the common
thought that PiM would activate more neurons than monolingual input to leverage
knowledge learned from diverse languages, PiM actually inhibits neurons and
promotes more precise neuron activation especially when more languages are
added. This phenomenon aligns with the neuroscience insight about synaptic
pruning, which removes less used neural connections, strengthens remainders,
and then enhances brain intelligence.",0
UniCode: Learning a Unified Codebook for Multimodal Large Language Models,2403.09072v1,http://arxiv.org/abs/2403.09072v1,2024-03-14 03:29:58+00:00,"In this paper, we propose \textbf{UniCode}, a novel approach within the
domain of multimodal large language models (MLLMs) that learns a unified
codebook to efficiently tokenize visual, text, and potentially other types of
signals. This innovation addresses a critical limitation in existing MLLMs:
their reliance on a text-only codebook, which restricts MLLM's ability to
generate images and texts in a multimodal context. Towards this end, we propose
a language-driven iterative training paradigm, coupled with an in-context
pre-training task we term ``image decompression'', enabling our model to
interpret compressed visual data and generate high-quality images.The unified
codebook empowers our model to extend visual instruction tuning to
non-linguistic generation tasks. Moreover, UniCode is adaptable to diverse
stacked quantization approaches in order to compress visual signals into a more
compact token representation. Despite using significantly fewer parameters and
less data during training, Unicode demonstrates promising capabilities in
visual reconstruction and generation. It also achieves performances comparable
to leading MLLMs across a spectrum of VQA benchmarks.",0
Dyadic Interaction Modeling for Social Behavior Generation,2403.09069v1,http://arxiv.org/abs/2403.09069v1,2024-03-14 03:21:33+00:00,"Human-human communication is like a delicate dance where listeners and
speakers concurrently interact to maintain conversational dynamics. Hence, an
effective model for generating listener nonverbal behaviors requires
understanding the dyadic context and interaction. In this paper, we present an
effective framework for creating 3D facial motions in dyadic interactions.
Existing work consider a listener as a reactive agent with reflexive behaviors
to the speaker's voice and facial motions. The heart of our framework is Dyadic
Interaction Modeling (DIM), a pre-training approach that jointly models
speakers' and listeners' motions through masking and contrastive learning to
learn representations that capture the dyadic context. To enable the generation
of non-deterministic behaviors, we encode both listener and speaker motions
into discrete latent representations, through VQ-VAE. The pre-trained model is
further fine-tuned for motion generation. Extensive experiments demonstrate the
superiority of our framework in generating listener motions, establishing a new
state-of-the-art according to the quantitative measures capturing the diversity
and realism of generated motions. Qualitative results demonstrate the superior
capabilities of the proposed approach in generating diverse and realistic
expressions, eye blinks and head gestures.",0
TBI Image/Text (TBI-IT): Comprehensive Text and Image Datasets for Traumatic Brain Injury Research,2403.09062v1,http://arxiv.org/abs/2403.09062v1,2024-03-14 03:07:49+00:00,"In this paper, we introduce a new dataset in the medical field of Traumatic
Brain Injury (TBI), called TBI-IT, which includes both electronic medical
records (EMRs) and head CT images. This dataset is designed to enhance the
accuracy of artificial intelligence in the diagnosis and treatment of TBI. This
dataset, built upon the foundation of standard text and image data,
incorporates specific annotations within the EMRs, extracting key content from
the text information, and categorizes the annotation content of imaging data
into five types: brain midline, hematoma, left cerebral ventricle, right
cerebral ventricle and fracture. TBI-IT aims to be a foundational dataset for
feature learning in image segmentation tasks and named entity recognition.",0
"BEHAVIOR-1K: A Human-Centered, Embodied AI Benchmark with 1,000 Everyday Activities and Realistic Simulation",2403.09227v1,http://arxiv.org/abs/2403.09227v1,2024-03-14 09:48:36+00:00,"We present BEHAVIOR-1K, a comprehensive simulation benchmark for
human-centered robotics. BEHAVIOR-1K includes two components, guided and
motivated by the results of an extensive survey on ""what do you want robots to
do for you?"". The first is the definition of 1,000 everyday activities,
grounded in 50 scenes (houses, gardens, restaurants, offices, etc.) with more
than 9,000 objects annotated with rich physical and semantic properties. The
second is OMNIGIBSON, a novel simulation environment that supports these
activities via realistic physics simulation and rendering of rigid bodies,
deformable bodies, and liquids. Our experiments indicate that the activities in
BEHAVIOR-1K are long-horizon and dependent on complex manipulation skills, both
of which remain a challenge for even state-of-the-art robot learning solutions.
To calibrate the simulation-to-reality gap of BEHAVIOR-1K, we provide an
initial study on transferring solutions learned with a mobile manipulator in a
simulated apartment to its real-world counterpart. We hope that BEHAVIOR-1K's
human-grounded nature, diversity, and realism make it valuable for embodied AI
and robot learning research. Project website: https://behavior.stanford.edu.",0
Query Rewriting via Large Language Models,2403.09060v1,http://arxiv.org/abs/2403.09060v1,2024-03-14 02:56:42+00:00,"Query rewriting is one of the most effective techniques for coping with
poorly written queries before passing them down to the query optimizer. Manual
rewriting is not scalable, as it is error-prone and requires deep expertise.
Similarly, traditional query rewriting algorithms can only handle a small
subset of queries: rule-based techniques do not generalize to new query
patterns and synthesis-based techniques cannot handle complex queries.
Fortunately, the rise of Large Language Models (LLMs), equipped with broad
general knowledge and advanced reasoning capabilities, has created hopes for
solving some of these previously open problems.
  In this paper, we present GenRewrite, the first holistic system that
leverages LLMs for query rewriting. We introduce the notion of Natural Language
Rewrite Rules (NLR2s), and use them as hints to the LLM but also a means for
transferring knowledge from rewriting one query to another, and thus becoming
smarter and more effective over time. We present a novel counterexample-guided
technique that iteratively corrects the syntactic and semantic errors in the
rewritten query, significantly reducing the LLM costs and the manual effort
required for verification. GenRewrite speeds up 22 out of 99 TPC queries (the
most complex public benchmark) by more than 2x, which is 2.5x--3.2x higher
coverage than state-of-the-art traditional query rewriting and 2.1x higher than
the out-of-the-box LLM baseline.",0
Keyformer: KV Cache Reduction through Key Tokens Selection for Efficient Generative Inference,2403.09054v1,http://arxiv.org/abs/2403.09054v1,2024-03-14 02:42:42+00:00,"Transformers have emerged as the underpinning architecture for Large Language
Models (LLMs). In generative language models, the inference process involves
two primary phases: prompt processing and token generation. Token generation,
which constitutes the majority of the computational workload, primarily entails
vector-matrix multiplications and interactions with the Key-Value (KV) Cache.
This phase is constrained by memory bandwidth due to the overhead of
transferring weights and KV cache values from the memory system to the
computing units. This memory bottleneck becomes particularly pronounced in
applications that require long-context and extensive text generation, both of
which are increasingly crucial for LLMs.
  This paper introduces ""Keyformer"", an innovative inference-time approach, to
mitigate the challenges associated with KV cache size and memory bandwidth
utilization. Keyformer leverages the observation that approximately 90% of the
attention weight in generative inference focuses on a specific subset of
tokens, referred to as ""key"" tokens. Keyformer retains only the key tokens in
the KV cache by identifying these crucial tokens using a novel score function.
This approach effectively reduces both the KV cache size and memory bandwidth
usage without compromising model accuracy. We evaluate Keyformer's performance
across three foundational models: GPT-J, Cerebras-GPT, and MPT, which employ
various positional embedding algorithms. Our assessment encompasses a variety
of tasks, with a particular emphasis on summarization and conversation tasks
involving extended contexts. Keyformer's reduction of KV cache reduces
inference latency by 2.1x and improves token generation throughput by 2.4x,
while preserving the model's accuracy.",0
Towards a theory of model distillation,2403.09053v1,http://arxiv.org/abs/2403.09053v1,2024-03-14 02:42:19+00:00,"Distillation is the task of replacing a complicated machine learning model
with a simpler model that approximates the original [BCNM06,HVD15]. Despite
many practical applications, basic questions about the extent to which models
can be distilled, and the runtime and amount of data needed to distill, remain
largely open.
  To study these questions, we initiate a general theory of distillation,
defining PAC-distillation in an analogous way to PAC-learning [Val84]. As
applications of this theory: (1) we propose new algorithms to extract the
knowledge stored in the trained weights of neural networks -- we show how to
efficiently distill neural networks into succinct, explicit decision tree
representations when possible by using the ``linear representation
hypothesis''; and (2) we prove that distillation can be much cheaper than
learning from scratch, and make progress on characterizing its complexity.",0
Taming Cross-Domain Representation Variance in Federated Prototype Learning with Heterogeneous Data Domains,2403.09048v1,http://arxiv.org/abs/2403.09048v1,2024-03-14 02:36:16+00:00,"Federated learning (FL) allows collaborative machine learning training
without sharing private data. While most FL methods assume identical data
domains across clients, real-world scenarios often involve heterogeneous data
domains. Federated Prototype Learning (FedPL) addresses this issue, using mean
feature vectors as prototypes to enhance model generalization. However,
existing FedPL methods create the same number of prototypes for each client,
leading to cross-domain performance gaps and disparities for clients with
varied data distributions. To mitigate cross-domain feature representation
variance, we introduce FedPLVM, which establishes variance-aware dual-level
prototypes clustering and employs a novel $\alpha$-sparsity prototype loss. The
dual-level prototypes clustering strategy creates local clustered prototypes
based on private data features, then performs global prototypes clustering to
reduce communication complexity and preserve local data privacy. The
$\alpha$-sparsity prototype loss aligns samples from underrepresented domains,
enhancing intra-class similarity and reducing inter-class similarity.
Evaluations on Digit-5, Office-10, and DomainNet datasets demonstrate our
method's superiority over existing approaches.",0
How do Older Adults Set Up Voice Assistants? Lessons Learned from a Deployment Experience for Older Adults to Set Up Standalone Voice Assistants,2403.09043v1,http://arxiv.org/abs/2403.09043v1,2024-03-14 02:28:29+00:00,"While standalone Voice Assistants (VAs) are promising to support older
adults' daily routine and wellbeing management, onboarding and setting up these
devices can be challenging. Although some older adults choose to seek
assistance from technicians and adult children, easy set up processes that
facilitate independent use are still critical, especially for those who do not
have access to external resources. We aim to understand the older adults'
experience while setting up commercially available voice-only and voice-first
screen-based VAs. Rooted in participants observations and semi-structured
interviews, we designed a within-subject study with 10 older adults using
Amazon Echo Dot and Echo Show. We identified the values of the built-in
touchscreen and the instruction documents, as well as the impact of form
factors, and outline important directions to support older adult independence
with VAs.",0
Spatial-temporal Memories Enhanced Graph Autoencoder for Anomaly Detection in Dynamic Graphs,2403.09039v1,http://arxiv.org/abs/2403.09039v1,2024-03-14 02:26:10+00:00,"Anomaly detection in dynamic graphs presents a significant challenge due to
the temporal evolution of graph structures and attributes. The conventional
approaches that tackle this problem typically employ an unsupervised learning
framework, capturing normality patterns with exclusive normal data during
training and identifying deviations as anomalies during testing. However, these
methods face critical drawbacks: they either only depend on proxy tasks for
general representation without directly pinpointing normal patterns, or they
neglect to differentiate between spatial and temporal normality patterns,
leading to diminished efficacy in anomaly detection. To address these
challenges, we introduce a novel Spatial-Temporal memories-enhanced graph
autoencoder (STRIPE). Initially, STRIPE employs Graph Neural Networks (GNNs)
and gated temporal convolution layers to extract spatial features and temporal
features, respectively. Then STRIPE incorporates separate spatial and temporal
memory networks, which capture and store prototypes of normal patterns, thereby
preserving the uniqueness of spatial and temporal normality. After that,
through a mutual attention mechanism, these stored patterns are then retrieved
and integrated with encoded graph embeddings. Finally, the integrated features
are fed into the decoder to reconstruct the graph streams which serve as the
proxy task for anomaly detection. This comprehensive approach not only
minimizes reconstruction errors but also refines the model by emphasizing the
compactness and distinctiveness of the embeddings in relation to the nearest
memory prototypes. Through extensive testing, STRIPE has demonstrated a
superior capability to discern anomalies by effectively leveraging the distinct
spatial and temporal dynamics of dynamic graphs, significantly outperforming
existing methodologies, with an average improvement of 15.39% on AUC values.",0
DiTMoS: Delving into Diverse Tiny-Model Selection on Microcontrollers,2403.09035v1,http://arxiv.org/abs/2403.09035v1,2024-03-14 02:11:38+00:00,"Enabling efficient and accurate deep neural network (DNN) inference on
microcontrollers is non-trivial due to the constrained on-chip resources.
Current methodologies primarily focus on compressing larger models yet at the
expense of model accuracy. In this paper, we rethink the problem from the
inverse perspective by constructing small/weak models directly and improving
their accuracy. Thus, we introduce DiTMoS, a novel DNN training and inference
framework with a selector-classifiers architecture, where the selector routes
each input sample to the appropriate classifier for classification. DiTMoS is
grounded on a key insight: a composition of weak models can exhibit high
diversity and the union of them can significantly boost the accuracy upper
bound. To approach the upper bound, DiTMoS introduces three strategies
including diverse training data splitting to increase the classifiers'
diversity, adversarial selector-classifiers training to ensure synergistic
interactions thereby maximizing their complementarity, and heterogeneous
feature aggregation to improve the capacity of classifiers. We further propose
a network slicing technique to alleviate the extra memory overhead incurred by
feature aggregation. We deploy DiTMoS on the Neucleo STM32F767ZI board and
evaluate it based on three time-series datasets for human activity recognition,
keywords spotting, and emotion recognition, respectively. The experiment
results manifest that: (a) DiTMoS achieves up to 13.4% accuracy improvement
compared to the best baseline; (b) network slicing almost completely eliminates
the memory overhead incurred by feature aggregation with a marginal increase of
latency.",0
Adaptivity is not helpful for Pauli channel learning,2403.09033v1,http://arxiv.org/abs/2403.09033v1,2024-03-14 01:54:29+00:00,"This note shows that adaptive strategies do not offer additional advantages
for learning and testing Pauli channels with entangled input. First, the tight
query complexity of learning Pauli channels with entangled input is established
for the general norm $l_p$. In particular, the complexities for the $l_{1}$,
$l_2$ and $l_\infty$ norms are improved or matched compared to previous results
using entanglement in the literature. We also settle the query complexity to
test if Pauli channels are white noise sources across $l_p$. Additionally, we
demonstrate that the query complexity of estimating the noise level of a Pauli
channel, characterized by the entropy of its error distribution and the count
of non-zero probabilities, is $\Theta(4^n/n)$. Further, $\Theta(4^n/n)$ queries
are sufficient to estimate the diamond norm between two Pauli channels.",0
CodeUltraFeedback: An LLM-as-a-Judge Dataset for Aligning Large Language Models to Coding Preferences,2403.09032v1,http://arxiv.org/abs/2403.09032v1,2024-03-14 01:51:35+00:00,"Evaluating the alignment of large language models (LLMs) with user-defined
coding preferences is a challenging endeavour that requires assessing intricate
textual LLMs' outputs. By relying on automated metrics and static analysis
tools, existing benchmarks fail to assess nuances in user instructions and LLM
outputs, highlighting the need for large-scale datasets and benchmarks for LLM
preference alignment. In this paper, we introduce CodeUltraFeedback, a
preference dataset of 10,000 complex instructions to tune and align LLMs to
coding preferences through AI feedback. We generate responses to the
instructions using a pool of 14 diverse LLMs, which we then annotate according
to their alignment with five coding preferences using the LLM-as-a-Judge
approach with GPT-3.5, producing both numerical and textual feedback. We also
present CODAL-Bench, a benchmark for assessing LLM alignment with these coding
preferences. Our results show that CodeLlama-7B-Instruct, aligned through
reinforcement learning from AI feedback (RLAIF) with direct preference
optimization (DPO) using CodeUltraFeedback's AI feedback data, outperforms 34B
LLMs on CODAL-Bench, validating the utility of CodeUltraFeedback for preference
tuning. Furthermore, we show our DPO-aligned CodeLlama model improves
functional correctness on HumanEval+ compared to the unaligned base model.
Therefore, our contributions bridge the gap in preference tuning of LLMs for
code and set the stage for further advancements in model alignment and RLAIF
for code intelligence. Our code and data are available at
https://github.com/martin-wey/CodeUltraFeedback.",0
An AI-Driven Approach to Wind Turbine Bearing Fault Diagnosis from Acoustic Signals,2403.09030v1,http://arxiv.org/abs/2403.09030v1,2024-03-14 01:46:30+00:00,"This study aimed to develop a deep learning model for the classification of
bearing faults in wind turbine generators from acoustic signals. A
convolutional LSTM model was successfully constructed and trained by using
audio data from five predefined fault types for both training and validation.
To create the dataset, raw audio signal data was collected and processed in
frames to capture time and frequency domain information. The model exhibited
outstanding accuracy on training samples and demonstrated excellent
generalization ability during validation, indicating its proficiency of
generalization capability. On the test samples, the model achieved remarkable
classification performance, with an overall accuracy exceeding 99.5%, and a
false positive rate of less than 1% for normal status. The findings of this
study provide essential support for the diagnosis and maintenance of bearing
faults in wind turbine generators, with the potential to enhance the
reliability and efficiency of wind power generation.",0
FlexNN: A Dataflow-aware Flexible Deep Learning Accelerator for Energy-Efficient Edge Devices,2403.09026v1,http://arxiv.org/abs/2403.09026v1,2024-03-14 01:39:12+00:00,"This paper introduces FlexNN, a Flexible Neural Network accelerator, which
adopts agile design principles to enable versatile dataflows, enhancing energy
efficiency. Unlike conventional convolutional neural network accelerator
architectures that adhere to fixed dataflows (such as input, weight, output, or
row stationary) for transferring activations and weights between storage and
compute units, our design revolutionizes by enabling adaptable dataflows of any
type through software configurable descriptors. Considering that data movement
costs considerably outweigh compute costs from an energy perspective, the
flexibility in dataflow allows us to optimize the movement per layer for
minimal data transfer and energy consumption, a capability unattainable in
fixed dataflow architectures. To further enhance throughput and reduce energy
consumption in the FlexNN architecture, we propose a novel sparsity-based
acceleration logic that utilizes fine-grained sparsity in both the activation
and weight tensors to bypass redundant computations, thus optimizing the
convolution engine within the hardware accelerator. Extensive experimental
results underscore a significant enhancement in the performance and energy
efficiency of FlexNN relative to existing DNN accelerators.",0
VDNA-PR: Using General Dataset Representations for Robust Sequential Visual Place Recognition,2403.09025v1,http://arxiv.org/abs/2403.09025v1,2024-03-14 01:30:28+00:00,"This paper adapts a general dataset representation technique to produce
robust Visual Place Recognition (VPR) descriptors, crucial to enable real-world
mobile robot localisation. Two parallel lines of work on VPR have shown, on one
side, that general-purpose off-the-shelf feature representations can provide
robustness to domain shifts, and, on the other, that fused information from
sequences of images improves performance. In our recent work on measuring
domain gaps between image datasets, we proposed a Visual Distribution of Neuron
Activations (VDNA) representation to represent datasets of images. This
representation can naturally handle image sequences and provides a general and
granular feature representation derived from a general-purpose model. Moreover,
our representation is based on tracking neuron activation values over the list
of images to represent and is not limited to a particular neural network layer,
therefore having access to high- and low-level concepts. This work shows how
VDNAs can be used for VPR by learning a very lightweight and simple encoder to
generate task-specific descriptors. Our experiments show that our
representation can allow for better robustness than current solutions to
serious domain shifts away from the training data distribution, such as to
indoor environments and aerial imagery.",0
Meta-Learning-Based Fronthaul Compression for Cloud Radio Access Networks,2403.09004v1,http://arxiv.org/abs/2403.09004v1,2024-03-13 23:50:32+00:00,"This paper investigates the fronthaul compression problem in a user-centric
cloud radio access network, in which single-antenna users are served by a
central processor (CP) cooperatively via a cluster of remote radio heads
(RRHs). To satisfy the fronthaul capacity constraint, this paper proposes a
transform-compress-forward scheme, which consists of well-designed
transformation matrices and uniform quantizers. The transformation matrices
perform dimension reduction in the uplink and dimension expansion in the
downlink. To reduce the communication overhead for designing the transformation
matrices, this paper further proposes a deep learning framework to first learn
a suboptimal transformation matrix at each RRH based on the local channel state
information (CSI), and then to refine it iteratively. To facilitate the
refinement process, we propose an efficient signaling scheme that only requires
the transmission of low-dimensional effective CSI and its gradient between the
CP and RRH, and further, a meta-learning based gated recurrent unit network to
reduce the number of signaling transmission rounds. For the sum-rate
maximization problem, simulation results show that the proposed two-stage
neural network can perform close to the fully cooperative global CSI based
benchmark with significantly reduced communication overhead for both the uplink
and the downlink. Moreover, using the first stage alone can already outperform
the existing local CSI based benchmark.",0
SINDy-RL: Interpretable and Efficient Model-Based Reinforcement Learning,2403.09110v1,http://arxiv.org/abs/2403.09110v1,2024-03-14 05:17:39+00:00,"Deep reinforcement learning (DRL) has shown significant promise for
uncovering sophisticated control policies that interact in environments with
complicated dynamics, such as stabilizing the magnetohydrodynamics of a tokamak
fusion reactor or minimizing the drag force exerted on an object in a fluid
flow. However, these algorithms require an abundance of training examples and
may become prohibitively expensive for many applications. In addition, the
reliance on deep neural networks often results in an uninterpretable, black-box
policy that may be too computationally expensive to use with certain embedded
systems. Recent advances in sparse dictionary learning, such as the sparse
identification of nonlinear dynamics (SINDy), have shown promise for creating
efficient and interpretable data-driven models in the low-data regime. In this
work we introduce SINDy-RL, a unifying framework for combining SINDy and DRL to
create efficient, interpretable, and trustworthy representations of the
dynamics model, reward function, and control policy. We demonstrate the
effectiveness of our approaches on benchmark control environments and
challenging fluids problems. SINDy-RL achieves comparable performance to
state-of-the-art DRL algorithms using significantly fewer interactions in the
environment and results in an interpretable control policy orders of magnitude
smaller than a deep neural network policy.",0
AutoLoRA: Automatically Tuning Matrix Ranks in Low-Rank Adaptation Based on Meta Learning,2403.09113v1,http://arxiv.org/abs/2403.09113v1,2024-03-14 05:29:35+00:00,"Large-scale pretraining followed by task-specific finetuning has achieved
great success in various NLP tasks. Since finetuning all parameters of large
pretrained models poses substantial computational and memory challenges,
several efficient finetuning methods have been developed. Among them, low-rank
adaptation (LoRA), which finetunes low-rank incremental update matrices on top
of frozen pretrained weights, has proven particularly effective. Nonetheless,
LoRA's uniform rank assignment across all layers, along with its reliance on an
exhaustive search to find the best rank, leads to high computation costs and
suboptimal finetuning performance. To address these limitations, we introduce
AutoLoRA, a meta learning based framework for automatically identifying the
optimal rank of each LoRA layer. AutoLoRA associates each rank-1 matrix in a
low-rank update matrix with a selection variable, which determines whether the
rank-1 matrix should be discarded. A meta learning based method is developed to
learn these selection variables. The optimal rank is determined by thresholding
the values of these variables. Our comprehensive experiments on natural
language understanding, generation, and sequence labeling demonstrate the
effectiveness of AutoLoRA.",0
Randomized Principal Component Analysis for Hyperspectral Image Classification,2403.09117v1,http://arxiv.org/abs/2403.09117v1,2024-03-14 05:40:23+00:00,"The high-dimensional feature space of the hyperspectral imagery poses major
challenges to the processing and analysis of the hyperspectral data sets. In
such a case, dimensionality reduction is necessary to decrease the
computational complexity. The random projections open up new ways of
dimensionality reduction, especially for large data sets. In this paper, the
principal component analysis (PCA) and randomized principal component analysis
(R-PCA) for the classification of hyperspectral images using support vector
machines (SVM) and light gradient boosting machines (LightGBM) have been
investigated. In this experimental research, the number of features was reduced
to 20 and 30 for classification of two hyperspectral datasets (Indian Pines and
Pavia University). The experimental results demonstrated that PCA outperformed
R-PCA for SVM for both datasets, but received close accuracy values for
LightGBM. The highest classification accuracies were obtained as 0.9925 and
0.9639 by LightGBM with original features for the Pavia University and Indian
Pines, respectively.",0
Optimal Top-Two Method for Best Arm Identification and Fluid Analysis,2403.09123v1,http://arxiv.org/abs/2403.09123v1,2024-03-14 06:14:07+00:00,"Top-$2$ methods have become popular in solving the best arm identification
(BAI) problem. The best arm, or the arm with the largest mean amongst finitely
many, is identified through an algorithm that at any sequential step
independently pulls the empirical best arm, with a fixed probability $\beta$,
and pulls the best challenger arm otherwise. The probability of incorrect
selection is guaranteed to lie below a specified $\delta >0$. Information
theoretic lower bounds on sample complexity are well known for BAI problem and
are matched asymptotically as $\delta \rightarrow 0$ by computationally
demanding plug-in methods. The above top 2 algorithm for any $\beta \in (0,1)$
has sample complexity within a constant of the lower bound. However,
determining the optimal $\beta$ that matches the lower bound has proven
difficult. In this paper, we address this and propose an optimal top-2 type
algorithm. We consider a function of allocations anchored at a threshold. If it
exceeds the threshold then the algorithm samples the empirical best arm.
Otherwise, it samples the challenger arm. We show that the proposed algorithm
is optimal as $\delta \rightarrow 0$. Our analysis relies on identifying a
limiting fluid dynamics of allocations that satisfy a series of ordinary
differential equations pasted together and that describe the asymptotic path
followed by our algorithm. We rely on the implicit function theorem to show
existence and uniqueness of these fluid ode's and to show that the proposed
algorithm remains close to the ode solution.",0
A Robust Semantic Communication System for Image,2403.09222v1,http://arxiv.org/abs/2403.09222v1,2024-03-14 09:39:18+00:00,"Semantic communications have gained significant attention as a promising
approach to address the transmission bottleneck, especially with the continuous
development of 6G techniques. Distinct from the well investigated physical
channel impairments, this paper focuses on semantic impairments in image,
particularly those arising from adversarial perturbations. Specifically, we
propose a novel metric for quantifying the intensity of semantic impairment and
develop a semantic impairment dataset. Furthermore, we introduce a deep
learning enabled semantic communication system, termed as DeepSC-RI, to enhance
the robustness of image transmission, which incorporates a multi-scale semantic
extractor with a dual-branch architecture for extracting semantics with varying
granularity, thereby improving the robustness of the system. The fine-grained
branch incorporates a semantic importance evaluation module to identify and
prioritize crucial semantics, while the coarse-grained branch adopts a
hierarchical approach for capturing the robust semantics. These two streams of
semantics are seamlessly integrated via an advanced cross-attention-based
semantic fusion module. Experimental results demonstrate the superior
performance of DeepSC-RI under various levels of semantic impairment intensity.",0
An Extensive Comparison of Static Application Security Testing Tools,2403.09219v1,http://arxiv.org/abs/2403.09219v1,2024-03-14 09:37:54+00:00,"Context: Static Application Security Testing Tools (SASTTs) identify software
vulnerabilities to support the security and reliability of software
applications. Interestingly, several studies have suggested that alternative
solutions may be more effective than SASTTs due to their tendency to generate
false alarms, commonly referred to as low Precision. Aim: We aim to
comprehensively evaluate SASTTs, setting a reliable benchmark for assessing and
finding gaps in vulnerability identification mechanisms based on SASTTs or
alternatives. Method: Our SASTTs evaluation is based on a controlled, though
synthetic, Java codebase. It involves an assessment of 1.5 million test
executions, and it features innovative methodological features such as
effort-aware accuracy metrics and method-level analysis. Results: Our findings
reveal that SASTTs detect a tiny range of vulnerabilities. In contrast to
prevailing wisdom, SASTTs exhibit high Precision while falling short in Recall.
Conclusions: The paper suggests that enhancing Recall, alongside expanding the
spectrum of detected vulnerability types, should be the primary focus for
improving SASTTs or alternative approaches, such as machine learning-based
vulnerability identification solutions.",0
Rumor Mitigation in Social Media Platforms with Deep Reinforcement Learning,2403.09217v1,http://arxiv.org/abs/2403.09217v1,2024-03-14 09:32:25+00:00,"Social media platforms have become one of the main channels where people
disseminate and acquire information, of which the reliability is severely
threatened by rumors widespread in the network. Existing approaches such as
suspending users or broadcasting real information to combat rumors are either
with high cost or disturbing users. In this paper, we introduce a novel rumor
mitigation paradigm, where only a minimal set of links in the social network
are intervened to decelerate the propagation of rumors, countering
misinformation with low business cost and user awareness. A knowledge-informed
agent embodying rumor propagation mechanisms is developed, which intervenes the
social network with a graph neural network for capturing information flow in
the social media platforms and a policy network for selecting links.
Experiments on real social media platforms demonstrate that the proposed
approach can effectively alleviate the influence of rumors, substantially
reducing the affected populations by over 25%. Codes for this paper are
released at https://github.com/tsinghua-fib-lab/DRL-Rumor-Mitigation.",0
On the Laplace Approximation as Model Selection Criterion for Gaussian Processes,2403.09215v1,http://arxiv.org/abs/2403.09215v1,2024-03-14 09:28:28+00:00,"Model selection aims to find the best model in terms of accuracy,
interpretability or simplicity, preferably all at once. In this work, we focus
on evaluating model performance of Gaussian process models, i.e. finding a
metric that provides the best trade-off between all those criteria. While
previous work considers metrics like the likelihood, AIC or dynamic nested
sampling, they either lack performance or have significant runtime issues,
which severely limits applicability. We address these challenges by introducing
multiple metrics based on the Laplace approximation, where we overcome a severe
inconsistency occuring during naive application of the Laplace approximation.
Experiments show that our metrics are comparable in quality to the gold
standard dynamic nested sampling without compromising for computational speed.
Our model selection criteria allow significantly faster and high quality model
selection of Gaussian process models.",0
LAN: Learning Adaptive Neighbors for Real-Time Insider Threat Detection,2403.09209v1,http://arxiv.org/abs/2403.09209v1,2024-03-14 09:22:17+00:00,"Enterprises and organizations are faced with potential threats from insider
employees that may lead to serious consequences. Previous studies on insider
threat detection (ITD) mainly focus on detecting abnormal users or abnormal
time periods (e.g., a week or a day). However, a user may have hundreds of
thousands of activities in the log, and even within a day there may exist
thousands of activities for a user, requiring a high investigation budget to
verify abnormal users or activities given the detection results. On the other
hand, existing works are mainly post-hoc methods rather than real-time
detection, which can not report insider threats in time before they cause loss.
In this paper, we conduct the first study towards real-time ITD at activity
level, and present a fine-grained and efficient framework LAN. Specifically,
LAN simultaneously learns the temporal dependencies within an activity sequence
and the relationships between activities across sequences with graph structure
learning. Moreover, to mitigate the data imbalance problem in ITD, we propose a
novel hybrid prediction loss, which integrates self-supervision signals {from
normal activities} and supervision signals from abnormal activities into a
unified loss for anomaly detection. We evaluate the performance of LAN on two
widely used datasets, i.e., CERT r4.2 and CERT r5.2. Extensive and comparative
experiments demonstrate the superiority of LAN, outperforming 9
state-of-the-art baselines by at least 9.92% and 6.35% in AUC for real-time ITD
on CERT r4.2 and r5.2, respectively. Moreover, LAN can be also applied to
post-hoc ITD, surpassing 8 competitive baselines by at least 7.70% and 4.03% in
AUC on two datasets. Finally, the ablation study, parameter analysis, and
compatibility analysis evaluate the impact of each module and hyper-parameter
in LAN.",0
TaxoLLaMA: WordNet-based Model for Solving Multiple Lexical Sematic Tasks,2403.09207v1,http://arxiv.org/abs/2403.09207v1,2024-03-14 09:21:25+00:00,"In this paper, we explore the capabilities of LLMs in capturing
lexical-semantic knowledge from WordNet on the example of the LLaMA-2-7b model
and test it on multiple lexical semantic tasks. As the outcome of our
experiments, we present TaxoLLaMA, the everything-in-one model, lightweight due
to 4-bit quantization and LoRA. It achieves 11 SotA results, 4 top-2 results
out of 16 tasks for the Taxonomy Enrichment, Hypernym Discovery, Taxonomy
Construction, and Lexical Entailment tasks. Moreover, it demonstrates very
strong zero-shot performance on Lexical Entailment and Taxonomy Construction
with no fine-tuning. We also explore its hidden multilingual and domain
adaptation capabilities with a little tuning or few-shot learning. All
datasets, code, and model are available online at
https://github.com/VityaVitalich/TaxoLLaMA",0
Upper Bound of Bayesian Generalization Error in Partial Concept Bottleneck Model (CBM): Partial CBM outperforms naive CBM,2403.09206v1,http://arxiv.org/abs/2403.09206v1,2024-03-14 09:19:50+00:00,"Concept Bottleneck Model (CBM) is a methods for explaining neural networks.
In CBM, concepts which correspond to reasons of outputs are inserted in the
last intermediate layer as observed values. It is expected that we can
interpret the relationship between the output and concept similar to linear
regression. However, this interpretation requires observing all concepts and
decreases the generalization performance of neural networks. Partial CBM
(PCBM), which uses partially observed concepts, has been devised to resolve
these difficulties. Although some numerical experiments suggest that the
generalization performance of PCBMs is almost as high as that of the original
neural networks, the theoretical behavior of its generalization error has not
been yet clarified since PCBM is singular statistical model. In this paper, we
reveal the Bayesian generalization error in PCBM with a three-layered and
linear architecture. The result indcates that the structure of partially
observed concepts decreases the Bayesian generalization error compared with
that of CBM (full-observed concepts).",0
Perspectives on physics-based one-dimensional modeling of lung physiology,2403.09203v1,http://arxiv.org/abs/2403.09203v1,2024-03-14 09:16:13+00:00,"The need to understand how infection spreads to the deep lung was acutely
realized during the Severe Acute Respiratory Syndrome Coronavirus-2
(SARS-CoV-2) pandemic. The challenge of modeling virus laden aerosol transport
and deposition in the airways, coupled with mucus clearance, and infection
kinetics, became evident. This perspective provides a consolidated view of
coupled one-dimensional physics-based mathematical models to probe multifaceted
aspects of lung physiology. Successes of 1D trumpet models in providing
mechanistic insights into lung function and optimalities are reviewed while
identifying limitations and future directions. Key non-dimensional numbers
defining lung function are reported. The need to quantitatively map various
pathologies on a physics-based parameter space of non-dimensional numbers (a
virtual disease landscape) is noted with an eye on translating modeling to
clinical practice. This could aid in disease diagnosis, get mechanistic
insights into pathologies, and determine patient specific treatment plan. 1D
modeling could be an important tool in developing novel measurement and
analysis platforms that could be deployed at point-of-care.",0
Rectified deep neural networks overcome the curse of dimensionality in the numerical approximation of gradient-dependent semilinear heat equations,2403.09200v1,http://arxiv.org/abs/2403.09200v1,2024-03-14 09:14:12+00:00,"Numerical experiments indicate that deep learning algorithms overcome the
curse of dimensionality when approximating solutions of semilinear PDEs. For
certain linear PDEs and semilinear PDEs with gradient-independent
nonlinearities this has also been proved mathematically, i.e., it has been
shown that the number of parameters of the approximating DNN increases at most
polynomially in both the PDE dimension $d\in \mathbb{N}$ and the reciprocal of
the prescribed accuracy $\epsilon\in (0,1)$. The main contribution of this
paper is to rigorously prove for the first time that deep neural networks can
also overcome the curse dimensionality in the approximation of a certain class
of nonlinear PDEs with gradient-dependent nonlinearities.",0
Customizing Segmentation Foundation Model via Prompt Learning for Instance Segmentation,2403.09199v1,http://arxiv.org/abs/2403.09199v1,2024-03-14 09:13:51+00:00,"Recently, foundation models trained on massive datasets to adapt to a wide
range of domains have attracted considerable attention and are actively being
explored within the computer vision community. Among these, the Segment
Anything Model (SAM) stands out for its remarkable progress in generalizability
and flexibility for image segmentation tasks, achieved through prompt-based
object mask generation. However, despite its strength, SAM faces two key
limitations when applied to customized instance segmentation that segments
specific objects or those in unique environments not typically present in the
training data: 1) the ambiguity inherent in input prompts and 2) the necessity
for extensive additional training to achieve optimal segmentation. To address
these challenges, we propose a novel method, customized instance segmentation
via prompt learning tailored to SAM. Our method involves a prompt learning
module (PLM), which adjusts input prompts into the embedding space to better
align with user intentions, thereby enabling more efficient training.
Furthermore, we introduce a point matching module (PMM) to enhance the feature
representation for finer segmentation by ensuring detailed alignment with
ground truth boundaries. Experimental results on various customized instance
segmentation scenarios demonstrate the effectiveness of the proposed method.",0
MetroGNN: Metro Network Expansion with Reinforcement Learning,2403.09197v1,http://arxiv.org/abs/2403.09197v1,2024-03-14 09:09:15+00:00,"Selecting urban regions for metro network expansion to meet maximal
transportation demands is crucial for urban development, while computationally
challenging to solve. The expansion process relies not only on complicated
features like urban demographics and origin-destination (OD) flow but is also
constrained by the existing metro network and urban geography. In this paper,
we introduce a reinforcement learning framework to address a Markov decision
process within an urban heterogeneous multi-graph. Our approach employs an
attentive policy network that intelligently selects nodes based on information
captured by a graph neural network. Experiments on real-world urban data
demonstrate that our proposed methodology substantially improve the satisfied
transportation demands by over 30\% when compared with state-of-the-art
methods. Codes are published at https://github.com/tsinghua-fib-lab/MetroGNN.",0
Are Vision Language Models Texture or Shape Biased and Can We Steer Them?,2403.09193v1,http://arxiv.org/abs/2403.09193v1,2024-03-14 09:07:14+00:00,"Vision language models (VLMs) have drastically changed the computer vision
model landscape in only a few years, opening an exciting array of new
applications from zero-shot image classification, over to image captioning, and
visual question answering. Unlike pure vision models, they offer an intuitive
way to access visual content through language prompting. The wide applicability
of such models encourages us to ask whether they also align with human vision -
specifically, how far they adopt human-induced visual biases through multimodal
fusion, or whether they simply inherit biases from pure vision models. One
important visual bias is the texture vs. shape bias, or the dominance of local
over global information. In this paper, we study this bias in a wide range of
popular VLMs. Interestingly, we find that VLMs are often more shape-biased than
their vision encoders, indicating that visual biases are modulated to some
extent through text in multimodal models. If text does indeed influence visual
biases, this suggests that we may be able to steer visual biases not just
through visual input but also through language: a hypothesis that we confirm
through extensive experiments. For instance, we are able to steer shape bias
from as low as 49% to as high as 72% through prompting alone. For now, the
strong human bias towards shape (96%) remains out of reach for all tested VLMs.",0
Design of an basis-projected layer for sparse datasets in deep learning training using gc-ms spectra as a case study,2403.09188v1,http://arxiv.org/abs/2403.09188v1,2024-03-14 09:03:51+00:00,"Deep learning (DL) models encompass millions or even billions of parameters
and learn complex patterns from big data. However, not all data are initially
stored in a suitable formation to effectively train a DL model, e.g., gas
chromatography-mass spectrometry (GC-MS) spectra and DNA sequence. These
datasets commonly contain many zero values, and the sparse data formation
causes difficulties in optimizing DL models. A DL module called the
basis-projected layer (BPL) was proposed to mitigate the issue by transforming
the sparse data into a dense representation. The transformed data is expected
to facilitate the gradient calculation and finetuned process in a DL training
process. The dataset, example of a sparse dataset, contained 362 specialty
coffee odorant spectra detected from GC-MS. The BPL layer was placed at the
beginning of the DL model. The tunable parameters in the layer were learnable
projected axes that were the bases of a new representation space. The layer
rotated these bases when its parameters were updated. When the number of the
bases was the same as the original dimension, the increasing percentage of the
F1 scores was 8.56%. Furthermore, when the number was set as 768 (the original
dimension was 490), the increasing percentage of the F1 score was 11.49%. The
layer not only maintained the model performance and even constructed a better
representation space in analyzing sparse datasets.",0
Learning Algorithms for Verification of Markov Decision Processes,2403.09184v1,http://arxiv.org/abs/2403.09184v1,2024-03-14 08:54:19+00:00,"We present a general framework for applying learning algorithms and
heuristical guidance to the verification of Markov decision processes (MDPs),
based on the ideas of Br\'azdil, T. et al. (2014). Verification of Markov
Decision Processes Using Learning Algorithms. The primary goal of the
techniques presented in that work is to improve performance by avoiding an
exhaustive exploration of the state space, guided by heuristics. This approach
is significantly extended in this work. Several details of the base theory are
refined and errors are fixed. Section 1.3 provides an overview of all
differences.
  The presented framework focuses on probabilistic reachability, which is a
core problem in verification, and is instantiated in two distinct scenarios.
The first assumes that full knowledge of the MDP is available, in particular
precise transition probabilities. It performs a heuristic-driven partial
exploration of the model, yielding precise lower and upper bounds on the
required probability. The second tackles the case where we may only sample the
MDP without knowing the exact transition dynamics. Here, we obtain
probabilistic guarantees, again in terms of both the lower and upper bounds,
which provides efficient stopping criteria for the approximation. In
particular, the latter is an extension of statistical model-checking (SMC) for
unbounded properties in MDPs. In contrast to other related approaches, we do
not restrict our attention to time-bounded (finite-horizon) or discounted
properties, nor assume any particular structural properties of the MDP.",0
Generalized Relevance Learning Grassmann Quantization,2403.09183v1,http://arxiv.org/abs/2403.09183v1,2024-03-14 08:53:01+00:00,"Due to advancements in digital cameras, it is easy to gather multiple images
(or videos) from an object under different conditions. Therefore, image-set
classification has attracted more attention, and different solutions were
proposed to model them. A popular way to model image sets is subspaces, which
form a manifold called the Grassmann manifold. In this contribution, we extend
the application of Generalized Relevance Learning Vector Quantization to deal
with Grassmann manifold. The proposed model returns a set of prototype
subspaces and a relevance vector. While prototypes model typical behaviours
within classes, the relevance factors specify the most discriminative principal
vectors (or images) for the classification task. They both provide insights
into the model's decisions by highlighting influential images and pixels for
predictions. Moreover, due to learning prototypes, the model complexity of the
new method during inference is independent of dataset size, unlike previous
works. We applied it to several recognition tasks including handwritten digit
recognition, face recognition, activity recognition, and object recognition.
Experiments demonstrate that it outperforms previous works with lower
complexity and can successfully model the variation, such as handwritten style
or lighting conditions. Moreover, the presence of relevances makes the model
robust to the selection of subspaces' dimensionality.",0
Switch Diffusion Transformer: Synergizing Denoising Tasks with Sparse Mixture-of-Experts,2403.09176v1,http://arxiv.org/abs/2403.09176v1,2024-03-14 08:43:43+00:00,"Diffusion models have achieved remarkable success across a range of
generative tasks. Recent efforts to enhance diffusion model architectures have
reimagined them as a form of multi-task learning, where each task corresponds
to a denoising task at a specific noise level. While these efforts have focused
on parameter isolation and task routing, they fall short of capturing detailed
inter-task relationships and risk losing semantic information, respectively. In
response, we introduce Switch Diffusion Transformer (Switch-DiT), which
establishes inter-task relationships between conflicting tasks without
compromising semantic information. To achieve this, we employ a sparse
mixture-of-experts within each transformer block to utilize semantic
information and facilitate handling conflicts in tasks through parameter
isolation. Additionally, we propose a diffusion prior loss, encouraging similar
tasks to share their denoising paths while isolating conflicting ones. Through
these, each transformer block contains a shared expert across all tasks, where
the common and task-specific denoising paths enable the diffusion model to
construct its beneficial way of synergizing denoising tasks. Extensive
experiments validate the effectiveness of our approach in improving both image
quality and convergence rate, and further analysis demonstrates that Switch-DiT
constructs tailored denoising paths across various generation scenarios.",0
ADEdgeDrop: Adversarial Edge Dropping for Robust Graph Neural Networks,2403.09171v1,http://arxiv.org/abs/2403.09171v1,2024-03-14 08:31:39+00:00,"Although Graph Neural Networks (GNNs) have exhibited the powerful ability to
gather graph-structured information from neighborhood nodes via various
message-passing mechanisms, the performance of GNNs is limited by poor
generalization and fragile robustness caused by noisy and redundant graph data.
As a prominent solution, Graph Augmentation Learning (GAL) has recently
received increasing attention. Among prior GAL approaches, edge-dropping
methods that randomly remove edges from a graph during training are effective
techniques to improve the robustness of GNNs. However, randomly dropping edges
often results in bypassing critical edges, consequently weakening the
effectiveness of message passing. In this paper, we propose a novel adversarial
edge-dropping method (ADEdgeDrop) that leverages an adversarial edge predictor
guiding the removal of edges, which can be flexibly incorporated into diverse
GNN backbones. Employing an adversarial training framework, the edge predictor
utilizes the line graph transformed from the original graph to estimate the
edges to be dropped, which improves the interpretability of the edge-dropping
method. The proposed ADEdgeDrop is optimized alternately by stochastic gradient
descent and projected gradient descent. Comprehensive experiments on six graph
benchmark datasets demonstrate that the proposed ADEdgeDrop outperforms
state-of-the-art baselines across various GNN backbones, demonstrating improved
generalization and robustness.",0
Analysis of singular subspaces under random perturbations,2403.09170v1,http://arxiv.org/abs/2403.09170v1,2024-03-14 08:30:25+00:00,"We present a comprehensive analysis of singular vector and singular subspace
perturbations in the context of the signal plus random Gaussian noise matrix
model. Assuming a low-rank signal matrix, we extend the Wedin-Davis-Kahan
theorem in a fully generalized manner, applicable to any unitarily invariant
matrix norm, extending previous results of O'Rourke, Vu and the author. We also
obtain the fine-grained results, which encompass the $\ell_\infty$ analysis of
singular vectors, the $\ell_{2, \infty}$ analysis of singular subspaces, as
well as the exploration of linear and bilinear functions related to the
singular vectors. Moreover, we explore the practical implications of these
findings, in the context of the Gaussian mixture model and the submatrix
localization problem.",0
VIVID: Human-AI Collaborative Authoring of Vicarious Dialogues from Lecture Videos,2403.09168v1,http://arxiv.org/abs/2403.09168v1,2024-03-14 08:27:34+00:00,"The lengthy monologue-style online lectures cause learners to lose engagement
easily. Designing lectures in a ""vicarious dialogue"" format can foster
learners' cognitive activities more than monologue-style. However, designing
online lectures in a dialogue style catered to the diverse needs of learners is
laborious for instructors. We conducted a design workshop with eight
educational experts and seven instructors to present key guidelines and the
potential use of large language models (LLM) to transform a monologue lecture
script into pedagogically meaningful dialogue. Applying these design
guidelines, we created VIVID which allows instructors to collaborate with LLMs
to design, evaluate, and modify pedagogical dialogues. In a within-subjects
study with instructors (N=12), we show that VIVID helped instructors select and
revise dialogues efficiently, thereby supporting the authoring of quality
dialogues. Our findings demonstrate the potential of LLMs to assist instructors
with creating high-quality educational dialogues across various learning
stages.",0
Unveiling the Generalization Power of Fine-Tuned Large Language Models,2403.09162v1,http://arxiv.org/abs/2403.09162v1,2024-03-14 08:18:59+00:00,"While Large Language Models (LLMs) have demonstrated exceptional multitasking
abilities, fine-tuning these models on downstream, domain-specific datasets is
often necessary to yield superior performance on test sets compared to their
counterparts without fine-tuning. However, the comprehensive effects of
fine-tuning on the LLMs' generalization ability are not fully understood. This
paper delves into the differences between original, unmodified LLMs and their
fine-tuned variants. Our primary investigation centers on whether fine-tuning
affects the generalization ability intrinsic to LLMs. To elaborate on this, we
conduct extensive experiments across five distinct language tasks on various
datasets. Our main findings reveal that models fine-tuned on generation and
classification tasks exhibit dissimilar behaviors in generalizing to different
domains and tasks. Intriguingly, we observe that integrating the in-context
learning strategy during fine-tuning on generation tasks can enhance the
model's generalization ability. Through this systematic investigation, we aim
to contribute valuable insights into the evolving landscape of fine-tuning
practices for LLMs.",0
Fairness-Aware Multi-Server Federated Learning Task Delegation over Wireless Networks,2403.09153v1,http://arxiv.org/abs/2403.09153v1,2024-03-14 08:04:46+00:00,"In the rapidly advancing field of federated learning (FL), ensuring efficient
FL task delegation while incentivising FL client participation poses
significant challenges, especially in wireless networks where FL participants'
coverage is limited. Existing Contract Theory-based methods are designed under
the assumption that there is only one FL server in the system (i.e., the
monopoly market assumption), which in unrealistic in practice. To address this
limitation, we propose Fairness-Aware Multi-Server FL task delegation approach
(FAMuS), a novel framework based on Contract Theory and Lyapunov optimization
to jointly address these intricate issues facing wireless multi-server FL
networks (WMSFLN). Within a given WMSFLN, a task requester products multiple FL
tasks and delegate them to FL servers which coordinate the training processes.
To ensure fair treatment of FL servers, FAMuS establishes virtual queues to
track their previous access to FL tasks, updating them in relation to the
resulting FL model performance. The objective is to minimize the time-averaged
cost in a WMSFLN, while ensuring all queues remain stable. This is particularly
challenging given the incomplete information regarding FL clients'
participation cost and the unpredictable nature of the WMSFLN state, which
depends on the locations of the mobile clients. Extensive experiments comparing
FAMuS against five state-of-the-art approaches based on two real-world datasets
demonstrate that it achieves 6.91% higher test accuracy, 27.34% lower cost, and
0.63% higher fairness on average than the best-performing baseline.",0
USimAgent: Large Language Models for Simulating Search Users,2403.09142v1,http://arxiv.org/abs/2403.09142v1,2024-03-14 07:40:54+00:00,"Due to the advantages in the cost-efficiency and reproducibility, user
simulation has become a promising solution to the user-centric evaluation of
information retrieval systems. Nonetheless, accurately simulating user search
behaviors has long been a challenge, because users' actions in search are
highly complex and driven by intricate cognitive processes such as learning,
reasoning, and planning. Recently, Large Language Models (LLMs) have
demonstrated remarked potential in simulating human-level intelligence and have
been used in building autonomous agents for various tasks. However, the
potential of using LLMs in simulating search behaviors has not yet been fully
explored. In this paper, we introduce a LLM-based user search behavior
simulator, USimAgent. The proposed simulator can simulate users' querying,
clicking, and stopping behaviors during search, and thus, is capable of
generating complete search sessions for specific search tasks. Empirical
investigation on a real user behavior dataset shows that the proposed simulator
outperforms existing methods in query generation and is comparable to
traditional methods in predicting user clicks and stopping behaviors. These
results not only validate the effectiveness of using LLMs for user simulation
but also shed light on the development of a more robust and generic user
simulators.",0
Uncertainty Estimation in Multi-Agent Distributed Learning for AI-Enabled Edge Devices,2403.09141v1,http://arxiv.org/abs/2403.09141v1,2024-03-14 07:40:32+00:00,"Initially considered as low-power units with limited autonomous processing,
Edge IoT devices have seen a paradigm shift with the introduction of FPGAs and
AI accelerators. This advancement has vastly amplified their computational
capabilities, emphasizing the practicality of edge AI. Such progress introduces
new challenges of optimizing AI tasks for the limitations of energy and network
resources typical in Edge computing environments. Our study explores methods
that enable distributed data processing through AI-enabled edge devices,
enhancing collaborative learning capabilities. A key focus of our research is
the challenge of determining confidence levels in learning outcomes,
considering the spatial and temporal variability of data sets encountered by
independent agents. To address this issue, we investigate the application of
Bayesian neural networks, proposing a novel approach to manage uncertainty in
distributed learning environments.",0
Metadata-Driven Federated Learning of Connectional Brain Templates in Non-IID Multi-Domain Scenarios,2403.09139v1,http://arxiv.org/abs/2403.09139v1,2024-03-14 07:38:22+00:00,"A connectional brain template (CBT) is a holistic representation of a
population of multi-view brain connectivity graphs, encoding shared patterns
and normalizing typical variations across individuals. The federation of CBT
learning allows for an inclusive estimation of the representative center of
multi-domain brain connectivity datasets in a fully data-preserving manner.
However, existing methods overlook the non-independent and identically
distributed (non-IDD) issue stemming from multidomain brain connectivity
heterogeneity, in which data domains are drawn from different hospitals and
imaging modalities. To overcome this limitation, we unprecedentedly propose a
metadata-driven federated learning framework, called MetaFedCBT, for
cross-domain CBT learning. Given the data drawn from a specific domain (i.e.,
hospital), our model aims to learn metadata in a fully supervised manner by
introducing a local client-based regressor network. The generated meta-data is
forced to meet the statistical attributes (e.g., mean) of other domains, while
preserving their privacy. Our supervised meta-data generation approach boosts
the unsupervised learning of a more centered, representative, and holistic CBT
of a particular brain state across diverse domains. As the federated learning
progresses over multiple rounds, the learned metadata and associated generated
connectivities are continuously updated to better approximate the target domain
information. MetaFedCBT overcomes the non-IID issue of existing methods by
generating informative brain connectivities for privacy-preserving holistic CBT
learning with guidance using metadata. Extensive experiments on multi-view
morphological brain networks of normal and patient subjects demonstrate that
our MetaFedCBT is a superior federated CBT learning model and significantly
advances the state-of-the-art performance.",0
Biophysics Informed Pathological Regularisation for Brain Tumour Segmentation,2403.09136v1,http://arxiv.org/abs/2403.09136v1,2024-03-14 07:21:46+00:00,"Recent advancements in deep learning have significantly improved brain tumour
segmentation techniques; however, the results still lack confidence and
robustness as they solely consider image data without biophysical priors or
pathological information. Integrating biophysics-informed regularisation is one
effective way to change this situation, as it provides an prior regularisation
for automated end-to-end learning. In this paper, we propose a novel approach
that designs brain tumour growth Partial Differential Equation (PDE) models as
a regularisation with deep learning, operational with any network model. Our
method introduces tumour growth PDE models directly into the segmentation
process, improving accuracy and robustness, especially in data-scarce
scenarios. This system estimates tumour cell density using a periodic
activation function. By effectively integrating this estimation with
biophysical models, we achieve a better capture of tumour characteristics. This
approach not only aligns the segmentation closer to actual biological behaviour
but also strengthens the model's performance under limited data conditions. We
demonstrate the effectiveness of our framework through extensive experiments on
the BraTS 2023 dataset, showcasing significant improvements in both precision
and reliability of tumour segmentation.",0
Viral Load Inference in Non-Adaptive Pooled Testing,2403.09130v1,http://arxiv.org/abs/2403.09130v1,2024-03-14 06:40:34+00:00,"Medical diagnostic testing can be made significantly more efficient using
pooled testing protocols. These typically require a sparse infection signal and
use either binary or real-valued entries of O(1). However, existing methods do
not allow for inferring viral loads which span many orders of magnitude. We
develop a message passing algorithm coupled with a PCR (Polymerase Chain
Reaction) specific noise function to allow accurate inference of realistic
viral load signals. This work is in the non-adaptive setting and could open the
possibility of efficient screening where viral load determination is clinically
important.",0
Rethinking Referring Object Removal,2403.09128v1,http://arxiv.org/abs/2403.09128v1,2024-03-14 06:26:34+00:00,"Referring object removal refers to removing the specific object in an image
referred by natural language expressions and filling the missing region with
reasonable semantics. To address this task, we construct the ComCOCO, a
synthetic dataset consisting of 136,495 referring expressions for 34,615
objects in 23,951 image pairs. Each pair contains an image with referring
expressions and the ground truth after elimination. We further propose an
end-to-end syntax-aware hybrid mapping network with an encoding-decoding
structure. Linguistic features are hierarchically extracted at the syntactic
level and fused in the downsampling process of visual features with multi-head
attention. The feature-aligned pyramid network is leveraged to generate
segmentation masks and replace internal pixels with region affinity learned
from external semantics in high-level feature maps. Extensive experiments
demonstrate that our model outperforms diffusion models and two-stage methods
which process the segmentation and inpainting task separately by a significant
margin.",0
Ventilation and Temperature Control for Energy-efficient and Healthy Buildings: A Differentiable PDE Approach,2403.08996v1,http://arxiv.org/abs/2403.08996v1,2024-03-13 23:30:26+00:00,"In this paper, we introduce a novel framework for building learning and
control, focusing on ventilation and thermal management to enhance energy
efficiency. We validate the performance of the proposed framework in system
model learning via two case studies: a synthetic study focusing on the joint
learning of temperature and CO2 fields, and an application to a real-world
dataset for CO2 field learning. For building control, we demonstrate that the
proposed framework can optimize the control actions and significantly reduce
the energy cost while maintaining a comfort and healthy indoor environment.
When compared to existing traditional methods, an optimization-based method
with ODE models and reinforcement learning, our approach can significantly
reduce the energy consumption while guarantees all the safety-critical air
quality and control constraints. Promising future research directions involve
validating and improving the proposed PDE models through accurate estimation of
airflow fields within indoor environments. Additionally, incorporating
uncertainty modeling into the PDE framework for HVAC control presents an
opportunity to enhance the efficiency and reliability of building HVAC system
management.",0
NTIRE 2023 Image Shadow Removal Challenge Technical Report: Team IIM_TTI,2403.08995v1,http://arxiv.org/abs/2403.08995v1,2024-03-13 23:27:31+00:00,"In this paper, we analyze and discuss ShadowFormer in preparation for the
NTIRE2023 Shadow Removal Challenge [1], implementing five key improvements:
image alignment, the introduction of a perceptual quality loss function, the
semi-automatic annotation for shadow detection, joint learning of shadow
detection and removal, and the introduction of new data augmentation techniques
for shadow removal. Our method achieved scores of 0.196 (3rd out of 19) in
LPIPS and 7.44 (3rd out of 19) in the Mean Opinion Score (MOS).",0
Safe Road-Crossing by Autonomous Wheelchairs: a Novel Dataset and its Experimental Evaluation,2403.08984v1,http://arxiv.org/abs/2403.08984v1,2024-03-13 22:19:06+00:00,"Safe road-crossing by self-driving vehicles is a crucial problem to address
in smart-cities. In this paper, we introduce a multi-sensor fusion approach to
support road-crossing decisions in a system composed by an autonomous
wheelchair and a flying drone featuring a robust sensory system made of diverse
and redundant components. To that aim, we designed an analytical danger
function based on explainable physical conditions evaluated by single sensors,
including those using machine learning and artificial vision. As a
proof-of-concept, we provide an experimental evaluation in a laboratory
environment, showing the advantages of using multiple sensors, which can
improve decision accuracy and effectively support safety assessment. We made
the dataset available to the scientific community for further experimentation.
The work has been developed in the context of an European project named
REXASI-PRO, which aims to develop trustworthy artificial intelligence for
social navigation of people with reduced mobility.",0
Moments of Clarity: Streamlining Latent Spaces in Machine Learning using Moment Pooling,2403.08854v1,http://arxiv.org/abs/2403.08854v1,2024-03-13 18:00:01+00:00,"Many machine learning applications involve learning a latent representation
of data, which is often high-dimensional and difficult to directly interpret.
In this work, we propose ""Moment Pooling"", a natural extension of Deep Sets
networks which drastically decrease latent space dimensionality of these
networks while maintaining or even improving performance. Moment Pooling
generalizes the summation in Deep Sets to arbitrary multivariate moments, which
enables the model to achieve a much higher effective latent dimensionality for
a fixed latent dimension. We demonstrate Moment Pooling on the collider physics
task of quark/gluon jet classification by extending Energy Flow Networks (EFNs)
to Moment EFNs. We find that Moment EFNs with latent dimensions as small as 1
perform similarly to ordinary EFNs with higher latent dimension. This small
latent dimension allows for the internal representation to be directly
visualized and interpreted, which in turn enables the learned internal jet
representation to be extracted in closed form.",0
PAPERCLIP: Associating Astronomical Observations and Natural Language with Multi-Modal Models,2403.08851v1,http://arxiv.org/abs/2403.08851v1,2024-03-13 18:00:00+00:00,"We present PAPERCLIP (Proposal Abstracts Provide an Effective Representation
for Contrastive Language-Image Pre-training), a method which associates
astronomical observations imaged by telescopes with natural language using a
neural network model. The model is fine-tuned from a pre-trained Contrastive
Language-Image Pre-training (CLIP) model using successful observing proposal
abstracts and corresponding downstream observations, with the abstracts
optionally summarized via guided generation using large language models (LLMs).
Using observations from the Hubble Space Telescope (HST) as an example, we show
that the fine-tuned model embodies a meaningful joint representation between
observations and natural language through tests targeting image retrieval
(i.e., finding the most relevant observations using natural language queries)
and description retrieval (i.e., querying for astrophysical object classes and
use cases most relevant to a given observation). Our study demonstrates the
potential for using generalist foundation models rather than task-specific
models for interacting with astronomical data by leveraging text as an
interface.",0
Impact of Electron Precipitation on Brown Dwarf Atmospheres and the Missing Auroral H$_{3}^{+}$ Emission,2403.08852v1,http://arxiv.org/abs/2403.08852v1,2024-03-13 18:00:00+00:00,"Recent observations have demonstrated that very-low mass stars and brown
dwarfs are capable of sustaining strong magnetic fields despite their cool and
neutral atmospheres. These kG field strengths are inferred based on strong
highly circularly polarized GHz radio emission, a consequence of the electron
cyclotron maser instability. Crucially, these observations imply the existence
of energetic non-thermal electron populations, associated with strong current
systems, as are found in the auroral regions of the magnetized planets of the
Solar System. Intense auroral electron precipitation will lead to electron
collisions with the H$_{2}$ gas that should ultimately generate the ion
H$_{3}^{+}$. With this motivation, we targeted a sample of ultracool dwarfs,
known to exhibit signatures associated with aurorae, in search of the K-band
emission features of H$_{3}^{+}$ using the Keck telescopes on Mauna Kea. From
our sample of 9 objects, we found no clear indication of H$_{3}^{+}$ emission
features in our low-medium resolution spectra (R$\sim$3600). We also modeled
the impact of an auroral electron beam on a brown dwarf atmosphere, determining
the depth at which energetic beams deposit their energy and drive particle
impact ionization. We find that the H$_{3}^{+}$ non-detections can be explained
by electron beams of typical energies $\gtrsim$2-10~keV, which penetrate deeply
enough that any H$_{3}^{+}$ produced is chemically destroyed before radiating
energy through its infrared transitions. Strong electron beams could further
explain the lack of UV detections, and suggest that most or nearly all of the
precipitating auroral energy must ultimately emerge as thermal emissions deep
in brown dwarf atmospheres.",0
Simple and Scalable Strategies to Continually Pre-train Large Language Models,2403.08763v1,http://arxiv.org/abs/2403.08763v1,2024-03-13 17:58:57+00:00,"Large language models (LLMs) are routinely pre-trained on billions of tokens,
only to start the process over again once new data becomes available. A much
more efficient solution is to continually pre-train these models, saving
significant compute compared to re-training. However, the distribution shift
induced by new data typically results in degraded performance on previous data
or poor adaptation to the new data. In this work, we show that a simple and
scalable combination of learning rate (LR) re-warming, LR re-decaying, and
replay of previous data is sufficient to match the performance of fully
re-training from scratch on all available data, as measured by final loss and
language model (LM) evaluation benchmarks. Specifically, we show this for a
weak but realistic distribution shift between two commonly used LLM
pre-training datasets (English$\rightarrow$English) and a stronger distribution
shift (English$\rightarrow$German) at the $405$M parameter model scale with
large dataset sizes (hundreds of billions of tokens). Selecting the weak but
realistic shift for larger-scale experiments, we also find that our continual
learning strategies match the re-training baseline for a 10B parameter LLM. Our
results demonstrate that LLMs can be successfully updated via simple and
scalable continual learning strategies, matching the re-training baseline using
only a fraction of the compute. Finally, inspired by previous work, we propose
alternatives to the cosine learning rate schedule that help circumvent
forgetting induced by LR re-warming and that are not bound to a fixed token
budget.",0
"Segmentation of Knee Bones for Osteoarthritis Assessment: A Comparative Analysis of Supervised, Few-Shot, and Zero-Shot Learning Approaches",2403.08761v1,http://arxiv.org/abs/2403.08761v1,2024-03-13 17:58:34+00:00,"Knee osteoarthritis is a degenerative joint disease that induces chronic pain
and disability. Bone morphological analysis is a promising tool to understand
the mechanical aspect of this disorder. This study proposes a 2D bone
morphological analysis using manually segmented bones to explore morphological
features related to distinct pain conditions. Furthermore, six semantic
segmentation algorithms are assessed for extracting femur and tibia bones from
X-ray images. Our analysis reveals that the morphology of the femur undergoes
significant changes in instances where pain worsens. Conversely, improvements
in pain may not manifest pronounced alterations in bone shape. The
few-shot-learning-based algorithm, UniverSeg, demonstrated superior
segmentation results with Dice scores of 99.69% for femur and 99.60% for tibia.
Regarding pain condition classification, the zero-shot-learning-based
algorithm, CP-SAM, achieved the highest accuracy at 66% among all models.
UniverSeg is recommended for automatic knee bone segmentation, while SAM models
show potential with prompt encoder modifications for optimized outcomes. These
findings highlight the effectiveness of few-shot learning for semantic
segmentation and the potential of zero-shot learning in enhancing
classification models for knee osteoarthritis diagnosis.",0
MIM4D: Masked Modeling with Multi-View Video for Autonomous Driving Representation Learning,2403.08760v1,http://arxiv.org/abs/2403.08760v1,2024-03-13 17:58:00+00:00,"Learning robust and scalable visual representations from massive multi-view
video data remains a challenge in computer vision and autonomous driving.
Existing pre-training methods either rely on expensive supervised learning with
3D annotations, limiting the scalability, or focus on single-frame or monocular
inputs, neglecting the temporal information. We propose MIM4D, a novel
pre-training paradigm based on dual masked image modeling (MIM). MIM4D
leverages both spatial and temporal relations by training on masked multi-view
video inputs. It constructs pseudo-3D features using continuous scene flow and
projects them onto 2D plane for supervision. To address the lack of dense 3D
supervision, MIM4D reconstruct pixels by employing 3D volumetric differentiable
rendering to learn geometric representations. We demonstrate that MIM4D
achieves state-of-the-art performance on the nuScenes dataset for visual
representation learning in autonomous driving. It significantly improves
existing methods on multiple downstream tasks, including BEV segmentation (8.7%
IoU), 3D object detection (3.5% mAP), and HD map construction (1.4% mAP). Our
work offers a new choice for learning representation at scale in autonomous
driving. Code and models are released at https://github.com/hustvl/MIM4D",0
Spatiotemporal Diffusion Model with Paired Sampling for Accelerated Cardiac Cine MRI,2403.08758v1,http://arxiv.org/abs/2403.08758v1,2024-03-13 17:56:12+00:00,"Current deep learning reconstruction for accelerated cardiac cine MRI suffers
from spatial and temporal blurring. We aim to improve image sharpness and
motion delineation for cine MRI under high undersampling rates. A
spatiotemporal diffusion enhancement model conditional on an existing deep
learning reconstruction along with a novel paired sampling strategy was
developed. The diffusion model provided sharper tissue boundaries and clearer
motion than the original reconstruction in experts evaluation on clinical data.
The innovative paired sampling strategy substantially reduced artificial noises
in the generative results.",0
Efficient Combinatorial Optimization via Heat Diffusion,2403.08757v2,http://arxiv.org/abs/2403.08757v2,2024-03-13 17:55:34+00:00,"Combinatorial optimization problems are widespread but inherently challenging
due to their discrete nature.The primary limitation of existing methods is that
they can only access a small fraction of the solution space at each iteration,
resulting in limited efficiency for searching the global optimal. To overcome
this challenge, diverging from conventional efforts of expanding the solver's
search scope, we focus on enabling information to actively propagate to the
solver through heat diffusion. By transforming the target function while
preserving its optima, heat diffusion facilitates information flow from distant
regions to the solver, providing more efficient navigation. Utilizing heat
diffusion, we propose a framework for solving general combinatorial
optimization problems. The proposed methodology demonstrates superior
performance across a range of the most challenging and widely encountered
combinatorial optimizations. Echoing recent advancements in harnessing
thermodynamics for generative artificial intelligence, our study further
reveals its significant potential in advancing combinatorial optimization.",0
DAM: Dynamic Adapter Merging for Continual Video QA Learning,2403.08755v1,http://arxiv.org/abs/2403.08755v1,2024-03-13 17:53:47+00:00,"We present a parameter-efficient method for continual video
question-answering (VidQA) learning. Our method, named DAM, uses the proposed
Dynamic Adapter Merging to (i) mitigate catastrophic forgetting, (ii) enable
efficient adaptation to continually arriving datasets, (iii) handle inputs from
unknown datasets during inference, and (iv) enable knowledge sharing across
similar dataset domains. Given a set of continually streaming VidQA datasets,
we sequentially train dataset-specific adapters for each dataset while freezing
the parameters of a large pretrained video-language backbone. During inference,
given a video-question sample from an unknown domain, our method first uses the
proposed non-parametric router function to compute a probability for each
adapter, reflecting how relevant that adapter is to the current video-question
input instance. Subsequently, the proposed dynamic adapter merging scheme
aggregates all the adapter weights into a new adapter instance tailored for
that particular test sample to compute the final VidQA prediction, mitigating
the impact of inaccurate router predictions and facilitating knowledge sharing
across domains. Our DAM model outperforms prior state-of-the-art continual
learning approaches by 9.1% while exhibiting 1.9% less forgetting on 6 VidQA
datasets spanning various domains. We further extend DAM to continual image
classification and image QA and outperform prior methods by a large margin. The
code is publicly available at: https://github.com/klauscc/DAM",0
Neural reproducing kernel Banach spaces and representer theorems for deep networks,2403.08750v1,http://arxiv.org/abs/2403.08750v1,2024-03-13 17:51:02+00:00,"Studying the function spaces defined by neural networks helps to understand
the corresponding learning models and their inductive bias. While in some
limits neural networks correspond to function spaces that are reproducing
kernel Hilbert spaces, these regimes do not capture the properties of the
networks used in practice. In contrast, in this paper we show that deep neural
networks define suitable reproducing kernel Banach spaces.
  These spaces are equipped with norms that enforce a form of sparsity,
enabling them to adapt to potential latent structures within the input data and
their representations. In particular, leveraging the theory of reproducing
kernel Banach spaces, combined with variational results, we derive representer
theorems that justify the finite architectures commonly employed in
applications. Our study extends analogous results for shallow networks and can
be seen as a step towards considering more practically plausible neural
architectures.",0
Real-time 3D semantic occupancy prediction for autonomous vehicles using memory-efficient sparse convolution,2403.08748v1,http://arxiv.org/abs/2403.08748v1,2024-03-13 17:50:59+00:00,"In autonomous vehicles, understanding the surrounding 3D environment of the
ego vehicle in real-time is essential. A compact way to represent scenes while
encoding geometric distances and semantic object information is via 3D semantic
occupancy maps. State of the art 3D mapping methods leverage transformers with
cross-attention mechanisms to elevate 2D vision-centric camera features into
the 3D domain. However, these methods encounter significant challenges in
real-time applications due to their high computational demands during
inference. This limitation is particularly problematic in autonomous vehicles,
where GPU resources must be shared with other tasks such as localization and
planning. In this paper, we introduce an approach that extracts features from
front-view 2D camera images and LiDAR scans, then employs a sparse convolution
network (Minkowski Engine), for 3D semantic occupancy prediction. Given that
outdoor scenes in autonomous driving scenarios are inherently sparse, the
utilization of sparse convolution is particularly apt. By jointly solving the
problems of 3D scene completion of sparse scenes and 3D semantic segmentation,
we provide a more efficient learning framework suitable for real-time
applications in autonomous vehicles. We also demonstrate competitive accuracy
on the nuScenes dataset.",0
Steering LLMs Towards Unbiased Responses: A Causality-Guided Debiasing Framework,2403.08743v1,http://arxiv.org/abs/2403.08743v1,2024-03-13 17:46:28+00:00,"Large language models (LLMs) can easily generate biased and discriminative
responses. As LLMs tap into consequential decision-making (e.g., hiring and
healthcare), it is of crucial importance to develop strategies to mitigate
these biases. This paper focuses on social bias, tackling the association
between demographic information and LLM outputs. We propose a causality-guided
debiasing framework that utilizes causal understandings of (1) the
data-generating process of the training corpus fed to LLMs, and (2) the
internal reasoning process of LLM inference, to guide the design of prompts for
debiasing LLM outputs through selection mechanisms. Our framework unifies
existing de-biasing prompting approaches such as inhibitive instructions and
in-context contrastive examples, and sheds light on new ways of debiasing by
encouraging bias-free reasoning. Our strong empirical performance on real-world
datasets demonstrates that our framework provides principled guidelines on
debiasing LLM outputs even with only the black-box access.",0
Learning How to Strategically Disclose Information,2403.08741v1,http://arxiv.org/abs/2403.08741v1,2024-03-13 17:44:16+00:00,"Strategic information disclosure, in its simplest form, considers a game
between an information provider (sender) who has access to some private
information that an information receiver is interested in. While the receiver
takes an action that affects the utilities of both players, the sender can
design information (or modify beliefs) of the receiver through signal
commitment, hence posing a Stackelberg game. However, obtaining a Stackelberg
equilibrium for this game traditionally requires the sender to have access to
the receiver's objective. In this work, we consider an online version of
information design where a sender interacts with a receiver of an unknown type
who is adversarially chosen at each round. Restricting attention to Gaussian
prior and quadratic costs for the sender and the receiver, we show that
$\mathcal{O}(\sqrt{T})$ regret is achievable with full information feedback,
where $T$ is the total number of interactions between the sender and the
receiver. Further, we propose a novel parametrization that allows the sender to
achieve $\mathcal{O}(\sqrt{T})$ regret for a general convex utility function.
We then consider the Bayesian Persuasion problem with an additional cost term
in the objective function, which penalizes signaling policies that are more
informative and obtain $\mathcal{O}(\log(T))$ regret. Finally, we establish a
sublinear regret bound for the partial information feedback setting and provide
simulations to support our theoretical results.",0
Improving Acoustic Word Embeddings through Correspondence Training of Self-supervised Speech Representations,2403.08738v1,http://arxiv.org/abs/2403.08738v1,2024-03-13 17:42:03+00:00,"Acoustic word embeddings (AWEs) are vector representations of spoken words.
An effective method for obtaining AWEs is the Correspondence Auto-Encoder
(CAE). In the past, the CAE method has been associated with traditional MFCC
features. Representations obtained from self-supervised learning (SSL)-based
speech models such as HuBERT, Wav2vec2, etc., are outperforming MFCC in many
downstream tasks. However, they have not been well studied in the context of
learning AWEs. This work explores the effectiveness of CAE with SSL-based
speech representations to obtain improved AWEs. Additionally, the capabilities
of SSL-based speech models are explored in cross-lingual scenarios for
obtaining AWEs. Experiments are conducted on five languages: Polish,
Portuguese, Spanish, French, and English. HuBERT-based CAE model achieves the
best results for word discrimination in all languages, despite Hu-BERT being
pre-trained on English only. Also, the HuBERT-based CAE model works well in
cross-lingual settings. It outperforms MFCC-based CAE models trained on the
target languages when trained on one source language and tested on target
languages.",0
ILCiteR: Evidence-grounded Interpretable Local Citation Recommendation,2403.08737v1,http://arxiv.org/abs/2403.08737v1,2024-03-13 17:38:05+00:00,"Existing Machine Learning approaches for local citation recommendation
directly map or translate a query, which is typically a claim or an entity
mention, to citation-worthy research papers. Within such a formulation, it is
challenging to pinpoint why one should cite a specific research paper for a
particular query, leading to limited recommendation interpretability. To
alleviate this, we introduce the evidence-grounded local citation
recommendation task, where the target latent space comprises evidence spans for
recommending specific papers. Using a distantly-supervised evidence retrieval
and multi-step re-ranking framework, our proposed system, ILCiteR, recommends
papers to cite for a query grounded on similar evidence spans extracted from
the existing research literature. Unlike past formulations that simply output
recommendations, ILCiteR retrieves ranked lists of evidence span and
recommended paper pairs. Secondly, previously proposed neural models for
citation recommendation require expensive training on massive labeled data,
ideally after every significant update to the pool of candidate papers. In
contrast, ILCiteR relies solely on distant supervision from a dynamic evidence
database and pre-trained Transformer-based Language Models without any model
training. We contribute a novel dataset for the evidence-grounded local
citation recommendation task and demonstrate the efficacy of our proposed
conditional neural rank-ensembling approach for re-ranking evidence spans.",0
Strengthening Multimodal Large Language Model with Bootstrapped Preference Optimization,2403.08730v1,http://arxiv.org/abs/2403.08730v1,2024-03-13 17:29:45+00:00,"Multimodal Large Language Models (MLLMs) excel in generating responses based
on visual inputs. However, they often suffer from a bias towards generating
responses similar to their pretraining corpus, overshadowing the importance of
visual information. We treat this bias as a ""preference"" for pretraining
statistics, which hinders the model's grounding in visual input. To mitigate
this issue, we propose Bootstrapped Preference Optimization (BPO), which
conducts preference learning with datasets containing negative responses
bootstrapped from the model itself. Specifically, we propose the following two
strategies: 1) using distorted image inputs to the MLLM for eliciting responses
that contain signified pretraining bias; 2) leveraging text-based LLM to
explicitly inject erroneous but common elements into the original response.
Those undesirable responses are paired with original annotated responses from
the datasets to construct the preference dataset, which is subsequently
utilized to perform preference learning. Our approach effectively suppresses
pretrained LLM bias, enabling enhanced grounding in visual inputs. Extensive
experimentation demonstrates significant performance improvements across
multiple benchmarks, advancing the state-of-the-art in multimodal
conversational systems.",0
Ambient Diffusion Posterior Sampling: Solving Inverse Problems with Diffusion Models trained on Corrupted Data,2403.08728v1,http://arxiv.org/abs/2403.08728v1,2024-03-13 17:28:20+00:00,"We provide a framework for solving inverse problems with diffusion models
learned from linearly corrupted data. Our method, Ambient Diffusion Posterior
Sampling (A-DPS), leverages a generative model pre-trained on one type of
corruption (e.g. image inpainting) to perform posterior sampling conditioned on
measurements from a potentially different forward process (e.g. image
blurring). We test the efficacy of our approach on standard natural image
datasets (CelebA, FFHQ, and AFHQ) and we show that A-DPS can sometimes
outperform models trained on clean data for several image restoration tasks in
both speed and performance. We further extend the Ambient Diffusion framework
to train MRI models with access only to Fourier subsampled multi-coil MRI
measurements at various acceleration factors (R=2, 4, 6, 8). We again observe
that models trained on highly subsampled data are better priors for solving
inverse problems in the high acceleration regime than models trained on fully
sampled data. We open-source our code and the trained Ambient Diffusion MRI
models: https://github.com/utcsilab/ambient-diffusion-mri .",0
Euclid: Testing photometric selection of emission-line galaxy targets,2403.08726v1,http://arxiv.org/abs/2403.08726v1,2024-03-13 17:27:31+00:00,"Multi-object spectroscopic galaxy surveys typically make use of photometric
and colour criteria to select targets. Conversely, the Euclid NISP slitless
spectrograph will record spectra for every source over its field of view.
Slitless spectroscopy has the advantage of avoiding defining a priori a galaxy
sample, but at the price of making the selection function harder to quantify.
The Euclid Wide Survey aims at building robust statistical samples of
emission-line galaxies with fluxes in the Halpha-NII complex brighter than
2e-16 erg/s/cm^2 and within 0.9<z<1.8. At faint fluxes, we expect significant
contamination by wrongly measured redshifts, either due to emission-line
misidentification or noise fluctuations, with the consequence of reducing the
purity of the final samples. This can be significantly improved by exploiting
Euclid photometric information to identify emission-line galaxies over the
redshifts of interest. To this goal, we compare and quantify the performance of
six machine-learning classification algorithms. We consider the case when only
Euclid photometric and morphological measurements are used and when these are
supplemented by ground-based photometric data. We train and test the
classifiers on two mock galaxy samples, the EL-COSMOS and Euclid Flagship2
catalogues. Dense neural networks and support vector classifiers obtain the
best performance, with comparable results in terms of the adopted metrics. When
training on Euclid photometry alone, these can remove 87% of the sources that
are fainter than the nominal flux limit or lie outside the range 0.9<z<1.8, a
figure that increases to 97% when ground-based photometry is included. These
results show how by using the photometric information available to Euclid it
will be possible to efficiently identify and discard spurious interlopers,
allowing us to build robust spectroscopic samples for cosmological
investigations.",0
Probabilistic Metaplasticity for Continual Learning with Memristors,2403.08718v1,http://arxiv.org/abs/2403.08718v1,2024-03-13 17:18:39+00:00,"Crossbar architectures utilizing memristor devices hold promise to address
continual learning challenges in resource-constrained edge devices. However,
these nanoscale devices often exhibit low precision and high variability in
conductance modulation, rendering them unsuitable for continual learning
solutions that consolidate weights through precise modulation. This issue can
be circumvented by accumulating weight gradients in auxiliary high-precision
memory and updating memristor weights when gradients are equivalent to
memristor weight resolution. However, it leads to frequent memory access, high
memory overhead, and energy dissipation. In this research, we propose
probabilistic metaplasticity, which consolidates weights by modulating their
update probability rather than magnitude. The proposed mechanism eliminates
high-precision modification to weight magnitude and consequently,
high-precision memory for gradient accumulation. We demonstrate the efficacy of
the proposed mechanism by integrating probabilistic metaplasticity into a
spiking network trained on an error threshold with low-precision memristor
weights. Evaluations of two continual learning benchmarks show that
probabilistic metaplasticity consumes ~67% lower memory for additional
parameters and up to two orders of magnitude lower energy during parameter
updates compared to an auxiliary memory-based solution while achieving
state-of-the-art performance. The proposed model shows potential for
energy-efficient continual learning with low-precision emerging devices.",0
DIFFTACTILE: A Physics-based Differentiable Tactile Simulator for Contact-rich Robotic Manipulation,2403.08716v1,http://arxiv.org/abs/2403.08716v1,2024-03-13 17:18:19+00:00,"We introduce DIFFTACTILE, a physics-based differentiable tactile simulation
system designed to enhance robotic manipulation with dense and physically
accurate tactile feedback. In contrast to prior tactile simulators which
primarily focus on manipulating rigid bodies and often rely on simplified
approximations to model stress and deformations of materials in contact,
DIFFTACTILE emphasizes physics-based contact modeling with high fidelity,
supporting simulations of diverse contact modes and interactions with objects
possessing a wide range of material properties. Our system incorporates several
key components, including a Finite Element Method (FEM)-based soft body model
for simulating the sensing elastomer, a multi-material simulator for modeling
diverse object types (such as elastic, elastoplastic, cables) under
manipulation, a penalty-based contact model for handling contact dynamics. The
differentiable nature of our system facilitates gradient-based optimization for
both 1) refining physical properties in simulation using real-world data, hence
narrowing the sim-to-real gap and 2) efficient learning of tactile-assisted
grasping and contact-rich manipulation skills. Additionally, we introduce a
method to infer the optical response of our tactile sensor to contact using an
efficient pixel-based neural module. We anticipate that DIFFTACTILE will serve
as a useful platform for studying contact-rich manipulations, leveraging the
benefits of dense tactile feedback and differentiable physics. Code and
supplementary materials are available at the project website
https://difftactile.github.io/.",0
SOTOPIA-$π$: Interactive Learning of Socially Intelligent Language Agents,2403.08715v2,http://arxiv.org/abs/2403.08715v2,2024-03-13 17:17:48+00:00,"Humans learn social skills through both imitation and social interaction.
This social learning process is largely understudied by existing research on
building language agents. Motivated by this gap, we propose an interactive
learning method, SOTOPIA-$\pi$, improving the social intelligence of language
agents. This method leverages behavior cloning and self-reinforcement training
on filtered social interaction data according to large language model (LLM)
ratings. We show that our training method allows a 7B LLM to reach the social
goal completion ability of an expert model (GPT-4-based agent), while improving
the safety of language agents and maintaining general QA ability on the MMLU
benchmark. We also find that this training paradigm uncovers some difficulties
in LLM-based evaluation of social intelligence: LLM-based evaluators
overestimate the abilities of the language agents trained specifically for
social interaction.",0
Dynamic computerized tomography using inexact models and motion estimation,2403.08714v1,http://arxiv.org/abs/2403.08714v1,2024-03-13 17:17:36+00:00,"Reconstructing a dynamic object with affine motion in computerized tomography
(CT) leads to motion artifacts if the motion is not taken into account. In most
cases, the actual motion is neither known nor can be determined easily. As a
consequence, the respective model that describes CT is incomplete. The
iterative RESESOP-Kaczmarz method can - under certain conditions and by
exploiting the modeling error - reconstruct dynamic objects at different time
points even if the exact motion is unknown. However, the method is very
time-consuming. To speed the reconstruction process up and obtain better
results, we combine the following three steps: 1. RESESOP-Kacmarz with only a
few iterations is implemented to reconstruct the object at different time
points. 2. The motion is estimated via landmark detection, e.g. using deep
learning. 3. The estimated motion is integrated into the reconstruction
process, allowing the use of dynamic filtered backprojection. We give a short
review of all methods involved and present numerical results as a proof of
principle.",0
On the Stochasticity of Aerosol-Cloud Interactions within a Data-driven Framework,2403.08702v1,http://arxiv.org/abs/2403.08702v1,2024-03-13 17:06:52+00:00,"Aerosol-cloud interactions (ACI) pose the largest uncertainty for climate
projections. Among many challenges of understanding ACI, the question of
whether ACI is deterministic or stochastic has not been explicitly formulated
and asked. Here we attempt to answer this question by predicting cloud droplet
number concentration Nc from aerosol number concentration Na and ambient
conditions. We use aerosol properties, vertical velocity fluctuation w', and
meteorological states (temperature T and water vapor mixing ratio q_v) from the
ACTIVATE field observations (2020 to 2022) as predictor variables to estimate
Nc. We show that the climatological Nc can be successfully predicted using a
machine learning model despite the strongly nonlinear and multi-scale nature of
ACI. However, the observation-trained machine learning model fails to predict
Nc in individual cases while it successfully predicts Nc of randomly selected
data points that cover a broad spatiotemporal scale, suggesting the stochastic
nature of ACI at fine spatiotemporal scales.",0
Diffusion-based Iterative Counterfactual Explanations for Fetal Ultrasound Image Quality Assessment,2403.08700v1,http://arxiv.org/abs/2403.08700v1,2024-03-13 17:04:56+00:00,"Obstetric ultrasound image quality is crucial for accurate diagnosis and
monitoring of fetal health. However, producing high-quality standard planes is
difficult, influenced by the sonographer's expertise and factors like the
maternal BMI or the fetus dynamics. In this work, we propose using
diffusion-based counterfactual explainable AI to generate realistic
high-quality standard planes from low-quality non-standard ones. Through
quantitative and qualitative evaluation, we demonstrate the effectiveness of
our method in producing plausible counterfactuals of increased quality. This
shows future promise both for enhancing training of clinicians by providing
visual feedback, as well as for improving image quality and, consequently,
downstream diagnosis and monitoring.",0
Implicit Regularization of Gradient Flow on One-Layer Softmax Attention,2403.08699v1,http://arxiv.org/abs/2403.08699v1,2024-03-13 17:02:27+00:00,"We study gradient flow on the exponential loss for a classification problem
with a one-layer softmax attention model, where the key and query weight
matrices are trained separately. Under a separability assumption on the data,
we show that when gradient flow achieves the minimal loss value, it further
implicitly minimizes the nuclear norm of the product of the key and query
weight matrices. Such implicit regularization can be described by a Support
Vector Machine (SVM) problem with respect to the attention weights. This
finding contrasts with prior results showing that the gradient descent induces
an implicit regularization on the Frobenius norm on the product weight matrix
when the key and query matrices are combined into a single weight matrix for
training. For diagonal key and query matrices, our analysis builds upon the
reparameterization technique and exploits approximate KKT conditions of the SVM
associated with the classification data. Moreover, the results are extended to
general weights configurations given proper alignment of the weight matrices'
singular spaces with the data features at initialization.",0
Deep Learning for In-Orbit Cloud Segmentation and Classification in Hyperspectral Satellite Data,2403.08695v1,http://arxiv.org/abs/2403.08695v1,2024-03-13 16:58:37+00:00,"This article explores the latest Convolutional Neural Networks (CNNs) for
cloud detection aboard hyperspectral satellites. The performance of the latest
1D CNN (1D-Justo-LiuNet) and two recent 2D CNNs (nnU-net and
2D-Justo-UNet-Simple) for cloud segmentation and classification is assessed.
Evaluation criteria include precision and computational efficiency for in-orbit
deployment. Experiments utilize NASA's EO-1 Hyperion data, with varying
spectral channel numbers after Principal Component Analysis. Results indicate
that 1D-Justo-LiuNet achieves the highest accuracy, outperforming 2D CNNs,
while maintaining compactness with larger spectral channel sets, albeit with
increased inference times. However, the performance of 1D CNN degrades with
significant channel reduction. In this context, the 2D-Justo-UNet-Simple offers
the best balance for in-orbit deployment, considering precision, memory, and
time costs. While nnU-net is suitable for on-ground processing, deployment of
lightweight 1D-Justo-LiuNet is recommended for high-precision applications.
Alternatively, lightweight 2D-Justo-UNet-Simple is recommended for balanced
costs between timing and precision in orbit.",0
TeaMs-RL: Teaching LLMs to Teach Themselves Better Instructions via Reinforcement Learning,2403.08694v1,http://arxiv.org/abs/2403.08694v1,2024-03-13 16:57:57+00:00,"The development of Large Language Models (LLMs) often confronts challenges
stemming from the heavy reliance on human annotators in the reinforcement
learning with human feedback (RLHF) framework, or the frequent and costly
external queries tied to the self-instruct paradigm. In this work, we pivot to
Reinforcement Learning (RL) -- but with a twist. Diverging from the typical
RLHF, which refines LLMs following instruction data training, we use RL to
directly generate the foundational instruction dataset that alone suffices for
fine-tuning. Our method, TeaMs-RL, uses a suite of textual operations and
rules, prioritizing the diversification of training datasets. It facilitates
the generation of high-quality data without excessive reliance on external
advanced models, paving the way for a single fine-tuning step and negating the
need for subsequent RLHF stages. Our findings highlight key advantages of our
approach: reduced need for human involvement and fewer model queries (only
$5.73\%$ of WizardLM's total), along with enhanced capabilities of LLMs in
crafting and comprehending complex instructions compared to strong baselines,
and substantially improved model privacy protection.",0
FocusMAE: Gallbladder Cancer Detection from Ultrasound Videos with Focused Masked Autoencoders,2403.08848v1,http://arxiv.org/abs/2403.08848v1,2024-03-13 16:57:04+00:00,"In recent years, automated Gallbladder Cancer (GBC) detection has gained the
attention of researchers. Current state-of-the-art (SOTA) methodologies relying
on ultrasound sonography (US) images exhibit limited generalization,
emphasizing the need for transformative approaches. We observe that individual
US frames may lack sufficient information to capture disease manifestation.
This study advocates for a paradigm shift towards video-based GBC detection,
leveraging the inherent advantages of spatiotemporal representations. Employing
the Masked Autoencoder (MAE) for representation learning, we address
shortcomings in conventional image-based methods. We propose a novel design
called FocusMAE to systematically bias the selection of masking tokens from
high-information regions, fostering a more refined representation of
malignancy. Additionally, we contribute the most extensive US video dataset for
GBC detection. We also note that, this is the first study on US video-based GBC
detection. We validate the proposed methods on the curated dataset, and report
a new state-of-the-art (SOTA) accuracy of 96.4% for the GBC detection problem,
against an accuracy of 84% by current Image-based SOTA - GBCNet, and RadFormer,
and 94.7% by Video-based SOTA - AdaMAE. We further demonstrate the generality
of the proposed FocusMAE on a public CT-based Covid detection dataset,
reporting an improvement in accuracy by 3.3% over current baselines. The source
code and pretrained models are available at:
https://github.com/sbasu276/FocusMAE.",0
The Present-Day Mass Function of Star Clusters in the Solar Neighborhood,2403.08850v1,http://arxiv.org/abs/2403.08850v1,2024-03-13 18:00:00+00:00,"This work analyses the present-day mass function (PDMF) of 93~star clusters
utilizing Gaia DR3 data, with membership determined by the StarGo machine
learning algorithm. The impact of unresolved binary systems on mass estimation
is rigorously assessed, adopting three mass ratio profiles for correction. The
PDMF is characterized by the power-law index, $\alpha$, derived through a
robust maximum likelihood method that avoids biases associated with data
binning. The value of $\alpha$ for stars between the completeness limited mass
of Gaia with a mean 0.3 $M_\odot$ for our cluster samples and 2 $M_\odot$,
exhibits stability for clusters younger than 200 Myr, decreasing for older
clusters, particularly when considering stars within the half-mass radius. The
PDMF of these star clusters is consistent with a dynamically evolved Kroupa IMF
via the loss of low-mass stars. Cluster morphology shows a correlation with
$\alpha$, as $\alpha$ values exhibit a decreasing trend from filamentary to
tidal-tail clusters, mirroring the sequence of increasing cluster age. The
dependence of $\alpha$ on total cluster mass is weak, with a subtle increase
for higher-mass clusters, especially outside the half-mass radius. We do not
observe a correlation between $\alpha$ and the mean metallicity of the
clusters. Younger clusters have lower metallicity compared to their older
counterparts, which indicates that the older clusters might have migrated to
the solar neighbourhood from the inner disk. A comparison with numerical models
incorporating a black hole population suggests the need for observations of
distant, older, massive open clusters to determine whether or not they contain
black holes.",0
Hotspots and Photon Rings in Schwarzschild Black Hole Spacetimes,2403.08862v1,http://arxiv.org/abs/2403.08862v1,2024-03-13 18:00:02+00:00,"Future black hole (BH) imaging observations with better sensitivity and
angular resolution are expected to resolve finer features corresponding to
higher-order images of both hotspots that are produced in the accretion flow,
as well as of the entire emitting region. In spherically symmetric spacetimes,
the image order is determined by the maximum number of half-loops executed
around the BH by the photons that form it. Due to the additional half-loop,
consecutive-order images arrive after a delay time of approximately $\pi$ times
the BH shadow radius. Furthermore, the deviation of the diameters from that of
the shadow, widths, and flux-densities of consecutive-order images are
exponentially demagnified by the lensing Lyapunov exponent, a characteristic of
the spacetime. We compare the exact time delay between the appearance of the
zeroth and first-order images of a hotspot to our best analytic estimate and
find an error $\lesssim 50\%$ for hotspot locations within $\approx 10M$ from a
Schwarzschild BH of mass $M$. We also explore the possibilities for inferring
kinetic properties of hotspots, and also our inclination, from future BH
movies. Furthermore, since the targets of such observations host
geometrically-thick accretion flows (also jets), we obtain simple theoretical
estimates of the variation in the diameters and widths of their first-order
images, for varying disk scale-heights. We find realistically that the
deviation of the former from that of the shadow is $\lesssim 30\%$ and that the
latter remain $\lesssim 1.3M$. Finally, we estimate the error in recovering the
lensing exponent, when using the first and second-order images, to be $\lesssim
20\%$. This provides further evidence that future observations could yield new
and independent estimates of the shadow size and, in principle, of the lensing
exponent, allowing us to robustly learn about the spacetimes of astrophysical
BHs.",0
"Architectural Implications of Neural Network Inference for High Data-Rate, Low-Latency Scientific Applications",2403.08980v1,http://arxiv.org/abs/2403.08980v1,2024-03-13 22:10:42+00:00,"With more scientific fields relying on neural networks (NNs) to process data
incoming at extreme throughputs and latencies, it is crucial to develop NNs
with all their parameters stored on-chip. In many of these applications, there
is not enough time to go off-chip and retrieve weights. Even more so, off-chip
memory such as DRAM does not have the bandwidth required to process these NNs
as fast as the data is being produced (e.g., every 25 ns). As such, these
extreme latency and bandwidth requirements have architectural implications for
the hardware intended to run these NNs: 1) all NN parameters must fit on-chip,
and 2) codesigning custom/reconfigurable logic is often required to meet these
latency and bandwidth constraints. In our work, we show that many scientific NN
applications must run fully on chip, in the extreme case requiring a custom
chip to meet such stringent constraints.",0
Multi-Objective Optimization Using Adaptive Distributed Reinforcement Learning,2403.08879v1,http://arxiv.org/abs/2403.08879v1,2024-03-13 18:05:16+00:00,"The Intelligent Transportation System (ITS) environment is known to be
dynamic and distributed, where participants (vehicle users, operators, etc.)
have multiple, changing and possibly conflicting objectives. Although
Reinforcement Learning (RL) algorithms are commonly applied to optimize ITS
applications such as resource management and offloading, most RL algorithms
focus on single objectives. In many situations, converting a multi-objective
problem into a single-objective one is impossible, intractable or insufficient,
making such RL algorithms inapplicable. We propose a multi-objective,
multi-agent reinforcement learning (MARL) algorithm with high learning
efficiency and low computational requirements, which automatically triggers
adaptive few-shot learning in a dynamic, distributed and noisy environment with
sparse and delayed reward. We test our algorithm in an ITS environment with
edge cloud computing. Empirical results show that the algorithm is quick to
adapt to new environments and performs better in all individual and system
metrics compared to the state-of-the-art benchmark. Our algorithm also
addresses various practical concerns with its modularized and asynchronous
online training method. In addition to the cloud simulation, we test our
algorithm on a single-board computer and show that it can make inference in 6
milliseconds.",0
7T MRI Synthesization from 3T Acquisitions,2403.08979v1,http://arxiv.org/abs/2403.08979v1,2024-03-13 22:06:44+00:00,"Supervised deep learning techniques can be used to generate synthetic 7T MRIs
from 3T MRI inputs. This image enhancement process leverages the advantages of
ultra-high-field MRI to improve the signal-to-noise and contrast-to-noise
ratios of 3T acquisitions. In this paper, we introduce multiple novel 7T
synthesization algorithms based on custom-designed variants of the V-Net
convolutional neural network. We demonstrate that the V-Net based model has
superior performance in enhancing both single-site and multi-site MRI datasets
compared to the existing benchmark model. When trained on 3T-7T MRI pairs from
8 subjects with mild Traumatic Brain Injury (TBI), our model achieves
state-of-the-art 7T synthesization performance. Compared to previous works,
synthetic 7T images generated from our pipeline also display superior
enhancement of pathological tissue. Additionally, we implement and test a data
augmentation scheme for training models that are robust to variations in the
input distribution. This allows synthetic 7T models to accommodate
intra-scanner and inter-scanner variability in multisite datasets. On a
harmonized dataset consisting of 18 3T-7T MRI pairs from two institutions,
including both healthy subjects and those with mild TBI, our model maintains
its performance and can generalize to 3T MRI inputs with lower resolution. Our
findings demonstrate the promise of V-Net based models for MRI enhancement and
offer a preliminary probe into improving the generalizability of synthetic 7T
models with data augmentation.",0
AutoGuide: Automated Generation and Selection of State-Aware Guidelines for Large Language Model Agents,2403.08978v1,http://arxiv.org/abs/2403.08978v1,2024-03-13 22:06:03+00:00,"The primary limitation of large language models (LLMs) is their restricted
understanding of the world. This poses significant difficulties for LLM-based
agents, particularly in domains where pre-trained LLMs lack sufficient
knowledge. In this paper, we introduce a novel framework, called AutoGuide,
that bridges the knowledge gap in pre-trained LLMs by leveraging implicit
knowledge in offline experiences. Specifically, AutoGuide effectively extracts
knowledge embedded in offline data by extracting a set of state-aware
guidelines. Importantly, each state-aware guideline is expressed in concise
natural language and follows a conditional structure, clearly describing the
state where it is applicable. As such, the resulting guidelines enable a
principled way to provide helpful knowledge pertinent to an agent's current
decision-making process. We show that our approach outperforms competitive
LLM-based baselines by a large margin in sequential decision-making benchmarks.",0
The Full-scale Assembly Simulation Testbed (FAST) Dataset,2403.08969v1,http://arxiv.org/abs/2403.08969v1,2024-03-13 21:30:01+00:00,"In recent years, numerous researchers have begun investigating how virtual
reality (VR) tracking and interaction data can be used for a variety of machine
learning purposes, including user identification, predicting cybersickness, and
estimating learning gains. One constraint for this research area is the dearth
of open datasets. In this paper, we present a new open dataset captured with
our VR-based Full-scale Assembly Simulation Testbed (FAST). This dataset
consists of data collected from 108 participants (50 females, 56 males, 2
non-binary) learning how to assemble two distinct full-scale structures in VR.
In addition to explaining how the dataset was collected and describing the data
included, we discuss how the dataset may be used by future researchers.",0
Domain Adaptation for Dense Retrieval and Conversational Dense Retrieval through Self-Supervision by Meticulous Pseudo-Relevance Labeling,2403.08970v1,http://arxiv.org/abs/2403.08970v1,2024-03-13 21:30:01+00:00,"Recent studies have demonstrated that the ability of dense retrieval models
to generalize to target domains with different distributions is limited, which
contrasts with the results obtained with interaction-based models. Prior
attempts to mitigate this challenge involved leveraging adversarial learning
and query generation approaches, but both approaches nevertheless resulted in
limited improvements. In this paper, we propose to combine the query-generation
approach with a self-supervision approach in which pseudo-relevance labels are
automatically generated on the target domain. To accomplish this, a T5-3B model
is utilized for pseudo-positive labeling, and meticulous hard negatives are
chosen. We also apply this strategy on conversational dense retrieval model for
conversational search. A similar pseudo-labeling approach is used, but with the
addition of a query-rewriting module to rewrite conversational queries for
subsequent labeling. This proposed approach enables a model's domain adaptation
with real queries and documents from the target dataset. Experiments on
standard dense retrieval and conversational dense retrieval models both
demonstrate improvements on baseline models when they are fine-tuned on the
pseudo-relevance labeled data.",0
PathM3: A Multimodal Multi-Task Multiple Instance Learning Framework for Whole Slide Image Classification and Captioning,2403.08967v1,http://arxiv.org/abs/2403.08967v1,2024-03-13 21:19:12+00:00,"In the field of computational histopathology, both whole slide images (WSIs)
and diagnostic captions provide valuable insights for making diagnostic
decisions. However, aligning WSIs with diagnostic captions presents a
significant challenge. This difficulty arises from two main factors: 1)
Gigapixel WSIs are unsuitable for direct input into deep learning models, and
the redundancy and correlation among the patches demand more attention; and 2)
Authentic WSI diagnostic captions are extremely limited, making it difficult to
train an effective model. To overcome these obstacles, we present PathM3, a
multimodal, multi-task, multiple instance learning (MIL) framework for WSI
classification and captioning. PathM3 adapts a query-based transformer to
effectively align WSIs with diagnostic captions. Given that histopathology
visual patterns are redundantly distributed across WSIs, we aggregate each
patch feature with MIL method that considers the correlations among instances.
Furthermore, our PathM3 overcomes data scarcity in WSI-level captions by
leveraging limited WSI diagnostic caption data in the manner of multi-task
joint learning. Extensive experiments with improved classification accuracy and
caption generation demonstrate the effectiveness of our method on both WSI
classification and captioning task.",0
Deep Learning Based Dynamics Identification and Linearization of Orbital Problems using Koopman Theory,2403.08965v1,http://arxiv.org/abs/2403.08965v1,2024-03-13 21:11:58+00:00,"The study of the Two-Body and Circular Restricted Three-Body Problems in the
field of aerospace engineering and sciences is deeply important because they
help describe the motion of both celestial and artificial satellites. With the
growing demand for satellites and satellite formation flying, fast and
efficient control of these systems is becoming ever more important. Global
linearization of these systems allows engineers to employ methods of control in
order to achieve these desired results. We propose a data-driven framework for
simultaneous system identification and global linearization of both the
Two-Body Problem and Circular Restricted Three-Body Problem via deep
learning-based Koopman Theory, i.e., a framework that can identify the
underlying dynamics and globally linearize it into a linear time-invariant
(LTI) system. The linear Koopman operator is discovered through purely
data-driven training of a Deep Neural Network with a custom architecture. This
paper displays the ability of the Koopman operator to generalize to various
other Two-Body systems without the need for retraining. We also demonstrate the
capability of the same architecture to be utilized to accurately learn a
Koopman operator that approximates the Circular Restricted Three-Body Problem.",0
Using Deep Learning for Morphological Classification in Pigs with a Focus on Sanitary Monitoring,2403.08962v1,http://arxiv.org/abs/2403.08962v1,2024-03-13 21:05:34+00:00,"The aim of this paper is to evaluate the use of D-CNN (Deep Convolutional
Neural Networks) algorithms to classify pig body conditions in normal or not
normal conditions, with a focus on characteristics that are observed in
sanitary monitoring, and were used six different algorithms to do this task.
The study focused on five pig characteristics, being these caudophagy, ear
hematoma, scratches on the body, redness, and natural stains (brown or black).
The results of the study showed that D-CNN was effective in classifying
deviations in pig body morphologies related to skin characteristics. The
evaluation was conducted by analyzing the performance metrics Precision,
Recall, and F-score, as well as the statistical analyses ANOVA and the
Scott-Knott test. The contribution of this article is characterized by the
proposal of using D-CNN networks for morphological classification in pigs, with
a focus on characteristics identified in sanitary monitoring. Among the best
results, the average Precision metric of 80.6\% to classify caudophagy was
achieved for the InceptionResNetV2 network, indicating the potential use of
this technology for the proposed task. Additionally, a new image database was
created, containing various pig's distinct body characteristics, which can
serve as data for future research.",0
scVGAE: A Novel Approach using ZINB-Based Variational Graph Autoencoder for Single-Cell RNA-Seq Imputation,2403.08959v1,http://arxiv.org/abs/2403.08959v1,2024-03-13 20:57:10+00:00,"Single-cell RNA sequencing (scRNA-seq) has revolutionized our ability to
study individual cellular distinctions and uncover unique cell characteristics.
However, a significant technical challenge in scRNA-seq analysis is the
occurrence of ""dropout"" events, where certain gene expressions cannot be
detected. This issue is particularly pronounced in genes with low or sparse
expression levels, impacting the precision and interpretability of the obtained
data. To address this challenge, various imputation methods have been
implemented to predict such missing values, aiming to enhance the analysis's
accuracy and usefulness. A prevailing hypothesis posits that scRNA-seq data
conforms to a zero-inflated negative binomial (ZINB) distribution.
Consequently, methods have been developed to model the data according to this
distribution. Recent trends in scRNA-seq analysis have seen the emergence of
deep learning approaches. Some techniques, such as the variational autoencoder,
incorporate the ZINB distribution as a model loss function. Graph-based methods
like Graph Convolutional Networks (GCN) and Graph Attention Networks (GAT) have
also gained attention as deep learning methodologies for scRNA-seq analysis.
This study introduces scVGAE, an innovative approach integrating GCN into a
variational autoencoder framework while utilizing a ZINB loss function. This
integration presents a promising avenue for effectively addressing dropout
events in scRNA-seq data, thereby enhancing the accuracy and reliability of
downstream analyses. scVGAE outperforms other methods in cell clustering, with
the best performance in 11 out of 14 datasets. Ablation study shows all
components of scVGAE are necessary. scVGAE is implemented in Python and
downloadable at https://github.com/inoue0426/scVGAE.",0
Towards Efficient Risk-Sensitive Policy Gradient: An Iteration Complexity Analysis,2403.08955v1,http://arxiv.org/abs/2403.08955v1,2024-03-13 20:50:49+00:00,"Reinforcement Learning (RL) has shown exceptional performance across various
applications, enabling autonomous agents to learn optimal policies through
interaction with their environments. However, traditional RL frameworks often
face challenges in terms of iteration complexity and robustness. Risk-sensitive
RL, which balances expected return and risk, has been explored for its
potential to yield probabilistically robust policies, yet its iteration
complexity analysis remains underexplored. In this study, we conduct a thorough
iteration complexity analysis for the risk-sensitive policy gradient method,
focusing on the REINFORCE algorithm and employing the exponential utility
function. We obtain an iteration complexity of $\mathcal{O}(\epsilon^{-2})$ to
reach an $\epsilon$-approximate first-order stationary point (FOSP). We
investigate whether risk-sensitive algorithms can achieve better iteration
complexity compared to their risk-neutral counterparts. Our theoretical
analysis demonstrates that risk-sensitive REINFORCE can have a reduced number
of iterations required for convergence. This leads to improved iteration
complexity, as employing the exponential utility does not entail additional
computation per iteration. We characterize the conditions under which
risk-sensitive algorithms can achieve better iteration complexity. Our
simulation results also validate that risk-averse cases can converge and
stabilize more quickly after approximately half of the episodes compared to
their risk-neutral counterparts.",0
Robust COVID-19 Detection in CT Images with CLIP,2403.08947v1,http://arxiv.org/abs/2403.08947v1,2024-03-13 20:26:50+00:00,"In the realm of medical imaging, particularly for COVID-19 detection, deep
learning models face substantial challenges such as the necessity for extensive
computational resources, the paucity of well-annotated datasets, and a
significant amount of unlabeled data. In this work, we introduce the first
lightweight detector designed to overcome these obstacles, leveraging a frozen
CLIP image encoder and a trainable multilayer perception (MLP). Enhanced with
Conditional Value at Risk (CVaR) for robustness and a loss landscape flattening
strategy for improved generalization, our model is tailored for high efficacy
in COVID-19 detection. Furthermore, we integrate a teacher-student framework to
capitalize on the vast amounts of unlabeled data, enabling our model to achieve
superior performance despite the inherent data limitations. Experimental
results on the COV19-CT-DB dataset demonstrate the effectiveness of our
approach, surpassing baseline by up to 10.6% in `macro' F1 score in supervised
learning. The code is available at
https://github.com/Purdue-M2/COVID-19_Detection_M2_PURDUE.",0
Usable XAI: 10 Strategies Towards Exploiting Explainability in the LLM Era,2403.08946v1,http://arxiv.org/abs/2403.08946v1,2024-03-13 20:25:27+00:00,"Explainable AI (XAI) refers to techniques that provide human-understandable
insights into the workings of AI models. Recently, the focus of XAI is being
extended towards Large Language Models (LLMs) which are often criticized for
their lack of transparency. This extension calls for a significant
transformation in XAI methodologies because of two reasons. First, many
existing XAI methods cannot be directly applied to LLMs due to their complexity
advanced capabilities. Second, as LLMs are increasingly deployed across diverse
industry applications, the role of XAI shifts from merely opening the ""black
box"" to actively enhancing the productivity and applicability of LLMs in
real-world settings. Meanwhile, unlike traditional machine learning models that
are passive recipients of XAI insights, the distinct abilities of LLMs can
reciprocally enhance XAI. Therefore, in this paper, we introduce Usable XAI in
the context of LLMs by analyzing (1) how XAI can benefit LLMs and AI systems,
and (2) how LLMs can contribute to the advancement of XAI. We introduce 10
strategies, introducing the key techniques for each and discussing their
associated challenges. We also provide case studies to demonstrate how to
obtain and leverage explanations. The code used in this paper can be found at:
https://github.com/JacksonWuxs/UsableXAI_LLM.",0
Towards Model-Agnostic Posterior Approximation for Fast and Accurate Variational Autoencoders,2403.08941v1,http://arxiv.org/abs/2403.08941v1,2024-03-13 20:16:21+00:00,"Inference for Variational Autoencoders (VAEs) consists of learning two
models: (1) a generative model, which transforms a simple distribution over a
latent space into the distribution over observed data, and (2) an inference
model, which approximates the posterior of the latent codes given data. The two
components are learned jointly via a lower bound to the generative model's log
marginal likelihood. In early phases of joint training, the inference model
poorly approximates the latent code posteriors. Recent work showed that this
leads optimization to get stuck in local optima, negatively impacting the
learned generative model. As such, recent work suggests ensuring a high-quality
inference model via iterative training: maximizing the objective function
relative to the inference model before every update to the generative model.
Unfortunately, iterative training is inefficient, requiring heuristic criteria
for reverting from iterative to joint training for speed. Here, we suggest an
inference method that trains the generative and inference models independently.
It approximates the posterior of the true model a priori; fixing this posterior
approximation, we then maximize the lower bound relative to only the generative
model. By conventional wisdom, this approach should rely on the true prior and
likelihood of the true model to approximate its posterior (which are unknown).
However, we show that we can compute a deterministic, model-agnostic posterior
approximation (MAPA) of the true model's posterior. We then use MAPA to develop
a proof-of-concept inference method. We present preliminary results on
low-dimensional synthetic data that (1) MAPA captures the trend of the true
posterior, and (2) our MAPA-based inference performs better density estimation
with less computation than baselines. Lastly, we present a roadmap for scaling
the MAPA-based inference method to high-dimensional data.",0
FogGuard: guarding YOLO against fog using perceptual loss,2403.08939v1,http://arxiv.org/abs/2403.08939v1,2024-03-13 20:13:25+00:00,"In this paper, we present a novel fog-aware object detection network called
FogGuard, designed to address the challenges posed by foggy weather conditions.
Autonomous driving systems heavily rely on accurate object detection
algorithms, but adverse weather conditions can significantly impact the
reliability of deep neural networks (DNNs).
  Existing approaches fall into two main categories, 1) image enhancement such
as IA-YOLO 2) domain adaptation based approaches. Image enhancement based
techniques attempt to generate fog-free image. However, retrieving a fogless
image from a foggy image is a much harder problem than detecting objects in a
foggy image. Domain-adaptation based approaches, on the other hand, do not make
use of labelled datasets in the target domain. Both categories of approaches
are attempting to solve a harder version of the problem. Our approach builds
over fine-tuning on the
  Our framework is specifically designed to compensate for foggy conditions
present in the scene, ensuring robust performance even. We adopt YOLOv3 as the
baseline object detection algorithm and introduce a novel Teacher-Student
Perceptual loss, to high accuracy object detection in foggy images.
  Through extensive evaluations on common datasets such as PASCAL VOC and RTTS,
we demonstrate the improvement in performance achieved by our network. We
demonstrate that FogGuard achieves 69.43\% mAP, as compared to 57.78\% for
YOLOv3 on the RTTS dataset.
  Furthermore, we show that while our training method increases time
complexity, it does not introduce any additional overhead during inference
compared to the regular YOLO network.",0
"A non-asymptotic theory of Kernel Ridge Regression: deterministic equivalents, test error, and GCV estimator",2403.08938v1,http://arxiv.org/abs/2403.08938v1,2024-03-13 20:12:03+00:00,"We consider learning an unknown target function $f_*$ using kernel ridge
regression (KRR) given i.i.d. data $(u_i,y_i)$, $i\leq n$, where $u_i \in U$ is
a covariate vector and $y_i = f_* (u_i) +\varepsilon_i \in \mathbb{R}$. A
recent string of work has empirically shown that the test error of KRR can be
well approximated by a closed-form estimate derived from an `equivalent'
sequence model that only depends on the spectrum of the kernel operator.
However, a theoretical justification for this equivalence has so far relied
either on restrictive assumptions -- such as subgaussian independent
eigenfunctions -- , or asymptotic derivations for specific kernels in high
dimensions.
  In this paper, we prove that this equivalence holds for a general class of
problems satisfying some spectral and concentration properties on the kernel
eigendecomposition. Specifically, we establish in this setting a non-asymptotic
deterministic approximation for the test error of KRR -- with explicit
non-asymptotic bounds -- that only depends on the eigenvalues and the target
function alignment to the eigenvectors of the kernel. Our proofs rely on a
careful derivation of deterministic equivalents for random matrix functionals
in the dimension free regime pioneered by Cheng and Montanari (2022).
  We apply this setting to several classical examples and show an excellent
agreement between theoretical predictions and numerical simulations. These
results rely on having access to the eigendecomposition of the kernel operator.
Alternatively, we prove that, under this same setting, the generalized
cross-validation (GCV) estimator concentrates on the test error uniformly over
a range of ridge regularization parameter that includes zero (the interpolating
solution). As a consequence, the GCV estimator can be used to estimate from
data the test error and optimal regularization parameter for KRR.",0
Beyond Joint Demonstrations: Personalized Expert Guidance for Efficient Multi-Agent Reinforcement Learning,2403.08936v1,http://arxiv.org/abs/2403.08936v1,2024-03-13 20:11:20+00:00,"Multi-Agent Reinforcement Learning (MARL) algorithms face the challenge of
efficient exploration due to the exponential increase in the size of the joint
state-action space. While demonstration-guided learning has proven beneficial
in single-agent settings, its direct applicability to MARL is hindered by the
practical difficulty of obtaining joint expert demonstrations. In this work, we
introduce a novel concept of personalized expert demonstrations, tailored for
each individual agent or, more broadly, each individual type of agent within a
heterogeneous team. These demonstrations solely pertain to single-agent
behaviors and how each agent can achieve personal goals without encompassing
any cooperative elements, thus naively imitating them will not achieve
cooperation due to potential conflicts. To this end, we propose an approach
that selectively utilizes personalized expert demonstrations as guidance and
allows agents to learn to cooperate, namely personalized expert-guided MARL
(PegMARL). This algorithm utilizes two discriminators: the first provides
incentives based on the alignment of policy behavior with demonstrations, and
the second regulates incentives based on whether the behavior leads to the
desired objective. We evaluate PegMARL using personalized demonstrations in
both discrete and continuous environments. The results demonstrate that PegMARL
learns near-optimal policies even when provided with suboptimal demonstrations,
and outperforms state-of-the-art MARL algorithms in solving coordinated tasks.
We also showcase PegMARL's capability to leverage joint demonstrations in the
StarCraft scenario and converge effectively even with demonstrations from
non-co-trained policies.",0
Neuromorphic force-control in an industrial task: validating energy and latency benefits,2403.08928v1,http://arxiv.org/abs/2403.08928v1,2024-03-13 19:36:03+00:00,"As robots become smarter and more ubiquitous, optimizing the power
consumption of intelligent compute becomes imperative towards ensuring the
sustainability of technological advancements. Neuromorphic computing hardware
makes use of biologically inspired neural architectures to achieve energy and
latency improvements compared to conventional von Neumann computing
architecture. Applying these benefits to robots has been demonstrated in
several works in the field of neurorobotics, typically on relatively simple
control tasks. Here, we introduce an example of neuromorphic computing applied
to the real-world industrial task of object insertion. We trained a spiking
neural network (SNN) to perform force-torque feedback control using a
reinforcement learning approach in simulation. We then ported the SNN to the
Intel neuromorphic research chip Loihi interfaced with a KUKA robotic arm. At
inference time we show latency competitive with current CPU/GPU architectures,
two orders of magnitude less energy usage in comparison to traditional
low-energy edge-hardware. We offer this example as a proof of concept
implementation of a neuromoprhic controller in real-world robotic setting,
highlighting the benefits of neuromorphic hardware for the development of
intelligent controllers for robots.",0
Principal stratification with U-statistics under principal ignorability,2403.08927v1,http://arxiv.org/abs/2403.08927v1,2024-03-13 19:33:27+00:00,"Principal stratification is a popular framework for causal inference in the
presence of an intermediate outcome. While the principal average treatment
effects have traditionally been the default target of inference, it may not be
sufficient when the interest lies in the relative favorability of one potential
outcome over the other within the principal stratum. We thus introduce the
principal generalized causal effect estimands, which extend the principal
average causal effects to accommodate nonlinear contrast functions. Under
principal ignorability, we expand the theoretical results in Jiang et. al.
(2022) to a much wider class of causal estimands in the presence of a binary
intermediate variable. We develop identification formulas and derive the
efficient influence functions of the generalized estimands for principal
stratification analyses. These efficient influence functions motivate a set of
multiply robust estimators and lay the ground for obtaining efficient debiased
machine learning estimators via cross-fitting based on $U$-statistics. The
proposed methods are illustrated through simulations and the analysis of a data
example.",0
On sampling diluted Spin Glasses using Glauber dynamics,2403.08921v1,http://arxiv.org/abs/2403.08921v1,2024-03-13 19:23:28+00:00,"Spin-glasses are Gibbs distributions that have been studied in CS for many
decades. Recently, they have gained renewed attention as they emerge naturally
in learning, inference, optimisation etc.
  We consider the Edwards-Anderson (EA) spin-glass distribution at inverse
temperature $\beta$ when the underlying graph is an instance of $G(n,d/n)$.
This is the random graph on $n$ vertices where each edge appears independently
with probability $d/n$ and $d=\Theta(1)$. We study the problem of approximate
sampling from this distribution using Glauber dynamics.
  For a range of $\beta$ that depends on $d$ and for typical instances of the
EA model on $G(n,d/n)$, we show that the corresponding Glauber dynamics
exhibits mixing time $O(n^{2+\frac{3}{\log^2 d}})$.
  The range of $\beta$ for which we obtain our rapid-mixing results correspond
to the expected influence being $<1/d$; we conjecture that this is the best
possible.
  Unlike the mean-field spin-glasses, where the problem has been studied
before, the diluted case has not.
  We utilise the well-known path-coupling technique. In the standard Glauber
dynamics on $G(n,d/n)$, one has to deal with the so-called effect of high
degree vertices. Here, rather than considering degrees, it is more natural to
use a different measure on the vertices called aggregate influence.
  We build on the block-construction approach proposed by [Dyer et al. 2006] to
circumvent the problem of high-degree vertices. Specifically, we first
establish rapid mixing for an appropriately defined block-dynamics. We design
this dynamics such that vertices of large aggregate influence are placed deep
inside their blocks. Then, we obtain rapid mixing for the Glauber dynamics
utilising a comparison argument.",0
CLIP-BEVFormer: Enhancing Multi-View Image-Based BEV Detector with Ground Truth Flow,2403.08919v1,http://arxiv.org/abs/2403.08919v1,2024-03-13 19:21:03+00:00,"Autonomous driving stands as a pivotal domain in computer vision, shaping the
future of transportation. Within this paradigm, the backbone of the system
plays a crucial role in interpreting the complex environment. However, a
notable challenge has been the loss of clear supervision when it comes to
Bird's Eye View elements. To address this limitation, we introduce
CLIP-BEVFormer, a novel approach that leverages the power of contrastive
learning techniques to enhance the multi-view image-derived BEV backbones with
ground truth information flow. We conduct extensive experiments on the
challenging nuScenes dataset and showcase significant and consistent
improvements over the SOTA. Specifically, CLIP-BEVFormer achieves an impressive
8.5\% and 9.2\% enhancement in terms of NDS and mAP, respectively, over the
previous best BEV model on the 3D object detection task.",0
Efficiently Computing Similarities to Private Datasets,2403.08917v1,http://arxiv.org/abs/2403.08917v1,2024-03-13 19:19:19+00:00,"Many methods in differentially private model training rely on computing the
similarity between a query point (such as public or synthetic data) and private
data. We abstract out this common subroutine and study the following
fundamental algorithmic problem: Given a similarity function $f$ and a large
high-dimensional private dataset $X \subset \mathbb{R}^d$, output a
differentially private (DP) data structure which approximates $\sum_{x \in X}
f(x,y)$ for any query $y$. We consider the cases where $f$ is a kernel
function, such as $f(x,y) = e^{-\|x-y\|_2^2/\sigma^2}$ (also known as DP kernel
density estimation), or a distance function such as $f(x,y) = \|x-y\|_2$, among
others.
  Our theoretical results improve upon prior work and give better
privacy-utility trade-offs as well as faster query times for a wide range of
kernels and distance functions. The unifying approach behind our results is
leveraging `low-dimensional structures' present in the specific functions $f$
that we study, using tools such as provable dimensionality reduction,
approximation theory, and one-dimensional decomposition of the functions. Our
algorithms empirically exhibit improved query times and accuracy over prior
state of the art. We also present an application to DP classification. Our
experiments demonstrate that the simple methodology of classifying based on
average similarity is orders of magnitude faster than prior DP-SGD based
approaches for comparable accuracy.",0
Cross-Modal Learning of Housing Quality in Amsterdam,2403.08915v1,http://arxiv.org/abs/2403.08915v1,2024-03-13 19:11:58+00:00,"In our research we test data and models for the recognition of housing
quality in the city of Amsterdam from ground-level and aerial imagery. For
ground-level images we compare Google StreetView (GSV) to Flickr images. Our
results show that GSV predicts the most accurate building quality scores,
approximately 30% better than using only aerial images. However, we find that
through careful filtering and by using the right pre-trained model, Flickr
image features combined with aerial image features are able to halve the
performance gap to GSV features from 30% to 15%. Our results indicate that
there are viable alternatives to GSV for liveability factor prediction, which
is encouraging as GSV images are more difficult to acquire and not always
available.",0
Meta-operators for Enabling Parallel Planning Using Deep Reinforcement Learning,2403.08910v1,http://arxiv.org/abs/2403.08910v1,2024-03-13 19:00:36+00:00,"There is a growing interest in the application of Reinforcement Learning (RL)
techniques to AI planning with the aim to come up with general policies.
Typically, the mapping of the transition model of AI planning to the state
transition system of a Markov Decision Process is established by assuming a
one-to-one correspondence of the respective action spaces. In this paper, we
introduce the concept of meta-operator as the result of simultaneously applying
multiple planning operators, and we show that including meta-operators in the
RL action space enables new planning perspectives to be addressed using RL,
such as parallel planning. Our research aims to analyze the performance and
complexity of including meta-operators in the RL process, concretely in domains
where satisfactory outcomes have not been previously achieved using usual
generalized planning models. The main objective of this article is thus to pave
the way towards a redefinition of the RL action space in a manner that is more
closely aligned with the planning perspective.",0
Strategizing against Q-learners: A Control-theoretical Approach,2403.08906v1,http://arxiv.org/abs/2403.08906v1,2024-03-13 18:54:27+00:00,"In this paper, we explore the susceptibility of the Q-learning algorithm (a
classical and widely used reinforcement learning method) to strategic
manipulation of sophisticated opponents in games. We quantify how much a
strategically sophisticated agent can exploit a naive Q-learner if she knows
the opponent's Q-learning algorithm. To this end, we formulate the strategic
actor's problem as a Markov decision process (with a continuum state space
encompassing all possible Q-values) as if the Q-learning algorithm is the
underlying dynamical system. We also present a quantization-based approximation
scheme to tackle the continuum state space and analyze its performance both
analytically and numerically.",0
A Framework for Strategic Discovery of Credible Neural Network Surrogate Models under Uncertainty,2403.08901v1,http://arxiv.org/abs/2403.08901v1,2024-03-13 18:45:51+00:00,"The widespread integration of deep neural networks in developing data-driven
surrogate models for high-fidelity simulations of complex physical systems
highlights the critical necessity for robust uncertainty quantification
techniques and credibility assessment methodologies, ensuring the reliable
deployment of surrogate models in consequential decision-making. This study
presents the Occam Plausibility Algorithm for surrogate models
(OPAL-surrogate), providing a systematic framework to uncover predictive neural
network-based surrogate models within the large space of potential models,
including various neural network classes and choices of architecture and
hyperparameters. The framework is grounded in hierarchical Bayesian inferences
and employs model validation tests to evaluate the credibility and prediction
reliability of the surrogate models under uncertainty. Leveraging these
principles, OPAL-surrogate introduces a systematic and efficient strategy for
balancing the trade-off between model complexity, accuracy, and prediction
uncertainty. The effectiveness of OPAL-surrogate is demonstrated through two
modeling problems, including the deformation of porous materials for building
insulation and turbulent combustion flow for the ablation of solid fuels within
hybrid rocket motors.",0
One-Shot Averaging for Distributed TD($λ$) Under Markov Sampling,2403.08896v1,http://arxiv.org/abs/2403.08896v1,2024-03-13 18:37:16+00:00,"We consider a distributed setup for reinforcement learning, where each agent
has a copy of the same Markov Decision Process but transitions are sampled from
the corresponding Markov chain independently by each agent. We show that in
this setting, we can achieve a linear speedup for TD($\lambda$), a family of
popular methods for policy evaluation, in the sense that $N$ agents can
evaluate a policy $N$ times faster provided the target accuracy is small
enough. Notably, this speedup is achieved by ``one shot averaging,'' a
procedure where the agents run TD($\lambda$) with Markov sampling independently
and only average their results after the final step. This significantly reduces
the amount of communication required to achieve a linear speedup relative to
previous work.",0
Federated Data Model,2403.08887v1,http://arxiv.org/abs/2403.08887v1,2024-03-13 18:16:54+00:00,"In artificial intelligence (AI), especially deep learning, data diversity and
volume play a pivotal role in model development. However, training a robust
deep learning model often faces challenges due to data privacy, regulations,
and the difficulty of sharing data between different locations, especially for
medical applications. To address this, we developed a method called the
Federated Data Model (FDM). This method uses diffusion models to learn the
characteristics of data at one site and then creates synthetic data that can be
used at another site without sharing the actual data. We tested this approach
with a medical image segmentation task, focusing on cardiac magnetic resonance
images from different hospitals. Our results show that models trained with this
method perform well both on the data they were originally trained on and on
data from other sites. This approach offers a promising way to train accurate
and privacy-respecting AI models across different locations.",0
REFRESH: Responsible and Efficient Feature Reselection Guided by SHAP Values,2403.08880v1,http://arxiv.org/abs/2403.08880v1,2024-03-13 18:06:43+00:00,"Feature selection is a crucial step in building machine learning models. This
process is often achieved with accuracy as an objective, and can be cumbersome
and computationally expensive for large-scale datasets. Several additional
model performance characteristics such as fairness and robustness are of
importance for model development. As regulations are driving the need for more
trustworthy models, deployed models need to be corrected for model
characteristics associated with responsible artificial intelligence. When
feature selection is done with respect to one model performance characteristic
(eg. accuracy), feature selection with secondary model performance
characteristics (eg. fairness and robustness) as objectives would require going
through the computationally expensive selection process from scratch. In this
paper, we introduce the problem of feature \emph{reselection}, so that features
can be selected with respect to secondary model performance characteristics
efficiently even after a feature selection process has been done with respect
to a primary objective. To address this problem, we propose REFRESH, a method
to reselect features so that additional constraints that are desirable towards
model performance can be achieved without having to train several new models.
REFRESH's underlying algorithm is a novel technique using SHAP values and
correlation analysis that can approximate for the predictions of a model
without having to train these models. Empirical evaluations on three datasets,
including a large-scale loan defaulting dataset show that REFRESH can help find
alternate models with better model characteristics efficiently. We also discuss
the need for reselection and REFRESH based on regulation desiderata.",0
MCformer: Multivariate Time Series Forecasting with Mixed-Channels Transformer,2403.09223v1,http://arxiv.org/abs/2403.09223v1,2024-03-14 09:43:07+00:00,"The massive generation of time-series data by largescale Internet of Things
(IoT) devices necessitates the exploration of more effective models for
multivariate time-series forecasting. In previous models, there was a
predominant use of the Channel Dependence (CD) strategy (where each channel
represents a univariate sequence). Current state-of-the-art (SOTA) models
primarily rely on the Channel Independence (CI) strategy. The CI strategy
treats all channels as a single channel, expanding the dataset to improve
generalization performance and avoiding inter-channel correlation that disrupts
long-term features. However, the CI strategy faces the challenge of
interchannel correlation forgetting. To address this issue, we propose an
innovative Mixed Channels strategy, combining the data expansion advantages of
the CI strategy with the ability to counteract inter-channel correlation
forgetting. Based on this strategy, we introduce MCformer, a multivariate
time-series forecasting model with mixed channel features. The model blends a
specific number of channels, leveraging an attention mechanism to effectively
capture inter-channel correlation information when modeling long-term features.
Experimental results demonstrate that the Mixed Channels strategy outperforms
pure CI strategy in multivariate time-series forecasting tasks.",0
Uncertainty Quantification for cross-subject Motor Imagery classification,2403.09228v1,http://arxiv.org/abs/2403.09228v1,2024-03-14 09:48:48+00:00,"Uncertainty Quantification aims to determine when the prediction from a
Machine Learning model is likely to be wrong. Computer Vision research has
explored methods for determining epistemic uncertainty (also known as model
uncertainty), which should correspond with generalisation error. These methods
theoretically allow to predict misclassifications due to inter-subject
variability. We applied a variety of Uncertainty Quantification methods to
predict misclassifications for a Motor Imagery Brain Computer Interface. Deep
Ensembles performed best, both in terms of classification performance and
cross-subject Uncertainty Quantification performance. However, we found that
standard CNNs with Softmax output performed better than some of the more
advanced methods.",0
Controllability of continuous networks and a kernel-based learning approximation,2403.08690v1,http://arxiv.org/abs/2403.08690v1,2024-03-13 16:49:36+00:00,"Residual deep neural networks are formulated as interacting particle systems
leading to a description through neural differential equations, and, in the
case of large input data, through mean-field neural networks. The mean-field
description allows also the recast of the training processes as a
controllability problem for the solution to the mean-field dynamics. We show
theoretical results on the controllability of the linear microscopic and
mean-field dynamics through the Hilbert Uniqueness Method and propose a
computational approach based on kernel learning methods to solve numerically,
and efficiently, the training problem. Further aspects of the structural
properties of the mean-field equation will be reviewed.",0
Faceptor: A Generalist Model for Face Perception,2403.09500v1,http://arxiv.org/abs/2403.09500v1,2024-03-14 15:42:31+00:00,"With the comprehensive research conducted on various face analysis tasks,
there is a growing interest among researchers to develop a unified approach to
face perception. Existing methods mainly discuss unified representation and
training, which lack task extensibility and application efficiency. To tackle
this issue, we focus on the unified model structure, exploring a face
generalist model. As an intuitive design, Naive Faceptor enables tasks with the
same output shape and granularity to share the structural design of the
standardized output head, achieving improved task extensibility. Furthermore,
Faceptor is proposed to adopt a well-designed single-encoder dual-decoder
architecture, allowing task-specific queries to represent new-coming semantics.
This design enhances the unification of model structure while improving
application efficiency in terms of storage overhead. Additionally, we introduce
Layer-Attention into Faceptor, enabling the model to adaptively select features
from optimal layers to perform the desired tasks. Through joint training on 13
face perception datasets, Faceptor achieves exceptional performance in facial
landmark localization, face parsing, age estimation, expression recognition,
binary attribute classification, and face recognition, achieving or surpassing
specialized methods in most tasks. Our training framework can also be applied
to auxiliary supervised learning, significantly improving performance in
data-sparse tasks such as age estimation and expression recognition. The code
and models will be made publicly available at
https://github.com/lxq1000/Faceptor.",0
Logits of API-Protected LLMs Leak Proprietary Information,2403.09539v1,http://arxiv.org/abs/2403.09539v1,2024-03-14 16:27:49+00:00,"The commercialization of large language models (LLMs) has led to the common
practice of high-level API-only access to proprietary models. In this work, we
show that even with a conservative assumption about the model architecture, it
is possible to learn a surprisingly large amount of non-public information
about an API-protected LLM from a relatively small number of API queries (e.g.,
costing under $1,000 for OpenAI's gpt-3.5-turbo). Our findings are centered on
one key observation: most modern LLMs suffer from a softmax bottleneck, which
restricts the model outputs to a linear subspace of the full output space. We
show that this lends itself to a model image or a model signature which unlocks
several capabilities with affordable cost: efficiently discovering the LLM's
hidden size, obtaining full-vocabulary outputs, detecting and disambiguating
different model updates, identifying the source LLM given a single full LLM
output, and even estimating the output layer parameters. Our empirical
investigations show the effectiveness of our methods, which allow us to
estimate the embedding size of OpenAI's gpt-3.5-turbo to be about 4,096.
Lastly, we discuss ways that LLM providers can guard against these attacks, as
well as how these capabilities can be viewed as a feature (rather than a bug)
by allowing for greater transparency and accountability.",0
Mixed Algorithm of SINDy and HAVOK for Measure-Based Analysis of Power System with Inverter-based Resources,2403.09536v1,http://arxiv.org/abs/2403.09536v1,2024-03-14 16:23:48+00:00,"Artificial intelligence and machine learning is enhancing electric grids by
offering data analysis tools that can be used to operate the power grid more
reliably. However, the complex nonlinear dynamics, particularly when coupled
with multi-scale interactions among Inverter-based renewable energy Resources,
calls for effective algorithms for power system application. This paper
presents affective novel algorithm to detect various nonlinear dynamics, which
is built upon: the Sparse Identification of Nonlinear Dynamics method for
nonlinear dynamics detection; and Hankel Alternative View of Koopman method for
multi-scale decomposition. We show that, by an appropriate integration of the
strengths of the two, the mixed algorithm not only can detect the nonlinearity,
but also it distinguishes the nonlinearity caused by coupled Inverter-based
resources from the more familiar ones caused synchronous generators. This shows
that the proposal algorithm can be a promising application of artificial
intelligence and machine learning for data measure-based analysis to support
operation of power system with integrated renewables.",0
Defense via Behavior Attestation against Attacks in Connected and Automated Vehicles based Federated Learning Systems,2403.09531v1,http://arxiv.org/abs/2403.09531v1,2024-03-14 16:17:11+00:00,"The recent application of Federated Learning algorithms in IOT and Wireless
vehicular networks have given rise to newer cyber threats in the mobile
environment which hitherto were not present in traditional fixed networks.
These threats arise due to the intrinsic nature of wireless transmission medium
and other inherent characteristics of mobile networks such as high-node
mobility and rapidly changing topology. This paper investigates the robustness
of Vehicular AttestedFL defense strategies against falsified information
attacks by tracking the behavior. We show that the defense strategies are
capable of detecting and eliminating malicious nodes in the wireless mobile
setting of the future smart road networks.",0
A general-purpose neural network potential for Ti-Al-Nb alloys towards large-scale molecular dynamics with ab initio accuracy,2403.09529v1,http://arxiv.org/abs/2403.09529v1,2024-03-14 16:11:14+00:00,"High Nb-containing TiAl alloys exhibit exceptional high-temperature strength
and room-temperature ductility, making them widely used in hot-section
components of automotive and aerospace engines. However, the lack of accurate
interatomic interaction potentials for large-scale modeling severely hampers a
comprehensive understanding of the failure mechanism of Ti-Al-Nb alloys and the
development of strategies to enhance the mechanical properties. Here, we
develop a general-purpose machine-learned potential (MLP) for the Ti-Al-Nb
ternary system by combining the neural evolution potentials framework with an
active learning scheme. The developed MLP, trained on extensive
first-principles datasets, demonstrates remarkable accuracy in predicting
various lattice and defect properties, as well as high-temperature
characteristics such as thermal expansion and melting point for TiAl systems.
Notably, this potential can effectively describe the key effect of Nb doping on
stacking fault energies and formation energies. Of practical importance is that
our MLP enables large-scale molecular dynamics simulations involving tens of
millions of atoms with ab initio accuracy, achieving an outstanding balance
between computational speed and accuracy. These results pave the way for
studying micro-mechanical behaviors in TiAl lamellar structures and developing
high-performance TiAl alloys towards applications at elevated temperatures.",0
WavCraft: Audio Editing and Generation with Natural Language Prompts,2403.09527v1,http://arxiv.org/abs/2403.09527v1,2024-03-14 16:10:34+00:00,"We introduce WavCraft, a collective system that leverages large language
models (LLMs) to connect diverse task-specific models for audio content
creation and editing. Specifically, WavCraft describes the content of raw sound
materials in natural language and prompts the LLM conditioned on audio
descriptions and users' requests. WavCraft leverages the in-context learning
ability of the LLM to decomposes users' instructions into several tasks and
tackle each task collaboratively with audio expert modules. Through task
decomposition along with a set of task-specific models, WavCraft follows the
input instruction to create or edit audio content with more details and
rationales, facilitating users' control. In addition, WavCraft is able to
cooperate with users via dialogue interaction and even produce the audio
content without explicit user commands. Experiments demonstrate that WavCraft
yields a better performance than existing methods, especially when adjusting
the local regions of audio clips. Moreover, WavCraft can follow complex
instructions to edit and even create audio content on the top of input
recordings, facilitating audio producers in a broader range of applications.
Our implementation and demos are available at
https://github.com/JinhuaLiang/WavCraft.",0
Physics-Informed Neural Network for Volumetric Sound field Reconstruction of Speech Signals,2403.09524v1,http://arxiv.org/abs/2403.09524v1,2024-03-14 16:10:17+00:00,"Recent developments in acoustic signal processing have seen the integration
of deep learning methodologies, alongside the continued prominence of classical
wave expansion-based approaches, particularly in sound field reconstruction.
Physics-Informed Neural Networks (PINNs) have emerged as a novel framework,
bridging the gap between data-driven and model-based techniques for addressing
physical phenomena governed by partial differential equations. This paper
introduces a PINN-based approach for the recovery of arbitrary volumetric
acoustic fields. The network incorporates the wave equation to impose a
regularization on signal reconstruction in the time domain. This methodology
enables the network to learn the underlying physics of sound propagation and
allows for the complete characterization of the sound field based on a limited
set of observations. The proposed method's efficacy is validated through
experiments involving speech signals in a real-world environment, considering
varying numbers of available measurements. Moreover, a comparative analysis is
undertaken against state-of-the-art frequency-domain and time-domain
reconstruction methods from existing literature, highlighting the increased
accuracy across the various measurement configurations.",0
MT-PATCHER: Selective and Extendable Knowledge Distillation from Large Language Models for Machine Translation,2403.09522v1,http://arxiv.org/abs/2403.09522v1,2024-03-14 16:07:39+00:00,"Large Language Models (LLM) have demonstrated their strong ability in the
field of machine translation (MT), yet they suffer from high computational cost
and latency. Therefore, transferring translation knowledge from giant LLMs to
medium-sized machine translation models is a promising research direction.
However, traditional knowledge distillation methods do not take the capability
of student and teacher models into consideration, therefore repeatedly teaching
student models on the knowledge they have learned, and failing to extend to
novel contexts and knowledge. In this paper, we propose a framework called
MT-Patcher, which transfers knowledge from LLMs to existing MT models in a
selective, comprehensive and proactive manner. Considering the current
translation ability of student MT models, we only identify and correct their
translation errors, instead of distilling the whole translation from the
teacher. Leveraging the strong language abilities of LLMs, we instruct LLM
teachers to synthesize diverse contexts and anticipate more potential errors
for the student. Experiment results on translating both specific language
phenomena and general MT benchmarks demonstrate that finetuning the student MT
model on about 10% examples can achieve comparable results to the traditional
knowledge distillation method, and synthesized potential errors and diverse
contexts further improve translation performances on unseen contexts and words.",0
Leveraging Prototypical Representations for Mitigating Social Bias without Demographic Information,2403.09516v1,http://arxiv.org/abs/2403.09516v1,2024-03-14 15:58:36+00:00,"Mitigating social biases typically requires identifying the social groups
associated with each data sample. In this paper, we present DAFair, a novel
approach to address social bias in language models. Unlike traditional methods
that rely on explicit demographic labels, our approach does not require any
such information. Instead, we leverage predefined prototypical demographic
texts and incorporate a regularization term during the fine-tuning process to
mitigate bias in the model's representations. Our empirical results across two
tasks and two models demonstrate the effectiveness of our method compared to
previous approaches that do not rely on labeled data. Moreover, with limited
demographic-annotated data, our approach outperforms common debiasing
approaches.",0
On STPA for Distributed Development of Safe Autonomous Driving: An Interview Study,2403.09509v1,http://arxiv.org/abs/2403.09509v1,2024-03-14 15:56:02+00:00,"Safety analysis is used to identify hazards and build knowledge during the
design phase of safety-relevant functions. This is especially true for complex
AI-enabled and software intensive systems such as Autonomous Drive (AD).
System-Theoretic Process Analysis (STPA) is a novel method applied in
safety-related fields like defense and aerospace, which is also becoming
popular in the automotive industry. However, STPA assumes prerequisites that
are not fully valid in the automotive system engineering with distributed
system development and multi-abstraction design levels. This would inhibit
software developers from using STPA to analyze their software as part of a
bigger system, resulting in a lack of traceability. This can be seen as a
maintainability challenge in continuous development and deployment (DevOps). In
this paper, STPA's different guidelines for the automotive industry, e.g.
J31887/ISO21448/STPA handbook, are firstly compared to assess their
applicability to the distributed development of complex AI-enabled systems like
AD. Further, an approach to overcome the challenges of using STPA in a
multi-level design context is proposed. By conducting an interview study with
automotive industry experts for the development of AD, the challenges are
validated and the effectiveness of the proposed approach is evaluated.",0
Don't Judge by the Look: A Motion Coherent Augmentation for Video Recognition,2403.09506v1,http://arxiv.org/abs/2403.09506v1,2024-03-14 15:53:04+00:00,"Current training pipelines in object recognition neglect Hue Jittering when
doing data augmentation as it not only brings appearance changes that are
detrimental to classification, but also the implementation is inefficient in
practice. In this study, we investigate the effect of hue variance in the
context of video recognition and find this variance to be beneficial since
static appearances are less important in videos that contain motion
information. Based on this observation, we propose a data augmentation method
for video recognition, named Motion Coherent Augmentation (MCA), that
introduces appearance variation in videos and implicitly encourages the model
to prioritize motion patterns, rather than static appearances. Concretely, we
propose an operation SwapMix to efficiently modify the appearance of video
samples, and introduce Variation Alignment (VA) to resolve the distribution
shift caused by SwapMix, enforcing the model to learn appearance invariant
representations. Comprehensive empirical evaluation across various
architectures and different datasets solidly validates the effectiveness and
generalization ability of MCA, and the application of VA in other augmentation
methods. Code is available at https://github.com/BeSpontaneous/MCA-pytorch.",0
Efficient Convolutional Forward Modeling and Sparse Coding in Multichannel Imaging,2403.09505v1,http://arxiv.org/abs/2403.09505v1,2024-03-14 15:52:13+00:00,"This study considers the Block-Toeplitz structural properties inherent in
traditional multichannel forward model matrices, using Full Matrix Capture
(FMC) in ultrasonic testing as a case study. We propose an analytical
convolutional forward model that transforms reflectivity maps into FMC data.
Our findings demonstrate that the convolutional model excels over its
matrix-based counterpart in terms of computational efficiency and storage
requirements. This accelerated forward modeling approach holds significant
potential for various inverse problems, notably enhancing Sparse Signal
Recovery (SSR) within the context LASSO regression, which facilitates efficient
Convolutional Sparse Coding (CSC) algorithms. Additionally, we explore the
integration of Convolutional Neural Networks (CNNs) for the forward model,
employing deep unfolding to implement the Learned Block Convolutional ISTA
(BC-LISTA).",0
Is Data All That Matters? The Role of Control Frequency for Learning-Based Sampled-Data Control of Uncertain Systems,2403.09504v1,http://arxiv.org/abs/2403.09504v1,2024-03-14 15:48:07+00:00,"Learning models or control policies from data has become a powerful tool to
improve the performance of uncertain systems. While a strong focus has been
placed on increasing the amount and quality of data to improve performance,
data can never fully eliminate uncertainty, making feedback necessary to ensure
stability and performance. We show that the control frequency at which the
input is recalculated is a crucial design parameter, yet it has hardly been
considered before. We address this gap by combining probabilistic model
learning and sampled-data control. We use Gaussian processes (GPs) to learn a
continuous-time model and compute a corresponding discrete-time controller. The
result is an uncertain sampled-data control system, for which we derive robust
stability conditions. We formulate semidefinite programs to compute the minimum
control frequency required for stability and to optimize performance. As a
result, our approach enables us to study the effect of both control frequency
and data on stability and closed-loop performance. We show in numerical
simulations of a quadrotor that performance can be improved by increasing
either the amount of data or the control frequency, and that we can trade off
one for the other. For example, by increasing the control frequency by 33%, we
can reduce the number of data points by half while still achieving similar
performance.",0
EquiAV: Leveraging Equivariance for Audio-Visual Contrastive Learning,2403.09502v1,http://arxiv.org/abs/2403.09502v1,2024-03-14 15:44:19+00:00,"Recent advancements in self-supervised audio-visual representation learning
have demonstrated its potential to capture rich and comprehensive
representations. However, despite the advantages of data augmentation verified
in many learning methods, audio-visual learning has struggled to fully harness
these benefits, as augmentations can easily disrupt the correspondence between
input pairs. To address this limitation, we introduce EquiAV, a novel framework
that leverages equivariance for audio-visual contrastive learning. Our approach
begins with extending equivariance to audio-visual learning, facilitated by a
shared attention-based transformation predictor. It enables the aggregation of
features from diverse augmentations into a representative embedding, providing
robust supervision. Notably, this is achieved with minimal computational
overhead. Extensive ablation studies and qualitative results verify the
effectiveness of our method. EquiAV outperforms previous works across various
audio-visual benchmarks.",0
A Reinforcement Learning Approach to Dairy Farm Battery Management using Q Learning,2403.09499v1,http://arxiv.org/abs/2403.09499v1,2024-03-14 15:42:26+00:00,"Dairy farming consumes a significant amount of energy, making it an
energy-intensive sector within agriculture. Integrating renewable energy
generation into dairy farming could help address this challenge. Effective
battery management is important for integrating renewable energy generation.
Managing battery charging and discharging poses significant challenges because
of fluctuations in electrical consumption, the intermittent nature of renewable
energy generation, and fluctuations in energy prices. Artificial Intelligence
(AI) has the potential to significantly improve the use of renewable energy in
dairy farming, however, there is limited research conducted in this particular
domain. This research considers Ireland as a case study as it works towards
attaining its 2030 energy strategy centered on the utilization of renewable
sources. This study proposes a Q-learning-based algorithm for scheduling
battery charging and discharging in a dairy farm setting. This research also
explores the effect of the proposed algorithm by adding wind generation data
and considering additional case studies. The proposed algorithm reduces the
cost of imported electricity from the grid by 13.41\%, peak demand by 2\%, and
24.49\% when utilizing wind generation. These results underline how
reinforcement learning is highly effective in managing batteries in the dairy
farming sector.",0
Improving Distant 3D Object Detection Using 2D Box Supervision,2403.09230v1,http://arxiv.org/abs/2403.09230v1,2024-03-14 09:54:31+00:00,"Improving the detection of distant 3d objects is an important yet challenging
task. For camera-based 3D perception, the annotation of 3d bounding relies
heavily on LiDAR for accurate depth information. As such, the distance of
annotation is often limited due to the sparsity of LiDAR points on distant
objects, which hampers the capability of existing detectors for long-range
scenarios. We address this challenge by considering only 2D box supervision for
distant objects since they are easy to annotate. We propose LR3D, a framework
that learns to recover the missing depth of distant objects. LR3D adopts an
implicit projection head to learn the generation of mapping between 2D boxes
and depth using the 3D supervision on close objects. This mapping allows the
depth estimation of distant objects conditioned on their 2D boxes, making
long-range 3D detection with 2D supervision feasible. Experiments show that
without distant 3D annotations, LR3D allows camera-based methods to detect
distant objects (over 200m) with comparable accuracy to full 3D supervision.
Our framework is general, and could widely benefit 3D detection methods to a
large extent.",0
Anomaly Detection by Adapting a pre-trained Vision Language Model,2403.09493v1,http://arxiv.org/abs/2403.09493v1,2024-03-14 15:35:07+00:00,"Recently, large vision and language models have shown their success when
adapting them to many downstream tasks. In this paper, we present a unified
framework named CLIP-ADA for Anomaly Detection by Adapting a pre-trained CLIP
model. To this end, we make two important improvements: 1) To acquire unified
anomaly detection across industrial images of multiple categories, we introduce
the learnable prompt and propose to associate it with abnormal patterns through
self-supervised learning. 2) To fully exploit the representation power of CLIP,
we introduce an anomaly region refinement strategy to refine the localization
quality. During testing, the anomalies are localized by directly calculating
the similarity between the representation of the learnable prompt and the
image. Comprehensive experiments demonstrate the superiority of our framework,
e.g., we achieve the state-of-the-art 97.5/55.6 and 89.3/33.1 on MVTec-AD and
VisA for anomaly detection and localization. In addition, the proposed method
also achieves encouraging performance with marginal training data, which is
more challenging.",0
On using Machine Learning Algorithms for Motorcycle Collision Detection,2403.09491v1,http://arxiv.org/abs/2403.09491v1,2024-03-14 15:32:25+00:00,"Globally, motorcycles attract vast and varied users. However, since the rate
of severe injury and fatality in motorcycle accidents far exceeds passenger car
accidents, efforts have been directed toward increasing passive safety systems.
Impact simulations show that the risk of severe injury or death in the event of
a motorcycle-to-car impact can be greatly reduced if the motorcycle is equipped
with passive safety measures such as airbags and seat belts. For the passive
safety systems to be activated, a collision must be detected within
milliseconds for a wide variety of impact configurations, but under no
circumstances may it be falsely triggered. For the challenge of reliably
detecting impending collisions, this paper presents an investigation towards
the applicability of machine learning algorithms. First, a series of
simulations of accidents and driving operation is introduced to collect data to
train machine learning classification models. Their performance is henceforth
assessed and compared via multiple representative and application-oriented
criteria.",0
Hyper-CL: Conditioning Sentence Representations with Hypernetworks,2403.09490v1,http://arxiv.org/abs/2403.09490v1,2024-03-14 15:30:25+00:00,"While the introduction of contrastive learning frameworks in sentence
representation learning has significantly contributed to advancements in the
field, it still remains unclear whether state-of-the-art sentence embeddings
can capture the fine-grained semantics of sentences, particularly when
conditioned on specific perspectives. In this paper, we introduce Hyper-CL, an
efficient methodology that integrates hypernetworks with contrastive learning
to compute conditioned sentence representations. In our proposed approach, the
hypernetwork is responsible for transforming pre-computed condition embeddings
into corresponding projection layers. This enables the same sentence embeddings
to be projected differently according to various conditions. Evaluation on two
representative conditioning benchmarks, namely conditional semantic text
similarity and knowledge graph completion, demonstrates that Hyper-CL is
effective in flexibly conditioning sentence representations, showcasing its
computational efficiency at the same time. We also provide a comprehensive
analysis of the inner workings of our approach, leading to a better
interpretation of its mechanisms.",0
Rectifying Demonstration Shortcut in In-Context Learning,2403.09488v1,http://arxiv.org/abs/2403.09488v1,2024-03-14 15:30:14+00:00,"Large language models (LLMs) are able to solve various tasks with only a few
demonstrations utilizing their in-context learning (ICL) abilities. However,
LLMs often rely on their pre-trained semantic priors of demonstrations rather
than on the input-label relationships to proceed with ICL prediction. In this
work, we term this phenomenon as the `Demonstration Shortcut'. While previous
works have primarily focused on improving ICL prediction results for predefined
tasks, we aim to rectify the Demonstration Shortcut, thereby enabling the LLM
to effectively learn new input-label relationships from demonstrations. To
achieve this, we introduce In-Context Calibration, a demonstration-aware
calibration method. We evaluate the effectiveness of the proposed method in two
settings: (1) the Original ICL Task using the standard label space and (2) the
Task Learning setting, where the label space is replaced with semantically
unrelated tokens. In both settings, In-Context Calibration demonstrates
substantial improvements, with results generalized across three LLM families
(OPT, GPT, and Llama2) under various configurations.",0
SpikeReveal: Unlocking Temporal Sequences from Real Blurry Inputs with Spike Streams,2403.09486v1,http://arxiv.org/abs/2403.09486v1,2024-03-14 15:29:09+00:00,"Reconstructing a sequence of sharp images from the blurry input is crucial
for enhancing our insights into the captured scene and poses a significant
challenge due to the limited temporal features embedded in the image. Spike
cameras, sampling at rates up to 40,000 Hz, have proven effective in capturing
motion features and beneficial for solving this ill-posed problem. Nonetheless,
existing methods fall into the supervised learning paradigm, which suffers from
notable performance degradation when applied to real-world scenarios that
diverge from the synthetic training data domain. Moreover, the quality of
reconstructed images is capped by the generated images based on motion analysis
interpolation, which inherently differs from the actual scene, affecting the
generalization ability of these methods in real high-speed scenarios. To
address these challenges, we propose the first self-supervised framework for
the task of spike-guided motion deblurring. Our approach begins with the
formulation of a spike-guided deblurring model that explores the theoretical
relationships among spike streams, blurry images, and their corresponding sharp
sequences. We subsequently develop a self-supervised cascaded framework to
alleviate the issues of spike noise and spatial-resolution mismatching
encountered in the deblurring model. With knowledge distillation and
re-blurring loss, we further design a lightweight deblur network to generate
high-quality sequences with brightness and texture consistency with the
original input. Quantitative and qualitative experiments conducted on our
real-world and synthetic datasets with spikes validate the superior
generalization of the proposed framework. Our code, data and trained models
will be available at \url{https://github.com/chenkang455/S-SDM}.",0
Laying the Foundation First? Investigating the Generalization from Atomic Skills to Complex Reasoning Tasks,2403.09479v1,http://arxiv.org/abs/2403.09479v1,2024-03-14 15:20:54+00:00,"Current language models have demonstrated their capability to develop basic
reasoning, but struggle in more complicated reasoning tasks that require a
combination of atomic skills, such as math word problem requiring skills like
arithmetic and unit conversion. Previous methods either do not improve the
inherent atomic skills of models or not attempt to generalize the atomic skills
to complex reasoning tasks. In this paper, we first propose a probing framework
to investigate whether the atomic skill can spontaneously generalize to complex
reasoning tasks. Then, we introduce a hierarchical curriculum learning training
strategy to achieve better skill generalization. In our experiments, we find
that atomic skills can not spontaneously generalize to compositional tasks. By
leveraging hierarchical curriculum learning, we successfully induce
generalization, significantly improve the performance of open-source LMs on
complex reasoning tasks. Promisingly, the skill generalization exhibit
effective in cross-dataset and cross-domain scenarios. Complex reasoning can
also help enhance atomic skills. Our findings offer valuable guidance for
designing better training strategies for complex reasoning tasks.",0
"VIRUS-NeRF -- Vision, InfraRed and UltraSonic based Neural Radiance Fields",2403.09477v1,http://arxiv.org/abs/2403.09477v1,2024-03-14 15:19:19+00:00,"Autonomous mobile robots are an increasingly integral part of modern factory
and warehouse operations. Obstacle detection, avoidance and path planning are
critical safety-relevant tasks, which are often solved using expensive LiDAR
sensors and depth cameras. We propose to use cost-effective low-resolution
ranging sensors, such as ultrasonic and infrared time-of-flight sensors by
developing VIRUS-NeRF - Vision, InfraRed, and UltraSonic based Neural Radiance
Fields. Building upon Instant Neural Graphics Primitives with a Multiresolution
Hash Encoding (Instant-NGP), VIRUS-NeRF incorporates depth measurements from
ultrasonic and infrared sensors and utilizes them to update the occupancy grid
used for ray marching. Experimental evaluation in 2D demonstrates that
VIRUS-NeRF achieves comparable mapping performance to LiDAR point clouds
regarding coverage. Notably, in small environments, its accuracy aligns with
that of LiDAR measurements, while in larger ones, it is bounded by the utilized
ultrasonic sensors. An in-depth ablation study reveals that adding ultrasonic
and infrared sensors is highly effective when dealing with sparse data and low
view variation. Further, the proposed occupancy grid of VIRUS-NeRF improves the
mapping capabilities and increases the training speed by 46% compared to
Instant-NGP. Overall, VIRUS-NeRF presents a promising approach for
cost-effective local mapping in mobile robotics, with potential applications in
safety and navigation tasks. The code can be found at
https://github.com/ethz-asl/virus nerf.",0
Easy-to-Hard Generalization: Scalable Alignment Beyond Human Supervision,2403.09472v1,http://arxiv.org/abs/2403.09472v1,2024-03-14 15:12:38+00:00,"Current AI alignment methodologies rely on human-provided demonstrations or
judgments, and the learned capabilities of AI systems would be upper-bounded by
human capabilities as a result. This raises a challenging research question:
How can we keep improving the systems when their capabilities have surpassed
the levels of humans? This paper answers this question in the context of
tackling hard reasoning tasks (e.g., level 4-5 MATH problems) via learning from
human annotations on easier tasks (e.g., level 1-3 MATH problems), which we
term as \textit{easy-to-hard generalization}. Our key insight is that an
evaluator (reward model) trained on supervisions for easier tasks can be
effectively used for scoring candidate solutions of harder tasks and hence
facilitating easy-to-hard generalization over different levels of tasks. Based
on this insight, we propose a novel approach to scalable alignment, which
firstly trains the process-supervised reward models on easy problems (e.g.,
level 1-3), and then uses them to evaluate the performance of policy models on
hard problems. We show that such \textit{easy-to-hard generalization from
evaluators} can enable \textit{easy-to-hard generalizations in generators}
either through re-ranking or reinforcement learning (RL). Notably, our
process-supervised 7b RL model achieves an accuracy of 34.0\% on MATH500,
despite only using human supervision on easy problems. Our approach suggests a
promising path toward AI systems that advance beyond the frontier of human
supervision.",0
Climate Immobility Traps: A Household-Level Test,2403.09470v1,http://arxiv.org/abs/2403.09470v1,2024-03-14 15:09:04+00:00,"The complex relationship between climate shocks, migration, and adaptation
hampers a rigorous understanding of the heterogeneous mobility outcomes of farm
households exposed to climate risk. To unpack this heterogeneity, the analysis
combines longitudinal multi-topic household survey data from Nigeria with a
causal machine learning approach, tailored to a conceptual framework bridging
economic migration theory and the poverty traps literature. The results show
that pre-shock asset levels, in situ adaptive capacity, and cumulative shock
exposure drive not just the magnitude but also the sign of the impact of
agriculture-relevant weather anomalies on the mobility outcomes of farming
households. While local adaptation acts as a substitute for migration, the
roles played by wealth constraints and repeated shock exposure suggest the
presence of climate-induced immobility traps.",0
Outlier Robust Multivariate Polynomial Regression,2403.09465v1,http://arxiv.org/abs/2403.09465v1,2024-03-14 15:04:45+00:00,"We study the problem of robust multivariate polynomial regression: let
$p\colon\mathbb{R}^n\to\mathbb{R}$ be an unknown $n$-variate polynomial of
degree at most $d$ in each variable. We are given as input a set of random
samples $(\mathbf{x}_i,y_i) \in [-1,1]^n \times \mathbb{R}$ that are noisy
versions of $(\mathbf{x}_i,p(\mathbf{x}_i))$. More precisely, each
$\mathbf{x}_i$ is sampled independently from some distribution $\chi$ on
$[-1,1]^n$, and for each $i$ independently, $y_i$ is arbitrary (i.e., an
outlier) with probability at most $\rho < 1/2$, and otherwise satisfies
$|y_i-p(\mathbf{x}_i)|\leq\sigma$. The goal is to output a polynomial
$\hat{p}$, of degree at most $d$ in each variable, within an
$\ell_\infty$-distance of at most $O(\sigma)$ from $p$.
  Kane, Karmalkar, and Price [FOCS'17] solved this problem for $n=1$. We
generalize their results to the $n$-variate setting, showing an algorithm that
achieves a sample complexity of $O_n(d^n\log d)$, where the hidden constant
depends on $n$, if $\chi$ is the $n$-dimensional Chebyshev distribution. The
sample complexity is $O_n(d^{2n}\log d)$, if the samples are drawn from the
uniform distribution instead. The approximation error is guaranteed to be at
most $O(\sigma)$, and the run-time depends on $\log(1/\sigma)$. In the setting
where each $\mathbf{x}_i$ and $y_i$ are known up to $N$ bits of precision, the
run-time's dependence on $N$ is linear. We also show that our sample
complexities are optimal in terms of $d^n$. Furthermore, we show that it is
possible to have the run-time be independent of $1/\sigma$, at the cost of a
higher sample complexity.",0
Machine learning activation function parameterization of the occupation numbers for natural orbital functionals based on electron pairing approaches,2403.09463v1,http://arxiv.org/abs/2403.09463v1,2024-03-14 15:04:14+00:00,"Within the framework of natural orbital functional theory, having a
convenient representation of the optimization function becomes critical for the
computational performance of the calculation. Recognizing this, we propose an
innovative parametrization of the occupation numbers that takes advantage of
the electron-pairing approach used in Piris natural orbital functionals,
through the adoption of the softmax function, a pivotal component in modern
deep-learning models. Our approach not only ensures adherence to the
N-representability of the first-order reduced density matrix (1RDM) but also
significantly enhances the computational efficiency of 1RDM functional theory
calculations. The effectiveness of this alternative parameterization approach
was assessed using the W4-17-MR molecular set, which demonstrated faster and
robust convergence compared to previous implementations.",0
Deep-learning-assisted optical communication with discretized state space of structural light,2403.09462v1,http://arxiv.org/abs/2403.09462v1,2024-03-14 15:03:09+00:00,"The rich structure of the transverse spatial mode of structural light has
facilitated its applications in quantum information and optical communication.
The Laguerre-Gaussian (LG) modes, with azimuthal and radial indexes, consist of
a complete orthogonal basis to describe the transverse spatial mode of light.
The azimuthal index is often endowed with the orbital angular momentum (OAM), a
high dimensional degree of freedom. The advent of OAM in optical science marks
a pivotal advancement, surpassing traditional optical techniques in light
manipulation for advanced data encoding and signal transmission. Here, we
present a scheme that utilizes the advanced deep learning technique for LG
modes recognition. By discretizing the state space of the LG modes, a neural
network model is trained to classify the given samples. A proof-of-principle
experiment is performed to show that our scheme requires less samples for model
training, while increasing the channel capacity within limited OAM number. We
further apply our scheme to an image transmission task, demonstrating the
ability to encode large data with low OAM number. Our work opens a new avenue
for high capacity optical communication based on structural light.",0
Development of control algorithms for mobile robotics focused on their potential use for FPGA-based robots,2403.09459v1,http://arxiv.org/abs/2403.09459v1,2024-03-14 15:02:24+00:00,"This paper investigates the development and optimization of control
algorithms for mobile robotics, with a keen focus on their implementation in
Field-Programmable Gate Arrays (FPGAs). It delves into both classical control
approaches such as PID and modern techniques including deep learning,
addressing their application in sectors ranging from industrial automation to
medical care. The study highlights the practical challenges and advancements in
embedding these algorithms into FPGAs, which offer significant benefits for
mobile robotics due to their high-speed processing and parallel computation
capabilities. Through an analysis of various control strategies, the paper
showcases the improvements in robot performance, particularly in navigation and
obstacle avoidance. It emphasizes the critical role of FPGAs in enhancing the
efficiency and adaptability of control algorithms in dynamic environments.
Additionally, the research discusses the difficulties in benchmarking and
evaluating the performance of these algorithms in real-world applications,
suggesting a need for standardized evaluation criteria. The contribution of
this work lies in its comprehensive examination of control algorithms'
potential in FPGA-based mobile robotics, offering insights into future research
directions for improving robotic autonomy and operational efficiency.",0
Explorations in Texture Learning,2403.09543v1,http://arxiv.org/abs/2403.09543v1,2024-03-14 16:30:52+00:00,"In this work, we investigate \textit{texture learning}: the identification of
textures learned by object classification models, and the extent to which they
rely on these textures. We build texture-object associations that uncover new
insights about the relationships between texture and object classes in CNNs and
find three classes of results: associations that are strong and expected,
strong and not expected, and expected but not present. Our analysis
demonstrates that investigations in texture learning enable new methods for
interpretability and have the potential to uncover unexpected biases.",0
How do Machine Learning Projects use Continuous Integration Practices? An Empirical Study on GitHub Actions,2403.09547v1,http://arxiv.org/abs/2403.09547v1,2024-03-14 16:35:39+00:00,"Continuous Integration (CI) is a well-established practice in traditional
software development, but its nuances in the domain of Machine Learning (ML)
projects remain relatively unexplored. Given the distinctive nature of ML
development, understanding how CI practices are adopted in this context is
crucial for tailoring effective approaches. In this study, we conduct a
comprehensive analysis of 185 open-source projects on GitHub (93 ML and 92
non-ML projects). Our investigation comprises both quantitative and qualitative
dimensions, aiming to uncover differences in CI adoption between ML and non-ML
projects. Our findings indicate that ML projects often require longer build
durations, and medium-sized ML projects exhibit lower test coverage compared to
non-ML projects. Moreover, small and medium-sized ML projects show a higher
prevalence of increasing build duration trends compared to their non-ML
counterparts. Additionally, our qualitative analysis illuminates the
discussions around CI in both ML and non-ML projects, encompassing themes like
CI Build Execution and Status, CI Testing, and CI Infrastructure. These
insights shed light on the unique challenges faced by ML projects in adopting
CI practices effectively.",0
Breast Cancer Classification Using Gradient Boosting Algorithms Focusing on Reducing the False Negative and SHAP for Explainability,2403.09548v1,http://arxiv.org/abs/2403.09548v1,2024-03-14 16:35:43+00:00,"Cancer is one of the diseases that kill the most women in the world, with
breast cancer being responsible for the highest number of cancer cases and
consequently deaths. However, it can be prevented by early detection and,
consequently, early treatment. Any development for detection or perdition this
kind of cancer is important for a better healthy life. Many studies focus on a
model with high accuracy in cancer prediction, but sometimes accuracy alone may
not always be a reliable metric. This study implies an investigative approach
to studying the performance of different machine learning algorithms based on
boosting to predict breast cancer focusing on the recall metric. Boosting
machine learning algorithms has been proven to be an effective tool for
detecting medical diseases. The dataset of the University of California, Irvine
(UCI) repository has been utilized to train and test the model classifier that
contains their attributes. The main objective of this study is to use
state-of-the-art boosting algorithms such as AdaBoost, XGBoost, CatBoost and
LightGBM to predict and diagnose breast cancer and to find the most effective
metric regarding recall, ROC-AUC, and confusion matrix. Furthermore, our study
is the first to use these four boosting algorithms with Optuna, a library for
hyperparameter optimization, and the SHAP method to improve the
interpretability of our model, which can be used as a support to identify and
predict breast cancer. We were able to improve AUC or recall for all the models
and reduce the False Negative for AdaBoost and LigthGBM the final AUC were more
than 99.41\% for all models.",0
Generalizing Denoising to Non-Equilibrium Structures Improves Equivariant Force Fields,2403.09549v1,http://arxiv.org/abs/2403.09549v1,2024-03-14 16:38:02+00:00,"Understanding the interactions of atoms such as forces in 3D atomistic
systems is fundamental to many applications like molecular dynamics and
catalyst design. However, simulating these interactions requires
compute-intensive ab initio calculations and thus results in limited data for
training neural networks. In this paper, we propose to use denoising
non-equilibrium structures (DeNS) as an auxiliary task to better leverage
training data and improve performance. For training with DeNS, we first corrupt
a 3D structure by adding noise to its 3D coordinates and then predict the
noise. Different from previous works on denoising, which are limited to
equilibrium structures, the proposed method generalizes denoising to a much
larger set of non-equilibrium structures. The main difference is that a
non-equilibrium structure does not correspond to local energy minima and has
non-zero forces, and therefore it can have many possible atomic positions
compared to an equilibrium structure. This makes denoising non-equilibrium
structures an ill-posed problem since the target of denoising is not uniquely
defined. Our key insight is to additionally encode the forces of the original
non-equilibrium structure to specify which non-equilibrium structure we are
denoising. Concretely, given a corrupted non-equilibrium structure and the
forces of the original one, we predict the non-equilibrium structure satisfying
the input forces instead of any arbitrary structures. Since DeNS requires
encoding forces, DeNS favors equivariant networks, which can easily incorporate
forces and other higher-order tensors in node embeddings. We study the
effectiveness of training equivariant networks with DeNS on OC20, OC22 and MD17
datasets and demonstrate that DeNS can achieve new state-of-the-art results on
OC20 and OC22 and significantly improve training efficiency on MD17.",0
Dynamic Memory Compression: Retrofitting LLMs for Accelerated Inference,2403.09636v1,http://arxiv.org/abs/2403.09636v1,2024-03-14 17:59:26+00:00,"Transformers have emerged as the backbone of large language models (LLMs).
However, generation remains inefficient due to the need to store in memory a
cache of key-value representations for past tokens, whose size scales linearly
with the input sequence length and batch size. As a solution, we propose
Dynamic Memory Compression (DMC), a method for on-line key-value cache
compression at inference time. Most importantly, the model learns to apply
different compression rates in different heads and layers. We retrofit
pre-trained LLMs such as Llama 2 (7B, 13B and 70B) into DMC Transformers,
achieving up to ~3.7x throughput increase in auto-regressive inference on a
NVIDIA H100 GPU. DMC is applied via continued pre-training on a negligible
percentage of the original data without adding any extra parameters. We find
that DMC preserves the original downstream performance with up to 4x cache
compression, outperforming up-trained grouped-query attention (GQA). GQA and
DMC can be even combined to obtain compounded gains. As a result DMC fits
longer contexts and larger batches within any given memory budget.",0
Transformers Get Stable: An End-to-End Signal Propagation Theory for Language Models,2403.09635v1,http://arxiv.org/abs/2403.09635v1,2024-03-14 17:59:14+00:00,"In spite of their huge success, transformer models remain difficult to scale
in depth. In this work, we develop a unified signal propagation theory and
provide formulae that govern the moments of the forward and backward signal
through the transformer model. Our framework can be used to understand and
mitigate vanishing/exploding gradients, rank collapse, and instability
associated with high attention scores. We also propose DeepScaleLM, an
initialization and scaling scheme that conserves unit output/gradient moments
throughout the model, enabling the training of very deep models with 100s of
layers. We find that transformer models could be much deeper - our deep models
with fewer parameters outperform shallow models in Language Modeling, Speech
Translation, and Image Classification, across Encoder-only, Decoder-only and
Encoder-Decoder variants, for both Pre-LN and Post-LN transformers, for
multiple datasets and model sizes. These improvements also translate into
improved performance on downstream Question Answering tasks and improved
robustness for image classification.",0
3D-VLA: A 3D Vision-Language-Action Generative World Model,2403.09631v1,http://arxiv.org/abs/2403.09631v1,2024-03-14 17:58:41+00:00,"Recent vision-language-action (VLA) models rely on 2D inputs, lacking
integration with the broader realm of the 3D physical world. Furthermore, they
perform action prediction by learning a direct mapping from perception to
action, neglecting the vast dynamics of the world and the relations between
actions and dynamics. In contrast, human beings are endowed with world models
that depict imagination about future scenarios to plan actions accordingly. To
this end, we propose 3D-VLA by introducing a new family of embodied foundation
models that seamlessly link 3D perception, reasoning, and action through a
generative world model. Specifically, 3D-VLA is built on top of a 3D-based
large language model (LLM), and a set of interaction tokens is introduced to
engage with the embodied environment. Furthermore, to inject generation
abilities into the model, we train a series of embodied diffusion models and
align them into the LLM for predicting the goal images and point clouds. To
train our 3D-VLA, we curate a large-scale 3D embodied instruction dataset by
extracting vast 3D-related information from existing robotics datasets. Our
experiments on held-in datasets demonstrate that 3D-VLA significantly improves
the reasoning, multimodal generation, and planning capabilities in embodied
environments, showcasing its potential in real-world applications.",0
Quiet-STaR: Language Models Can Teach Themselves to Think Before Speaking,2403.09629v1,http://arxiv.org/abs/2403.09629v1,2024-03-14 17:58:16+00:00,"When writing and talking, people sometimes pause to think. Although
reasoning-focused works have often framed reasoning as a method of answering
questions or completing agentic tasks, reasoning is implicit in almost all
written text. For example, this applies to the steps not stated between the
lines of a proof or to the theory of mind underlying a conversation. In the
Self-Taught Reasoner (STaR, Zelikman et al. 2022), useful thinking is learned
by inferring rationales from few-shot examples in question-answering and
learning from those that lead to a correct answer. This is a highly constrained
setting -- ideally, a language model could instead learn to infer unstated
rationales in arbitrary text. We present Quiet-STaR, a generalization of STaR
in which LMs learn to generate rationales at each token to explain future text,
improving their predictions. We address key challenges, including 1) the
computational cost of generating continuations, 2) the fact that the LM does
not initially know how to generate or use internal thoughts, and 3) the need to
predict beyond individual next tokens. To resolve these, we propose a tokenwise
parallel sampling algorithm, using learnable tokens indicating a thought's
start and end, and an extended teacher-forcing technique. Encouragingly,
generated rationales disproportionately help model difficult-to-predict tokens
and improve the LM's ability to directly answer difficult questions. In
particular, after continued pretraining of an LM on a corpus of internet text
with Quiet-STaR, we find zero-shot improvements on GSM8K
(5.9%$\rightarrow$10.9%) and CommonsenseQA (36.3%$\rightarrow$47.2%) and
observe a perplexity improvement of difficult tokens in natural text.
Crucially, these improvements require no fine-tuning on these tasks. Quiet-STaR
marks a step towards LMs that can learn to reason in a more general and
scalable way.",0
Make-Your-3D: Fast and Consistent Subject-Driven 3D Content Generation,2403.09625v1,http://arxiv.org/abs/2403.09625v1,2024-03-14 17:57:04+00:00,"Recent years have witnessed the strong power of 3D generation models, which
offer a new level of creative flexibility by allowing users to guide the 3D
content generation process through a single image or natural language. However,
it remains challenging for existing 3D generation methods to create
subject-driven 3D content across diverse prompts. In this paper, we introduce a
novel 3D customization method, dubbed Make-Your-3D that can personalize
high-fidelity and consistent 3D content from only a single image of a subject
with text description within 5 minutes. Our key insight is to harmonize the
distributions of a multi-view diffusion model and an identity-specific 2D
generative model, aligning them with the distribution of the desired 3D
subject. Specifically, we design a co-evolution framework to reduce the
variance of distributions, where each model undergoes a process of learning
from the other through identity-aware optimization and subject-prior
optimization, respectively. Extensive experiments demonstrate that our method
can produce high-quality, consistent, and subject-specific 3D content with
text-driven modifications that are unseen in subject image.",0
Minimax Optimal and Computationally Efficient Algorithms for Distributionally Robust Offline Reinforcement Learning,2403.09621v1,http://arxiv.org/abs/2403.09621v1,2024-03-14 17:55:10+00:00,"Distributionally robust offline reinforcement learning (RL), which seeks
robust policy training against environment perturbation by modeling dynamics
uncertainty, calls for function approximations when facing large state-action
spaces. However, the consideration of dynamics uncertainty introduces essential
nonlinearity and computational burden, posing unique challenges for analyzing
and practically employing function approximation. Focusing on a basic setting
where the nominal model and perturbed models are linearly parameterized, we
propose minimax optimal and computationally efficient algorithms realizing
function approximation and initiate the study on instance-dependent
suboptimality analysis in the context of robust offline RL. Our results uncover
that function approximation in robust offline RL is essentially distinct from
and probably harder than that in standard offline RL. Our algorithms and
theoretical results crucially depend on a variety of new techniques, involving
a novel function approximation mechanism incorporating variance information, a
new procedure of suboptimality and estimation uncertainty decomposition, a
quantification of the robust value function shrinkage, and a meticulously
designed family of hard instances, which might be of independent interest.",0
Explore In-Context Segmentation via Latent Diffusion Models,2403.09616v1,http://arxiv.org/abs/2403.09616v1,2024-03-14 17:52:31+00:00,"In-context segmentation has drawn more attention with the introduction of
vision foundation models. Most existing approaches adopt metric learning or
masked image modeling to build the correlation between visual prompts and input
image queries. In this work, we explore this problem from a new perspective,
using one representative generation model, the latent diffusion model (LDM). We
observe a task gap between generation and segmentation in diffusion models, but
LDM is still an effective minimalist for in-context segmentation. In
particular, we propose two meta-architectures and correspondingly design
several output alignment and optimization strategies. We have conducted
comprehensive ablation studies and empirically found that the segmentation
quality counts on output alignment and in-context instructions. Moreover, we
build a new and fair in-context segmentation benchmark that includes both image
and video datasets. Experiments validate the efficiency of our approach,
demonstrating comparable or even stronger results than previous specialist
models or visual foundation models. Our study shows that LDMs can also achieve
good enough results for challenging in-context segmentation tasks.",0
Reawakening knowledge: Anticipatory recovery from catastrophic interference via structured training,2403.09613v1,http://arxiv.org/abs/2403.09613v1,2024-03-14 17:51:54+00:00,"We explore the training dynamics of neural networks in a structured non-IID
setting where documents are presented cyclically in a fixed, repeated sequence.
Typically, networks suffer from catastrophic interference when training on a
sequence of documents; however, we discover a curious and remarkable property
of LLMs fine-tuned sequentially in this setting: they exhibit anticipatory
behavior, recovering from the forgetting on documents before encountering them
again. The behavior emerges and becomes more robust as the architecture scales
up its number of parameters. Through comprehensive experiments and
visualizations, we uncover new insights into training over-parameterized
networks in structured environments.",0
Compute-first optical detection for noise-resilient visual perception,2403.09612v1,http://arxiv.org/abs/2403.09612v1,2024-03-14 17:51:38+00:00,"In the context of visual perception, the optical signal from a scene is
transferred into the electronic domain by detectors in the form of image data,
which are then processed for the extraction of visual information. In noisy and
weak-signal environments such as thermal imaging for night vision applications,
however, the performance of neural computing tasks faces a significant
bottleneck due to the inherent degradation of data quality upon noisy
detection. Here, we propose a concept of optical signal processing before
detection to address this issue. We demonstrate that spatially redistributing
optical signals through a properly designed linear transformer can enhance the
detection noise resilience of visual perception tasks, as benchmarked with the
MNIST classification. Our idea is supported by a quantitative analysis
detailing the relationship between signal concentration and noise robustness,
as well as its practical implementation in an incoherent imaging system. This
compute-first detection scheme can pave the way for advancing infrared machine
vision technologies widely used for industrial and defense applications.",0
"MM1: Methods, Analysis & Insights from Multimodal LLM Pre-training",2403.09611v1,http://arxiv.org/abs/2403.09611v1,2024-03-14 17:51:32+00:00,"In this work, we discuss building performant Multimodal Large Language Models
(MLLMs). In particular, we study the importance of various architecture
components and data choices. Through careful and comprehensive ablations of the
image encoder, the vision language connector, and various pre-training data
choices, we identified several crucial design lessons. For example, we
demonstrate that for large-scale multimodal pre-training using a careful mix of
image-caption, interleaved image-text, and text-only data is crucial for
achieving state-of-the-art (SOTA) few-shot results across multiple benchmarks,
compared to other published pre-training results. Further, we show that the
image encoder together with image resolution and the image token count has
substantial impact, while the vision-language connector design is of
comparatively negligible importance. By scaling up the presented recipe, we
build MM1, a family of multimodal models up to 30B parameters, consisting of
both dense models and mixture-of-experts (MoE) variants, that are SOTA in
pre-training metrics and achieve competitive performance after supervised
fine-tuning on a range of established multimodal benchmarks. Thanks to
large-scale pre-training, MM1 enjoys appealing properties such as enhanced
in-context learning, and multi-image reasoning, enabling few-shot
chain-of-thought prompting.",0
Signal Recovery with Proximal Comixtures,2403.09610v1,http://arxiv.org/abs/2403.09610v1,2024-03-14 17:50:51+00:00,"In variational signal processing and machine learning problems, loss
functions and linear operators are typically aggregated as an average of
composite terms. We propose an alternative formulation using proximal
comixtures, an operation that combines functions and linear operators in such a
way that the proximity operator of the resulting function is computable
explicitly. The benefits of comixture formulations are illustrated through
image recovery and machine learning applications.",0
Counterfactual contrastive learning: robust representations via causal image synthesis,2403.09605v1,http://arxiv.org/abs/2403.09605v1,2024-03-14 17:47:01+00:00,"Contrastive pretraining is well-known to improve downstream task performance
and model generalisation, especially in limited label settings. However, it is
sensitive to the choice of augmentation pipeline. Positive pairs should
preserve semantic information while destroying domain-specific information.
Standard augmentation pipelines emulate domain-specific changes with
pre-defined photometric transformations, but what if we could simulate
realistic domain changes instead? In this work, we show how to utilise recent
progress in counterfactual image generation to this effect. We propose
CF-SimCLR, a counterfactual contrastive learning approach which leverages
approximate counterfactual inference for positive pair creation. Comprehensive
evaluation across five datasets, on chest radiography and mammography,
demonstrates that CF-SimCLR substantially improves robustness to acquisition
shift with higher downstream performance on both in- and out-of-distribution
data, particularly for domains which are under-represented during training.",0
Extremal graphical modeling with latent variables,2403.09604v1,http://arxiv.org/abs/2403.09604v1,2024-03-14 17:45:24+00:00,"Extremal graphical models encode the conditional independence structure of
multivariate extremes and provide a powerful tool for quantifying the risk of
rare events. Prior work on learning these graphs from data has focused on the
setting where all relevant variables are observed. For the popular class of
H\""usler-Reiss models, we propose the \texttt{eglatent} method, a tractable
convex program for learning extremal graphical models in the presence of latent
variables. Our approach decomposes the H\""usler-Reiss precision matrix into a
sparse component encoding the graphical structure among the observed variables
after conditioning on the latent variables, and a low-rank component encoding
the effect of a few latent variables on the observed variables. We provide
finite-sample guarantees of \texttt{eglatent} and show that it consistently
recovers the conditional graph as well as the number of latent variables. We
highlight the improved performances of our approach on synthetic and real data.",0
Optimistic Verifiable Training by Controlling Hardware Nondeterminism,2403.09603v1,http://arxiv.org/abs/2403.09603v1,2024-03-14 17:44:35+00:00,"The increasing compute demands of AI systems has led to the emergence of
services that train models on behalf of clients lacking necessary resources.
However, ensuring correctness of training and guarding against potential
training-time attacks, such as data poisoning, poses challenges. Existing works
on verifiable training largely fall into two classes: proof-based systems,
which struggle to scale due to requiring cryptographic techniques, and
""optimistic"" methods that consider a trusted third-party auditor who replicates
the training process. A key challenge with the latter is that hardware
nondeterminism between GPU types during training prevents an auditor from
replicating the training process exactly, and such schemes are therefore
non-robust. We propose a method that combines training in a higher precision
than the target model, rounding after intermediate computation steps, and
storing rounding decisions based on an adaptive thresholding procedure, to
successfully control for nondeterminism. Across three different NVIDIA GPUs
(A40, Titan XP, RTX 2080 Ti), we achieve exact training replication at FP32
precision for both full-training and fine-tuning of ResNet-50 (23M) and GPT-2
(117M) models. Our verifiable training scheme significantly decreases the
storage and time costs compared to proof-based systems.",0
Mixture of Mixups for Multi-label Classification of Rare Anuran Sounds,2403.09598v1,http://arxiv.org/abs/2403.09598v1,2024-03-14 17:39:14+00:00,"Multi-label imbalanced classification poses a significant challenge in
machine learning, particularly evident in bioacoustics where animal sounds
often co-occur, and certain sounds are much less frequent than others. This
paper focuses on the specific case of classifying anuran species sounds using
the dataset AnuraSet, that contains both class imbalance and multi-label
examples. To address these challenges, we introduce Mixture of Mixups (Mix2), a
framework that leverages mixing regularization methods Mixup, Manifold Mixup,
and MultiMix. Experimental results show that these methods, individually, may
lead to suboptimal results; however, when applied randomly, with one selected
at each training iteration, they prove effective in addressing the mentioned
challenges, particularly for rare classes with few occurrences. Further
analysis reveals that Mix2 is also proficient in classifying sounds across
various levels of class co-occurrences.",0
Iterative Forgetting: Online Data Stream Regression Using Database-Inspired Adaptive Granulation,2403.09588v1,http://arxiv.org/abs/2403.09588v1,2024-03-14 17:26:00+00:00,"Many modern systems, such as financial, transportation, and
telecommunications systems, are time-sensitive in the sense that they demand
low-latency predictions for real-time decision-making. Such systems often have
to contend with continuous unbounded data streams as well as concept drift,
which are challenging requirements that traditional regression techniques are
unable to cater to. There exists a need to create novel data stream regression
methods that can handle these scenarios. We present a database-inspired
datastream regression model that (a) uses inspiration from R*-trees to create
granules from incoming datastreams such that relevant information is retained,
(b) iteratively forgets granules whose information is deemed to be outdated,
thus maintaining a list of only recent, relevant granules, and (c) uses the
recent data and granules to provide low-latency predictions. The
R*-tree-inspired approach also makes the algorithm amenable to integration with
database systems. Our experiments demonstrate that the ability of this method
to discard data produces a significant order-of-magnitude improvement in
latency and training time when evaluated against the most accurate
state-of-the-art algorithms, while the R*-tree-inspired granulation technique
provides competitively accurate predictions",0
ExploRLLM: Guiding Exploration in Reinforcement Learning with Large Language Models,2403.09583v1,http://arxiv.org/abs/2403.09583v1,2024-03-14 17:18:15+00:00,"In image-based robot manipulation tasks with large observation and action
spaces, reinforcement learning struggles with low sample efficiency, slow
training speed, and uncertain convergence. As an alternative, large pre-trained
foundation models have shown promise in robotic manipulation, particularly in
zero-shot and few-shot applications. However, using these models directly is
unreliable due to limited reasoning capabilities and challenges in
understanding physical and spatial contexts. This paper introduces ExploRLLM, a
novel approach that leverages the inductive bias of foundation models (e.g.
Large Language Models) to guide exploration in reinforcement learning. We also
exploit these foundation models to reformulate the action and observation
spaces to enhance the training efficiency in reinforcement learning. Our
experiments demonstrate that guided exploration enables much quicker
convergence than training without it. Additionally, we validate that ExploRLLM
outperforms vanilla foundation model baselines and that the policy trained in
simulation can be applied in real-world settings without additional training.",0
Algorithmic syntactic causal identification,2403.09580v1,http://arxiv.org/abs/2403.09580v1,2024-03-14 17:14:53+00:00,"Causal identification in causal Bayes nets (CBNs) is an important tool in
causal inference allowing the derivation of interventional distributions from
observational distributions where this is possible in principle. However, most
existing formulations of causal identification using techniques such as
d-separation and do-calculus are expressed within the mathematical language of
classical probability theory on CBNs. However, there are many causal settings
where probability theory and hence current causal identification techniques are
inapplicable such as relational databases, dataflow programs such as hardware
description languages, distributed systems and most modern machine learning
algorithms. We show that this restriction can be lifted by replacing the use of
classical probability theory with the alternative axiomatic foundation of
symmetric monoidal categories. In this alternative axiomatization, we show how
an unambiguous and clean distinction can be drawn between the general syntax of
causal models and any specific semantic implementation of that causal model.
This allows a purely syntactic algorithmic description of general causal
identification by a translation of recent formulations of the general ID
algorithm through fixing. Our description is given entirely in terms of the
non-parametric ADMG structure specifying a causal model and the algebraic
signature of the corresponding monoidal category, to which a sequence of
manipulations is then applied so as to arrive at a modified monoidal category
in which the desired, purely syntactic interventional causal model, is
obtained. We use this idea to derive purely syntactic analogues of classical
back-door and front-door causal adjustment, and illustrate an application to a
more complex causal model.",0
uaMix-MAE: Efficient Tuning of Pretrained Audio Transformers with Unsupervised Audio Mixtures,2403.09579v1,http://arxiv.org/abs/2403.09579v1,2024-03-14 17:13:37+00:00,"Masked Autoencoders (MAEs) learn rich low-level representations from
unlabeled data but require substantial labeled data to effectively adapt to
downstream tasks. Conversely, Instance Discrimination (ID) emphasizes
high-level semantics, offering a potential solution to alleviate annotation
requirements in MAEs. Although combining these two approaches can address
downstream tasks with limited labeled data, naively integrating ID into MAEs
leads to extended training times and high computational costs. To address this
challenge, we introduce uaMix-MAE, an efficient ID tuning strategy that
leverages unsupervised audio mixtures. Utilizing contrastive tuning, uaMix-MAE
aligns the representations of pretrained MAEs, thereby facilitating effective
adaptation to task-specific semantics. To optimize the model with small amounts
of unlabeled data, we propose an audio mixing technique that manipulates audio
samples in both input and virtual label spaces. Experiments in low/few-shot
settings demonstrate that \modelname achieves 4-6% accuracy improvements over
various benchmarks when tuned with limited unlabeled data, such as
AudioSet-20K. Code is available at https://github.com/PLAN-Lab/uamix-MAE",0
The NeRFect Match: Exploring NeRF Features for Visual Localization,2403.09577v1,http://arxiv.org/abs/2403.09577v1,2024-03-14 17:11:49+00:00,"In this work, we propose the use of Neural Radiance Fields (NeRF) as a scene
representation for visual localization. Recently, NeRF has been employed to
enhance pose regression and scene coordinate regression models by augmenting
the training database, providing auxiliary supervision through rendered images,
or serving as an iterative refinement module. We extend its recognized
advantages -- its ability to provide a compact scene representation with
realistic appearances and accurate geometry -- by exploring the potential of
NeRF's internal features in establishing precise 2D-3D matches for
localization. To this end, we conduct a comprehensive examination of NeRF's
implicit knowledge, acquired through view synthesis, for matching under various
conditions. This includes exploring different matching network architectures,
extracting encoder features at multiple layers, and varying training
configurations. Significantly, we introduce NeRFMatch, an advanced 2D-3D
matching function that capitalizes on the internal knowledge of NeRF learned
via view synthesis. Our evaluation of NeRFMatch on standard localization
benchmarks, within a structure-based pipeline, sets a new state-of-the-art for
localization performance on Cambridge Landmarks.",0
Learning High-Order Control Barrier Functions for Safety-Critical Control with Gaussian Processes,2403.09573v1,http://arxiv.org/abs/2403.09573v1,2024-03-14 17:03:11+00:00,"Control barrier functions (CBFs) have recently introduced a systematic tool
to ensure system safety by establishing set invariance. When combined with a
nominal control strategy, they form a safety-critical control mechanism.
However, the effectiveness of CBFs is closely tied to the system model. In
practice, model uncertainty can compromise safety guarantees and may lead to
conservative safety constraints, or conversely, allow the system to operate in
unsafe regions. In this paper, we use Gaussian processes to mitigate the
adverse effects of uncertainty on high-order CBFs (HOCBFs). A properly
structured covariance function enables us to convert the chance constraints of
HOCBFs into a second-order cone constraint. This results in a convex
constrained optimization as a safety filter. We analyze the feasibility of the
resulting optimization and provide the necessary and sufficient conditions for
feasibility. The effectiveness of the proposed strategy is validated through
two numerical results.",0
Are you a robot? Detecting Autonomous Vehicles from Behavior Analysis,2403.09571v1,http://arxiv.org/abs/2403.09571v1,2024-03-14 17:00:29+00:00,"The tremendous hype around autonomous driving is eagerly calling for emerging
and novel technologies to support advanced mobility use cases. As car
manufactures keep developing SAE level 3+ systems to improve the safety and
comfort of passengers, traffic authorities need to establish new procedures to
manage the transition from human-driven to fully-autonomous vehicles while
providing a feedback-loop mechanism to fine-tune envisioned autonomous systems.
Thus, a way to automatically profile autonomous vehicles and differentiate
those from human-driven ones is a must. In this paper, we present a
fully-fledged framework that monitors active vehicles using camera images and
state information in order to determine whether vehicles are autonomous,
without requiring any active notification from the vehicles themselves.
Essentially, it builds on the cooperation among vehicles, which share their
data acquired on the road feeding a machine learning model to identify
autonomous cars. We extensively tested our solution and created the NexusStreet
dataset, by means of the CARLA simulator, employing an autonomous driving
control agent and a steering wheel maneuvered by licensed drivers. Experiments
show it is possible to discriminate the two behaviors by analyzing video clips
with an accuracy of 80%, which improves up to 93% when the target state
information is available. Lastly, we deliberately degraded the state to observe
how the framework performs under non-ideal data collection conditions.",0
Multi-Fidelity Bayesian Optimization With Across-Task Transferable Max-Value Entropy Search,2403.09570v1,http://arxiv.org/abs/2403.09570v1,2024-03-14 17:00:01+00:00,"In many applications, ranging from logistics to engineering, a designer is
faced with a sequence of optimization tasks for which the objectives are in the
form of black-box functions that are costly to evaluate. For example, the
designer may need to tune the hyperparameters of neural network models for
different learning tasks over time. Rather than evaluating the objective
function for each candidate solution, the designer may have access to
approximations of the objective functions, for which higher-fidelity
evaluations entail a larger cost. Existing multi-fidelity black-box
optimization strategies select candidate solutions and fidelity levels with the
goal of maximizing the information accrued about the optimal value or solution
for the current task. Assuming that successive optimization tasks are related,
this paper introduces a novel information-theoretic acquisition function that
balances the need to acquire information about the current task with the goal
of collecting information transferable to future tasks. The proposed method
includes shared inter-task latent variables, which are transferred across tasks
by implementing particle-based variational Bayesian updates. Experimental
results across synthetic and real-world examples reveal that the proposed
provident acquisition strategy that caters to future tasks can significantly
improve the optimization efficiency as soon as a sufficient number of tasks is
processed.",0
PaperBot: Learning to Design Real-World Tools Using Paper,2403.09566v1,http://arxiv.org/abs/2403.09566v1,2024-03-14 16:56:56+00:00,"Paper is a cheap, recyclable, and clean material that is often used to make
practical tools. Traditional tool design either relies on simulation or
physical analysis, which is often inaccurate and time-consuming. In this paper,
we propose PaperBot, an approach that directly learns to design and use a tool
in the real world using paper without human intervention. We demonstrated the
effectiveness and efficiency of PaperBot on two tool design tasks: 1. learning
to fold and throw paper airplanes for maximum travel distance 2. learning to
cut paper into grippers that exert maximum gripping force. We present a
self-supervised learning framework that learns to perform a sequence of
folding, cutting, and dynamic manipulation actions in order to optimize the
design and use of a tool. We deploy our system to a real-world two-arm robotic
system to solve challenging design tasks that involve aerodynamics (paper
airplane) and friction (paper gripper) that are impossible to simulate
accurately.",0
Characterization of Polarimetric Properties in Various Brain Tumor Types Using Wide-Field Imaging Mueller Polarimetry,2403.09561v1,http://arxiv.org/abs/2403.09561v1,2024-03-14 16:53:12+00:00,"Neuro-oncological surgery is the primary brain cancer treatment, yet it faces
challenges with gliomas due to their invasiveness and the need to preserve
neurological function. Hence, radical resection is often unfeasible,
highlighting the importance of precise tumor margin delineation to prevent
neurological deficits and improve prognosis. Imaging Mueller polarimetry, an
effective modality in various organ tissues, seems a promising approach for
tumor delineation in neurosurgery. To further assess its use, we characterized
the polarimetric properties by analysing 45 polarimetric measurements of 27
fresh brain tumor samples, including different tumor types with a strong focus
on gliomas. Our study integrates a wide-field imaging Mueller polarimetric
system and a novel neuropathology protocol, correlating polarimetric and
histological data for accurate tissue identification. An image processing
pipeline facilitated the alignment and overlay of polarimetric images and
histological masks. Variations in depolarization values were observed for grey
and white matter of brain tumor tissue, while differences in linear retardance
were seen only within white matter of brain tumor tissue. Notably, we
identified pronounced optical axis azimuth randomization within tumor regions.
This study lays the foundation for machine learning-based brain tumor
segmentation algorithms using polarimetric data, facilitating intraoperative
diagnosis and decision making.",0
Self-Consistency Training for Hamiltonian Prediction,2403.09560v1,http://arxiv.org/abs/2403.09560v1,2024-03-14 16:52:57+00:00,"Hamiltonian prediction is a versatile formulation to leverage machine
learning for solving molecular science problems. Yet, its applicability is
limited by insufficient labeled data for training. In this work, we highlight
that Hamiltonian prediction possesses a self-consistency principle, based on
which we propose an exact training method that does not require labeled data.
This merit addresses the data scarcity difficulty, and distinguishes the task
from other property prediction formulations with unique benefits: (1)
self-consistency training enables the model to be trained on a large amount of
unlabeled data, hence substantially enhances generalization; (2)
self-consistency training is more efficient than labeling data with DFT for
supervised training, since it is an amortization of DFT calculation over a set
of molecular structures. We empirically demonstrate the better generalization
in data-scarce and out-of-distribution scenarios, and the better efficiency
from the amortization. These benefits push forward the applicability of
Hamiltonian prediction to an ever larger scale.",0
Cloud gap-filling with deep learning for improved grassland monitoring,2403.09554v1,http://arxiv.org/abs/2403.09554v1,2024-03-14 16:41:26+00:00,"Uninterrupted optical image time series are crucial for the timely monitoring
of agricultural land changes. However, the continuity of such time series is
often disrupted by clouds. In response to this challenge, we propose a deep
learning method that integrates cloud-free optical (Sentinel-2) observations
and weather-independent (Sentinel-1) Synthetic Aperture Radar (SAR) data, using
a combined Convolutional Neural Network (CNN)-Recurrent Neural Network (RNN)
architecture to generate continuous Normalized Difference Vegetation Index
(NDVI) time series. We emphasize the significance of observation continuity by
assessing the impact of the generated time series on the detection of grassland
mowing events. We focus on Lithuania, a country characterized by extensive
cloud coverage, and compare our approach with alternative interpolation
techniques (i.e., linear, Akima, quadratic). Our method surpasses these
techniques, with an average MAE of 0.024 and R^2 of 0.92. It not only improves
the accuracy of event detection tasks by employing a continuous time series,
but also effectively filters out sudden shifts and noise originating from
cloudy observations that cloud masks often fail to detect.",0
The Neural-SRP method for positional sound source localization,2403.09455v1,http://arxiv.org/abs/2403.09455v1,2024-03-14 14:53:54+00:00,"Steered Response Power (SRP) is a widely used method for the task of sound
source localization using microphone arrays, showing satisfactory localization
performance on many practical scenarios. However, its performance is diminished
under highly reverberant environments. Although Deep Neural Networks (DNNs)
have been previously proposed to overcome this limitation, most are trained for
a specific number of microphones with fixed spatial coordinates. This restricts
their practical application on scenarios frequently observed in wireless
acoustic sensor networks, where each application has an ad-hoc microphone
topology. We propose Neural-SRP, a DNN which combines the flexibility of SRP
with the performance gains of DNNs. We train our network using simulated data
and transfer learning, and evaluate our approach on recorded and simulated
data. Results verify that Neural-SRP's localization performance significantly
outperforms the baselines.",0
Machine learning for structural design models of continuous beam systems via influence zones,2403.09454v1,http://arxiv.org/abs/2403.09454v1,2024-03-14 14:53:18+00:00,"This work develops a machine learned structural design model for continuous
beam systems from the inverse problem perspective. After demarcating between
forward, optimisation and inverse machine learned operators, the investigation
proposes a novel methodology based on the recently developed influence zone
concept which represents a fundamental shift in approach compared to
traditional structural design methods. The aim of this approach is to
conceptualise a non-iterative structural design model that predicts
cross-section requirements for continuous beam systems of arbitrary system
size. After generating a dataset of known solutions, an appropriate neural
network architecture is identified, trained, and tested against unseen data.
The results show a mean absolute percentage testing error of 1.6% for
cross-section property predictions, along with a good ability of the neural
network to generalise well to structural systems of variable size. The CBeamXP
dataset generated in this work and an associated python-based neural network
training script are available at an open-source data repository to allow for
the reproducibility of results and to encourage further investigations.",0
M&M: Multimodal-Multitask Model Integrating Audiovisual Cues in Cognitive Load Assessment,2403.09451v1,http://arxiv.org/abs/2403.09451v1,2024-03-14 14:49:40+00:00,"This paper introduces the M&M model, a novel multimodal-multitask learning
framework, applied to the AVCAffe dataset for cognitive load assessment (CLA).
M&M uniquely integrates audiovisual cues through a dual-pathway architecture,
featuring specialized streams for audio and video inputs. A key innovation lies
in its cross-modality multihead attention mechanism, fusing the different
modalities for synchronized multitasking. Another notable feature is the
model's three specialized branches, each tailored to a specific cognitive load
label, enabling nuanced, task-specific analysis. While it shows modest
performance compared to the AVCAffe's single-task baseline, M\&M demonstrates a
promising framework for integrated multimodal processing. This work paves the
way for future enhancements in multimodal-multitask learning systems,
emphasizing the fusion of diverse data types for complex task handling.",0
SketchINR: A First Look into Sketches as Implicit Neural Representations,2403.09344v1,http://arxiv.org/abs/2403.09344v1,2024-03-14 12:49:29+00:00,"We propose SketchINR, to advance the representation of vector sketches with
implicit neural models. A variable length vector sketch is compressed into a
latent space of fixed dimension that implicitly encodes the underlying shape as
a function of time and strokes. The learned function predicts the $xy$ point
coordinates in a sketch at each time and stroke. Despite its simplicity,
SketchINR outperforms existing representations at multiple tasks: (i) Encoding
an entire sketch dataset into a fixed size latent vector, SketchINR gives
$60\times$ and $10\times$ data compression over raster and vector sketches,
respectively. (ii) SketchINR's auto-decoder provides a much higher-fidelity
representation than other learned vector sketch representations, and is
uniquely able to scale to complex vector sketches such as FS-COCO. (iii)
SketchINR supports parallelisation that can decode/render $\sim$$100\times$
faster than other learned vector representations such as SketchRNN. (iv)
SketchINR, for the first time, emulates the human ability to reproduce a sketch
with varying abstraction in terms of number and complexity of strokes. As a
first look at implicit sketches, SketchINR's compact high-fidelity
representation will support future work in modelling long and complex sketches.",0
Perspective-Equivariant Imaging: an Unsupervised Framework for Multispectral Pansharpening,2403.09327v1,http://arxiv.org/abs/2403.09327v1,2024-03-14 12:17:07+00:00,"Ill-posed image reconstruction problems appear in many scenarios such as
remote sensing, where obtaining high quality images is crucial for
environmental monitoring, disaster management and urban planning. Deep learning
has seen great success in overcoming the limitations of traditional methods.
However, these inverse problems rarely come with ground truth data,
highlighting the importance of unsupervised learning from partial and noisy
measurements alone. We propose perspective-equivariant imaging (EI), a
framework that leverages perspective variability in optical camera-based
imaging systems, such as satellites or handheld cameras, to recover information
lost in ill-posed optical camera imaging problems. This extends previous EI
work to include a much richer non-linear class of group transforms and is shown
to be an excellent prior for satellite and urban image data, where
perspective-EI achieves state-of-the-art results in multispectral
pansharpening, outperforming other unsupervised methods in the literature. Code
at https://andrewwango.github.io/perspective-equivariant-imaging",0
EfficientMFD: Towards More Efficient Multimodal Synchronous Fusion Detection,2403.09323v1,http://arxiv.org/abs/2403.09323v1,2024-03-14 12:12:17+00:00,"Multimodal image fusion and object detection play a vital role in autonomous
driving. Current joint learning methods have made significant progress in the
multimodal fusion detection task combining the texture detail and objective
semantic information. However, the tedious training steps have limited its
applications to wider real-world industrial deployment. To address this
limitation, we propose a novel end-to-end multimodal fusion detection
algorithm, named EfficientMFD, to simplify models that exhibit decent
performance with only one training step. Synchronous joint optimization is
utilized in an end-to-end manner between two components, thus not being
affected by the local optimal solution of the individual task. Besides, a
comprehensive optimization is established in the gradient matrix between the
shared parameters for both tasks. It can converge to an optimal point with
fusion detection weights. We extensively test it on several public datasets,
demonstrating superior performance on not only visually appealing fusion but
also favorable detection performance (e.g., 6.6% mAP50:95) over other
state-of-the-art approaches.",0
A Hierarchical Fused Quantum Fuzzy Neural Network for Image Classification,2403.09318v1,http://arxiv.org/abs/2403.09318v1,2024-03-14 12:09:36+00:00,"Neural network is a powerful learning paradigm for data feature learning in
the era of big data. However, most neural network models are deterministic
models that ignore the uncertainty of data. Fuzzy neural networks are proposed
to address this problem. FDNN is a hierarchical deep neural network that
derives information from both fuzzy and neural representations, the
representations are then fused to form representation to be classified. FDNN
perform well on uncertain data classification tasks. In this paper, we proposed
a novel hierarchical fused quantum fuzzy neural network (HQFNN). Different from
classical FDNN, HQFNN uses quantum neural networks to learn fuzzy membership
functions in fuzzy neural network. We conducted simulated experiment on two
types of datasets (Dirty-MNIST and 15-Scene), the results show that the
proposed model can outperform several existing methods. In addition, we
demonstrate the robustness of the proposed quantum circuit.",0
SD-Net: Symmetric-Aware Keypoint Prediction and Domain Adaptation for 6D Pose Estimation In Bin-picking Scenarios,2403.09317v1,http://arxiv.org/abs/2403.09317v1,2024-03-14 12:08:44+00:00,"Despite the success in 6D pose estimation in bin-picking scenarios, existing
methods still struggle to produce accurate prediction results for symmetry
objects and real world scenarios. The primary bottlenecks include 1) the
ambiguity keypoints caused by object symmetries; 2) the domain gap between real
and synthetic data. To circumvent these problem, we propose a new 6D pose
estimation network with symmetric-aware keypoint prediction and self-training
domain adaptation (SD-Net). SD-Net builds on pointwise keypoint regression and
deep hough voting to perform reliable detection keypoint under clutter and
occlusion. Specifically, at the keypoint prediction stage, we designe a robust
3D keypoints selection strategy considering the symmetry class of objects and
equivalent keypoints, which facilitate locating 3D keypoints even in highly
occluded scenes. Additionally, we build an effective filtering algorithm on
predicted keypoint to dynamically eliminate multiple ambiguity and outlier
keypoint candidates. At the domain adaptation stage, we propose the
self-training framework using a student-teacher training scheme. To carefully
distinguish reliable predictions, we harnesses a tailored heuristics for 3D
geometry pseudo labelling based on semi-chamfer distance. On public Sil'eane
dataset, SD-Net achieves state-of-the-art results, obtaining an average
precision of 96%. Testing learning and generalization abilities on public
Parametric datasets, SD-Net is 8% higher than the state-of-the-art method. The
code is available at https://github.com/dingthuang/SD-Net.",0
Semi- and Weakly-Supervised Learning for Mammogram Mass Segmentation with Limited Annotations,2403.09315v1,http://arxiv.org/abs/2403.09315v1,2024-03-14 12:05:25+00:00,"Accurate identification of breast masses is crucial in diagnosing breast
cancer; however, it can be challenging due to their small size and being
camouflaged in surrounding normal glands. Worse still, it is also expensive in
clinical practice to obtain adequate pixel-wise annotations for training deep
neural networks. To overcome these two difficulties with one stone, we propose
a semi- and weakly-supervised learning framework for mass segmentation that
utilizes limited strongly-labeled samples and sufficient weakly-labeled samples
to achieve satisfactory performance. The framework consists of an auxiliary
branch to exclude lesion-irrelevant background areas, a segmentation branch for
final prediction, and a spatial prompting module to integrate the complementary
information of the two branches. We further disentangle encoded obscure
features into lesion-related and others to boost performance. Experiments on
CBIS-DDSM and INbreast datasets demonstrate the effectiveness of our method.",0
MOTPose: Multi-object 6D Pose Estimation for Dynamic Video Sequences using Attention-based Temporal Fusion,2403.09309v1,http://arxiv.org/abs/2403.09309v1,2024-03-14 11:59:32+00:00,"Cluttered bin-picking environments are challenging for pose estimation
models. Despite the impressive progress enabled by deep learning, single-view
RGB pose estimation models perform poorly in cluttered dynamic environments.
Imbuing the rich temporal information contained in the video of scenes has the
potential to enhance models ability to deal with the adverse effects of
occlusion and the dynamic nature of the environments. Moreover, joint object
detection and pose estimation models are better suited to leverage the
co-dependent nature of the tasks for improving the accuracy of both tasks. To
this end, we propose attention-based temporal fusion for multi-object 6D pose
estimation that accumulates information across multiple frames of a video
sequence. Our MOTPose method takes a sequence of images as input and performs
joint object detection and pose estimation for all objects in one forward pass.
It learns to aggregate both object embeddings and object parameters over
multiple time steps using cross-attention-based fusion modules. We evaluate our
method on the physically-realistic cluttered bin-picking dataset SynPick and
the YCB-Video dataset and demonstrate improved pose estimation accuracy as well
as better object detection accuracy",0
Enabling Waypoint Generation for Collaborative Robots using LLMs and Mixed Reality,2403.09308v1,http://arxiv.org/abs/2403.09308v1,2024-03-14 11:59:07+00:00,"Programming a robotic is a complex task, as it demands the user to have a
good command of specific programming languages and awareness of the robot's
physical constraints. We propose a framework that simplifies robot deployment
by allowing direct communication using natural language. It uses large language
models (LLM) for prompt processing, workspace understanding, and waypoint
generation. It also employs Augmented Reality (AR) to provide visual feedback
of the planned outcome. We showcase the effectiveness of our framework with a
simple pick-and-place task, which we implement on a real robot. Moreover, we
present an early concept of expressive robot behavior and skill generation that
can be used to communicate with the user and learn new skills (e.g., object
grasping).",0
Rethinking Autoencoders for Medical Anomaly Detection from A Theoretical Perspective,2403.09303v1,http://arxiv.org/abs/2403.09303v1,2024-03-14 11:51:01+00:00,"Medical anomaly detection aims to identify abnormal findings using only
normal training data, playing a crucial role in health screening and
recognizing rare diseases. Reconstruction-based methods, particularly those
utilizing autoencoders (AEs), are dominant in this field. They work under the
assumption that AEs trained on only normal data cannot reconstruct unseen
abnormal regions well, thereby enabling the anomaly detection based on
reconstruction errors. However, this assumption does not always hold due to the
mismatch between the reconstruction training objective and the anomaly
detection task objective, rendering these methods theoretically unsound. This
study focuses on providing a theoretical foundation for AE-based reconstruction
methods in anomaly detection. By leveraging information theory, we elucidate
the principles of these methods and reveal that the key to improving AE in
anomaly detection lies in minimizing the information entropy of latent vectors.
Experiments on four datasets with two image modalities validate the
effectiveness of our theory. To the best of our knowledge, this is the first
effort to theoretically clarify the principles and design philosophy of AE for
anomaly detection. Code will be available upon acceptance.",0
StainFuser: Controlling Diffusion for Faster Neural Style Transfer in Multi-Gigapixel Histology Images,2403.09302v1,http://arxiv.org/abs/2403.09302v1,2024-03-14 11:49:43+00:00,"Stain normalization algorithms aim to transform the color and intensity
characteristics of a source multi-gigapixel histology image to match those of a
target image, mitigating inconsistencies in the appearance of stains used to
highlight cellular components in the images. We propose a new approach,
StainFuser, which treats this problem as a style transfer task using a novel
Conditional Latent Diffusion architecture, eliminating the need for handcrafted
color components. With this method, we curate SPI-2M the largest stain
normalization dataset to date of over 2 million histology images with neural
style transfer for high-quality transformations. Trained on this data,
StainFuser outperforms current state-of-the-art GAN and handcrafted methods in
terms of the quality of normalized images. Additionally, compared to existing
approaches, it improves the performance of nuclei instance segmentation and
classification models when used as a test time augmentation method on the
challenging CoNIC dataset. Finally, we apply StainFuser on multi-gigapixel
Whole Slide Images (WSIs) and demonstrate improved performance in terms of
computational efficiency, image quality and consistency across tiles over
current methods.",0
CRB Analysis for Mixed-ADC Based DOA Estimation,2403.09301v1,http://arxiv.org/abs/2403.09301v1,2024-03-14 11:49:14+00:00,"We consider a mixed analog-to-digital converter (ADC) based architecture
consisting of high-precision and one-bit ADCs with the antenna-varying
threshold for direction of arrival (DOA) estimation using a uniform linear
array (ULA), which utilizes fixed but different thresholds for one-bit ADCs
across different receive antennas. The Cram{\'e}r-Rao bound (CRB) with the
antenna-varying threshold is obtained. Then based on the lower bound of the
CRB, we derive the asymptotic CRB of the DOA, which depends on the placement of
mixed-ADC. Our analysis shows that distributing high-precision ADCs evenly
around the two edges of the ULA yields improved performance. This result can be
extended to a more general case where the ULA is equipped with two types of
ADCs with different quantization precisions. To efficiently obtain the maximum
likelihood DOA estimates, we propose a two-step algorithm. Firstly, we
formulate the model as a sparse signal representation problem, and modify the
sparse learning via iterative minimization (SLIM) approach to the mixed-ADC
based DOA estimation. In the second step, we use the relaxation-based approach
to cyclically refine the estimates of SLIM, further enhancing the DOA
estimation performance. Numerical examples are presented to demonstrate the
validity of the CRB analysis and the effectiveness of our methods.",0
Recursive Causal Discovery,2403.09300v1,http://arxiv.org/abs/2403.09300v1,2024-03-14 11:46:25+00:00,"Causal discovery, i.e., learning the causal graph from data, is often the
first step toward the identification and estimation of causal effects, a key
requirement in numerous scientific domains. Causal discovery is hampered by two
main challenges: limited data results in errors in statistical testing and the
computational complexity of the learning task is daunting. This paper builds
upon and extends four of our prior publications (Mokhtarian et al., 2021;
Akbari et al., 2021; Mokhtarian et al., 2022, 2023a). These works introduced
the concept of removable variables, which are the only variables that can be
removed recursively for the purpose of causal discovery. Presence and
identification of removable variables allow recursive approaches for causal
discovery, a promising solution that helps to address the aforementioned
challenges by reducing the problem size successively. This reduction not only
minimizes conditioning sets in each conditional independence (CI) test, leading
to fewer errors but also significantly decreases the number of required CI
tests. The worst-case performances of these methods nearly match the lower
bound. In this paper, we present a unified framework for the proposed
algorithms, refined with additional details and enhancements for a coherent
presentation. A comprehensive literature review is also included, comparing the
computational complexity of our methods with existing approaches, showcasing
their state-of-the-art efficiency. Another contribution of this paper is the
release of RCD, a Python package that efficiently implements these algorithms.
This package is designed for practitioners and researchers interested in
applying these methods in practical scenarios. The package is available at
github.com/ban-epfl/rcd, with comprehensive documentation provided at
rcdpackage.com.",0
More than words: Advancements and challenges in speech recognition for singing,2403.09298v1,http://arxiv.org/abs/2403.09298v1,2024-03-14 11:37:02+00:00,"This paper addresses the challenges and advancements in speech recognition
for singing, a domain distinctly different from standard speech recognition.
Singing encompasses unique challenges, including extensive pitch variations,
diverse vocal styles, and background music interference. We explore key areas
such as phoneme recognition, language identification in songs, keyword
spotting, and full lyrics transcription. I will describe some of my own
experiences when performing research on these tasks just as they were starting
to gain traction, but will also show how recent developments in deep learning
and large-scale datasets have propelled progress in this field. My goal is to
illuminate the complexities of applying speech recognition to singing, evaluate
current capabilities, and outline future research directions.",0
Select and Distill: Selective Dual-Teacher Knowledge Transfer for Continual Learning on Vision-Language Models,2403.09296v1,http://arxiv.org/abs/2403.09296v1,2024-03-14 11:36:36+00:00,"Large-scale vision-language models (VLMs) have shown a strong zero-shot
generalization capability on unseen-domain data. However, when adapting
pre-trained VLMs to a sequence of downstream tasks, they are prone to
forgetting previously learned knowledge and degrade their zero-shot
classification capability. To tackle this problem, we propose a unique
Selective Dual-Teacher Knowledge Transfer framework that leverages the most
recent fine-tuned and the original pre-trained VLMs as dual teachers to
preserve the previously learned knowledge and zero-shot capabilities,
respectively. With only access to an unlabeled reference dataset, our proposed
framework performs a selective knowledge distillation mechanism by measuring
the feature discrepancy from the dual teacher VLMs. Consequently, our selective
dual-teacher knowledge distillation would mitigate catastrophic forgetting of
previously learned knowledge while preserving the zero-shot capabilities from
pre-trained VLMs. Through extensive experiments on benchmark datasets, we show
that our proposed framework is favorable against state-of-the-art continual
learning approaches for preventing catastrophic forgetting and zero-shot
degradation.",0
Anatomical Structure-Guided Medical Vision-Language Pre-training,2403.09294v1,http://arxiv.org/abs/2403.09294v1,2024-03-14 11:29:47+00:00,"Learning medical visual representations through vision-language pre-training
has reached remarkable progress. Despite the promising performance, it still
faces challenges, i.e., local alignment lacks interpretability and clinical
relevance, and the insufficient internal and external representation learning
of image-report pairs. To address these issues, we propose an Anatomical
Structure-Guided (ASG) framework. Specifically, we parse raw reports into
triplets <anatomical region, finding, existence>, and fully utilize each
element as supervision to enhance representation learning. For anatomical
region, we design an automatic anatomical region-sentence alignment paradigm in
collaboration with radiologists, considering them as the minimum semantic units
to explore fine-grained local alignment. For finding and existence, we regard
them as image tags, applying an image-tag recognition decoder to associate
image features with their respective tags within each sample and constructing
soft labels for contrastive learning to improve the semantic association of
different image-report pairs. We evaluate the proposed ASG framework on two
downstream tasks, including five public benchmarks. Experimental results
demonstrate that our method outperforms the state-of-the-art methods.",0
SELECTOR: Heterogeneous graph network with convolutional masked autoencoder for multimodal robust prediction of cancer survival,2403.09290v1,http://arxiv.org/abs/2403.09290v1,2024-03-14 11:23:39+00:00,"Accurately predicting the survival rate of cancer patients is crucial for
aiding clinicians in planning appropriate treatment, reducing cancer-related
medical expenses, and significantly enhancing patients' quality of life.
Multimodal prediction of cancer patient survival offers a more comprehensive
and precise approach. However, existing methods still grapple with challenges
related to missing multimodal data and information interaction within
modalities. This paper introduces SELECTOR, a heterogeneous graph-aware network
based on convolutional mask encoders for robust multimodal prediction of cancer
patient survival. SELECTOR comprises feature edge reconstruction, convolutional
mask encoder, feature cross-fusion, and multimodal survival prediction modules.
Initially, we construct a multimodal heterogeneous graph and employ the
meta-path method for feature edge reconstruction, ensuring comprehensive
incorporation of feature information from graph edges and effective embedding
of nodes. To mitigate the impact of missing features within the modality on
prediction accuracy, we devised a convolutional masked autoencoder (CMAE) to
process the heterogeneous graph post-feature reconstruction. Subsequently, the
feature cross-fusion module facilitates communication between modalities,
ensuring that output features encompass all features of the modality and
relevant information from other modalities. Extensive experiments and analysis
on six cancer datasets from TCGA demonstrate that our method significantly
outperforms state-of-the-art methods in both modality-missing and
intra-modality information-confirmed cases. Our codes are made available at
https://github.com/panliangrui/Selector.",0
DA-PFL: Dynamic Affinity Aggregation for Personalized Federated Learning,2403.09284v1,http://arxiv.org/abs/2403.09284v1,2024-03-14 11:12:10+00:00,"Personalized federated learning becomes a hot research topic that can learn a
personalized learning model for each client. Existing personalized federated
learning models prefer to aggregate similar clients with similar data
distribution to improve the performance of learning models. However,
similaritybased personalized federated learning methods may exacerbate the
class imbalanced problem. In this paper, we propose a novel Dynamic
Affinity-based Personalized Federated Learning model (DA-PFL) to alleviate the
class imbalanced problem during federated learning. Specifically, we build an
affinity metric from a complementary perspective to guide which clients should
be aggregated. Then we design a dynamic aggregation strategy to dynamically
aggregate clients based on the affinity metric in each round to reduce the
class imbalanced risk. Extensive experiments show that the proposed DA-PFL
model can significantly improve the accuracy of each client in three real-world
datasets with state-of-the-art comparison methods.",0
CLIP-EBC: CLIP Can Count Accurately through Enhanced Blockwise Classification,2403.09281v1,http://arxiv.org/abs/2403.09281v1,2024-03-14 11:08:33+00:00,"The CLIP (Contrastive Language-Image Pretraining) model has exhibited
outstanding performance in recognition problems, such as zero-shot image
classification and object detection. However, its ability to count remains
understudied due to the inherent challenges of transforming counting--a
regression task--into a recognition task. In this paper, we investigate CLIP's
potential in counting, focusing specifically on estimating crowd sizes.
Existing classification-based crowd-counting methods have encountered issues,
including inappropriate discretization strategies, which impede the application
of CLIP and result in suboptimal performance. To address these challenges, we
propose the Enhanced Blockwise Classification (EBC) framework. In contrast to
previous methods, EBC relies on integer-valued bins that facilitate the
learning of robust decision boundaries. Within our model-agnostic EBC
framework, we introduce CLIP-EBC, the first fully CLIP-based crowd-counting
model capable of generating density maps. Comprehensive evaluations across
diverse crowd-counting datasets demonstrate the state-of-the-art performance of
our methods. Particularly, EBC can improve existing models by up to 76.9%.
Moreover, our CLIP-EBC model surpasses current crowd-counting methods,
achieving mean absolute errors of 55.0 and 6.3 on ShanghaiTech part A and part
B datasets, respectively. The code will be made publicly available.",0
Identifying Galaxy Cluster Mergers with Deep Neural Networks using Idealized Compton-y and X-ray maps,2403.09273v1,http://arxiv.org/abs/2403.09273v1,2024-03-14 10:50:24+00:00,"We present a novel approach to identify galaxy clusters that are undergoing a
merger using a deep learning approach. This paper uses massive galaxy clusters
spanning $0 \leq z \leq 2$ from \textsc{The Three Hundred} project, a suite of
hydrodynamic re-simulations of 324 large galaxy clusters. Mock, idealised
Compton-{\it y} and X-ray maps were constructed for the sample, capturing them
out to a radius of $2R_{200}$. The idealised nature of these maps mean they do
not consider observational effects such as foreground or background
astrophysical objects, any spatial resolution limits or restriction on X-ray
energy bands. Half of the maps belong to a merging population as defined by a
mass increase $\Delta${\it M/M} $\geq$ 0.75, and the other half serve as a
control, relaxed population. We employ a convolutional neural network
architecture and train the model to classify clusters into one of the groups. A
best-performing model was able to correctly distinguish between the two
populations with a balanced accuracy (BA) and recall of 0.77, ROC-AUC of 0.85,
PR-AUC of 0.55 and $F_{1}$ score of 0.53. Using a multichannel model relative
to a single channel model, we obtain a 3\% improvement in BA score, and a 6\%
improvement in $F_{1}$ score. We use a saliency interpretation approach to
discern the regions most important to each classification decision. By
analysing radially binned saliency values we find a preference to utilise
regions out to larger distances for mergers with respect to non-mergers,
greater than $\sim1.2 R_{200}$ and $\sim0.7 R_{200}$ for SZ and X-ray
respectively.",0
A Deep Reinforcement Learning Approach for Autonomous Reconfigurable Intelligent Surfaces,2403.09270v1,http://arxiv.org/abs/2403.09270v1,2024-03-14 10:47:01+00:00,"A reconfigurable intelligent surface (RIS) is a prospective wireless
technology that enhances wireless channel quality. An RIS is often equipped
with passive array of elements and provides cost and power-efficient solutions
for coverage extension of wireless communication systems. Without any radio
frequency (RF) chains or computing resources, however, the RIS requires control
information to be sent to it from an external unit, e.g., a base station (BS).
The control information can be delivered by wired or wireless channels, and the
BS must be aware of the RIS and the RIS-related channel conditions in order to
effectively configure its behavior. Recent works have introduced hybrid RIS
structures possessing a few active elements that can sense and digitally
process received data. Here, we propose the operation of an entirely autonomous
RIS that operates without a control link between the RIS and BS. Using a few
sensing elements, the autonomous RIS employs a deep Q network (DQN) based on
reinforcement learning in order to enhance the sum rate of the network. Our
results illustrate the potential of deploying autonomous RISs in wireless
networks with essentially no network overhead.",0
Deep Limit Order Book Forecasting,2403.09267v1,http://arxiv.org/abs/2403.09267v1,2024-03-14 10:44:10+00:00,"We exploit cutting-edge deep learning methodologies to explore the
predictability of high-frequency Limit Order Book mid-price changes for a
heterogeneous set of stocks traded on the NASDAQ exchange. In so doing, we
release `LOBFrame', an open-source code base, to efficiently process
large-scale Limit Order Book data and quantitatively assess state-of-the-art
deep learning models' forecasting capabilities. Our results are twofold. We
demonstrate that the stocks' microstructural characteristics influence the
efficacy of deep learning methods and that their high forecasting power does
not necessarily correspond to actionable trading signals. We argue that
traditional machine learning metrics fail to adequately assess the quality of
forecasts in the Limit Order Book context. As an alternative, we propose an
innovative operational framework that assesses predictions' practicality by
focusing on the probability of accurately forecasting complete transactions.
This work offers academics and practitioners an avenue to make informed and
robust decisions on the application of deep learning techniques, their scope
and limitations, effectively exploiting emergent statistical properties of the
Limit Order Book.",0
To Label or Not to Label: Hybrid Active Learning for Neural Machine Translation,2403.09259v1,http://arxiv.org/abs/2403.09259v1,2024-03-14 10:33:28+00:00,"Active learning (AL) techniques reduce labeling costs for training neural
machine translation (NMT) models by selecting smaller representative subsets
from unlabeled data for annotation. Diversity sampling techniques select
heterogeneous instances, while uncertainty sampling methods select instances
with the highest model uncertainty. Both approaches have limitations -
diversity methods may extract varied but trivial examples, while uncertainty
sampling can yield repetitive, uninformative instances. To bridge this gap, we
propose HUDS, a hybrid AL strategy for domain adaptation in NMT that combines
uncertainty and diversity for sentence selection. HUDS computes uncertainty
scores for unlabeled sentences and subsequently stratifies them. It then
clusters sentence embeddings within each stratum using k-MEANS and computes
diversity scores by distance to the centroid. A weighted hybrid score that
combines uncertainty and diversity is then used to select the top instances for
annotation in each AL iteration. Experiments on multi-domain German-English
datasets demonstrate the better performance of HUDS over other strong AL
baselines. We analyze the sentence selection with HUDS and show that it
prioritizes diverse instances having high model uncertainty for annotation in
early AL iterations.",0
WSI-SAM: Multi-resolution Segment Anything Model (SAM) for histopathology whole-slide images,2403.09257v1,http://arxiv.org/abs/2403.09257v1,2024-03-14 10:30:43+00:00,"The Segment Anything Model (SAM) marks a significant advancement in
segmentation models, offering powerful zero-shot capabilities and dynamic
prompting. However, existing medical SAMs are not suitable for the multi-scale
nature of whole-slide images (WSIs), restricting their effectiveness. To
resolve this drawback, we present WSI-SAM, enhancing SAM with precise object
segmentation capabilities for histopathology images using multi-resolution
patches, while preserving its original prompt-driven design, efficiency, and
zero-shot adaptability. To fully exploit pretrained knowledge while minimizing
training overhead, we keep SAM frozen, only introducing minimal additional
parameters and computation. In particular, we introduce High-Resolution (HR)
token, Low-Resolution (LR) token and dual mask decoder. This decoder integrates
the original SAM mask decoder with a lightweight fusion module that integrates
features at multiple scales. Instead of predicting a mask independently, we
integrate HR and LR token at intermediate layer to jointly learn features of
the same object across multiple resolutions. Experiments show that our WSI-SAM
outperforms state-of-the-art SAM and its variants. In particular, our model
outperforms SAM by 4.1 and 2.5 percent points on a ductal carcinoma in situ
(DCIS) segmentation tasks and breast cancer metastasis segmentation task
(CAMELYON16 dataset). The code will be available at
https://github.com/HongLiuuuuu/WSI-SAM.",0
A Modified da Vinci Surgical Instrument for OCE based Elasticity Estimation with Deep Learning,2403.09256v1,http://arxiv.org/abs/2403.09256v1,2024-03-14 10:27:38+00:00,"Robot-assisted surgery has advantages compared to conventional laparoscopic
procedures, e.g., precise movement of the surgical instruments, improved
dexterity, and high-resolution visualization of the surgical field. However,
mechanical tissue properties may provide additional information, e.g., on the
location of lesions or vessels. While elastographic imaging has been proposed,
it is not readily available as an online modality during robot-assisted
surgery. We propose modifying a da~Vinci surgical instrument to realize optical
coherence elastography (OCE) for quantitative elasticity estimation. The
modified da~Vinci instrument is equipped with piezoelectric elements for shear
wave excitation and we employ fast optical coherence tomography (OCT) imaging
to track propagating wave fields, which are directly related to biomechanical
tissue properties. All high-voltage components are mounted at the proximal end
outside the patient. We demonstrate that external excitation at the instrument
shaft can effectively stimulate shear waves, even when considering damping.
Comparing conventional and deep learning-based signal processing, resulting in
mean absolute errors of 19.27 kPa and 6.29 kPa, respectively. These results
illustrate that precise quantitative elasticity estimates can be obtained. We
also demonstrate quantitative elasticity estimation on ex-vivo tissue samples
of heart, liver and stomach, and show that the measurements can be used to
distinguish soft and stiff tissue types.",0
Leveraging Constraint Programming in a Deep Learning Approach for Dynamically Solving the Flexible Job-Shop Scheduling Problem,2403.09249v1,http://arxiv.org/abs/2403.09249v1,2024-03-14 10:16:57+00:00,"Recent advancements in the flexible job-shop scheduling problem (FJSSP) are
primarily based on deep reinforcement learning (DRL) due to its ability to
generate high-quality, real-time solutions. However, DRL approaches often fail
to fully harness the strengths of existing techniques such as exact methods or
constraint programming (CP), which can excel at finding optimal or near-optimal
solutions for smaller instances. This paper aims to integrate CP within a deep
learning (DL) based methodology, leveraging the benefits of both. In this
paper, we introduce a method that involves training a DL model using optimal
solutions generated by CP, ensuring the model learns from high-quality data,
thereby eliminating the need for the extensive exploration typical in DRL and
enhancing overall performance. Further, we integrate CP into our DL framework
to jointly construct solutions, utilizing DL for the initial complex stages and
transitioning to CP for optimal resolution as the problem is simplified. Our
hybrid approach has been extensively tested on three public FJSSP benchmarks,
demonstrating superior performance over five state-of-the-art DRL approaches
and a widely-used CP solver. Additionally, with the objective of exploring the
application to other combinatorial optimization problems, promising preliminary
results are presented on applying our hybrid approach to the traveling salesman
problem, combining an exact method with a well-known DRL method.",0
Hyper-3DG: Text-to-3D Gaussian Generation via Hypergraph,2403.09236v1,http://arxiv.org/abs/2403.09236v1,2024-03-14 09:59:55+00:00,"Text-to-3D generation represents an exciting field that has seen rapid
advancements, facilitating the transformation of textual descriptions into
detailed 3D models. However, current progress often neglects the intricate
high-order correlation of geometry and texture within 3D objects, leading to
challenges such as over-smoothness, over-saturation and the Janus problem. In
this work, we propose a method named ``3D Gaussian Generation via Hypergraph
(Hyper-3DG)'', designed to capture the sophisticated high-order correlations
present within 3D objects. Our framework is anchored by a well-established
mainflow and an essential module, named ``Geometry and Texture Hypergraph
Refiner (HGRefiner)''. This module not only refines the representation of 3D
Gaussians but also accelerates the update process of these 3D Gaussians by
conducting the Patch-3DGS Hypergraph Learning on both explicit attributes and
latent visual features. Our framework allows for the production of finely
generated 3D objects within a cohesive optimization, effectively circumventing
degradation. Extensive experimentation has shown that our proposed method
significantly enhances the quality of 3D generation while incurring no
additional computational overhead for the underlying framework. (Project code:
https://github.com/yjhboy/Hyper3DG)",0
D-YOLO a robust framework for object detection in adverse weather conditions,2403.09233v1,http://arxiv.org/abs/2403.09233v1,2024-03-14 09:57:15+00:00,"Adverse weather conditions including haze, snow and rain lead to decline in
image qualities, which often causes a decline in performance for deep-learning
based detection networks. Most existing approaches attempts to rectify hazy
images before performing object detection, which increases the complexity of
the network and may result in the loss in latent information. To better
integrate image restoration and object detection tasks, we designed a
double-route network with an attention feature fusion module, taking both hazy
and dehazed features into consideration. We also proposed a subnetwork to
provide haze-free features to the detection network. Specifically, our D-YOLO
improves the performance of the detection network by minimizing the distance
between the clear feature extraction subnetwork and detection network.
Experiments on RTTS and FoggyCityscapes datasets show that D-YOLO demonstrates
better performance compared to the state-of-the-art methods. It is a robust
detection framework for bridging the gap between low-level dehazing and
high-level detection.",0
Generating Feasible and Plausible Counterfactual Explanations for Outcome Prediction of Business Processes,2403.09232v1,http://arxiv.org/abs/2403.09232v1,2024-03-14 09:56:35+00:00,"In recent years, various machine and deep learning architectures have been
successfully introduced to the field of predictive process analytics.
Nevertheless, the inherent opacity of these algorithms poses a significant
challenge for human decision-makers, hindering their ability to understand the
reasoning behind the predictions. This growing concern has sparked the
introduction of counterfactual explanations, designed as human-understandable
what if scenarios, to provide clearer insights into the decision-making process
behind undesirable predictions. The generation of counterfactual explanations,
however, encounters specific challenges when dealing with the sequential nature
of the (business) process cases typically used in predictive process analytics.
Our paper tackles this challenge by introducing a data-driven approach,
REVISEDplus, to generate more feasible and plausible counterfactual
explanations. First, we restrict the counterfactual algorithm to generate
counterfactuals that lie within a high-density region of the process data,
ensuring that the proposed counterfactuals are realistic and feasible within
the observed process data distribution. Additionally, we ensure plausibility by
learning sequential patterns between the activities in the process cases,
utilising Declare language templates. Finally, we evaluate the properties that
define the validity of counterfactuals.",0
A secular solar system resonance that disrupts the dominant cycle in Earth's orbital eccentricity (g2-g5): Implications for astrochronology,2403.09332v1,http://arxiv.org/abs/2403.09332v1,2024-03-14 12:20:59+00:00,"The planets' gravitational interaction causes rhythmic changes in Earth's
orbital parameters (also called Milankovi\'c cycles), which have powerful
applications in geology and astrochronology. For instance, the primary
astronomical eccentricity cycle due to the secular frequency term (g2-g5) (~405
kyr in the recent past) utilized in deep-time analyses is dominated by Venus'
and Jupiter's orbits, aka long eccentricity cycle. The widely accepted and
long-held view is that (g2-g5) was practically stable in the past and may hence
be used as a ""metronome"" to reconstruct accurate ages and chronologies.
However, using state-of-the-art integrations of the solar system, we show here
that (g2-g5) can become unstable over long time scales, without major changes
in, or destabilization of, planetary orbits. The (g2-g5) disruption is due to
the secular resonance $\sigma_{12}$ = (g1 - g2) + (s1 - s2), a major
contributor to solar system chaos. We demonstrate that entering/exiting the
$\sigma_{12}$ resonance is a common phenomenon on long time scales, occurring
in ~40% of our solutions. During $\sigma_{12}$-resonance episodes, (g2-g5) is
very weak or absent and Earth's orbital eccentricity and climate-forcing
spectrum are unrecognizable compared to the recent past. Our results have
fundamental implications for geology and astrochronology, as well as climate
forcing because the paradigm that the longest Milankovi\'c cycle dominates
Earth's astronomical forcing, is stable, and has a period of ~405 kyr requires
revision.",0
BurstAttention: An Efficient Distributed Attention Framework for Extremely Long Sequences,2403.09347v1,http://arxiv.org/abs/2403.09347v1,2024-03-14 12:51:58+00:00,"Effective attention modules have played a crucial role in the success of
Transformer-based large language models (LLMs), but the quadratic time and
memory complexities of these attention modules also pose a challenge when
processing long sequences. One potential solution for the long sequence problem
is to utilize distributed clusters to parallelize the computation of attention
modules across multiple devices (e.g., GPUs). However, adopting a distributed
approach inevitably introduces extra memory overheads to store local attention
results and incurs additional communication costs to aggregate local results
into global ones. In this paper, we propose a distributed attention framework
named ``BurstAttention'' to optimize memory access and communication operations
at both the global cluster and local device levels. In our experiments, we
compare BurstAttention with other competitive distributed attention solutions
for long sequence processing. The experimental results under different length
settings demonstrate that BurstAttention offers significant advantages for
processing long sequences compared with these competitive baselines, reducing
40% communication overheads and achieving 2 X speedup during training 32K
sequence length on 8 X A100.",0
Shake to Leak: Fine-tuning Diffusion Models Can Amplify the Generative Privacy Risk,2403.09450v1,http://arxiv.org/abs/2403.09450v1,2024-03-14 14:48:37+00:00,"While diffusion models have recently demonstrated remarkable progress in
generating realistic images, privacy risks also arise: published models or APIs
could generate training images and thus leak privacy-sensitive training
information. In this paper, we reveal a new risk, Shake-to-Leak (S2L), that
fine-tuning the pre-trained models with manipulated data can amplify the
existing privacy risks. We demonstrate that S2L could occur in various standard
fine-tuning strategies for diffusion models, including concept-injection
methods (DreamBooth and Textual Inversion) and parameter-efficient methods
(LoRA and Hypernetwork), as well as their combinations. In the worst case, S2L
can amplify the state-of-the-art membership inference attack (MIA) on diffusion
models by $5.4\%$ (absolute difference) AUC and can increase extracted private
samples from almost $0$ samples to $16.3$ samples on average per target domain.
This discovery underscores that the privacy risk with diffusion models is even
more severe than previously recognized. Codes are available at
https://github.com/VITA-Group/Shake-to-Leak.",0
LDPRecover: Recovering Frequencies from Poisoning Attacks against Local Differential Privacy,2403.09351v1,http://arxiv.org/abs/2403.09351v1,2024-03-14 12:57:20+00:00,"Local differential privacy (LDP), which enables an untrusted server to
collect aggregated statistics from distributed users while protecting the
privacy of those users, has been widely deployed in practice. However, LDP
protocols for frequency estimation are vulnerable to poisoning attacks, in
which an attacker can poison the aggregated frequencies by manipulating the
data sent from malicious users. Therefore, it is an open challenge to recover
the accurate aggregated frequencies from poisoned ones.
  In this work, we propose LDPRecover, a method that can recover accurate
aggregated frequencies from poisoning attacks, even if the server does not
learn the details of the attacks. In LDPRecover, we establish a genuine
frequency estimator that theoretically guides the server to recover the
frequencies aggregated from genuine users' data by eliminating the impact of
malicious users' data in poisoned frequencies. Since the server has no idea of
the attacks, we propose an adaptive attack to unify existing attacks and learn
the statistics of the malicious data within this adaptive attack by exploiting
the properties of LDP protocols. By taking the estimator and the learning
statistics as constraints, we formulate the problem of recovering aggregated
frequencies to approach the genuine ones as a constraint inference (CI)
problem. Consequently, the server can obtain accurate aggregated frequencies by
solving this problem optimally. Moreover, LDPRecover can serve as a frequency
recovery paradigm that recovers more accurate aggregated frequencies by
integrating attack details as new constraints in the CI problem. Our evaluation
on two real-world datasets, three LDP protocols, and untargeted and targeted
poisoning attacks shows that LDPRecover is both accurate and widely applicable
against various poisoning attacks.",0
Adversarial Fine-tuning of Compressed Neural Networks for Joint Improvement of Robustness and Efficiency,2403.09441v1,http://arxiv.org/abs/2403.09441v1,2024-03-14 14:34:25+00:00,"As deep learning (DL) models are increasingly being integrated into our
everyday lives, ensuring their safety by making them robust against adversarial
attacks has become increasingly critical. DL models have been found to be
susceptible to adversarial attacks which can be achieved by introducing small,
targeted perturbations to disrupt the input data. Adversarial training has been
presented as a mitigation strategy which can result in more robust models. This
adversarial robustness comes with additional computational costs required to
design adversarial attacks during training. The two objectives -- adversarial
robustness and computational efficiency -- then appear to be in conflict of
each other. In this work, we explore the effects of two different model
compression methods -- structured weight pruning and quantization -- on
adversarial robustness. We specifically explore the effects of fine-tuning on
compressed models, and present the trade-off between standard fine-tuning and
adversarial fine-tuning. Our results show that compression does not inherently
lead to loss in model robustness and adversarial fine-tuning of a compressed
model can yield large improvement to the robustness performance of models. We
present experiments on two benchmark datasets showing that adversarial
fine-tuning of compressed models can achieve robustness performance comparable
to adversarially trained models, while also improving computational efficiency.",0
Emergent time scales of epistasis in protein evolution,2403.09436v1,http://arxiv.org/abs/2403.09436v1,2024-03-14 14:30:12+00:00,"We introduce a data-driven epistatic model of protein evolution, capable of
generating evolutionary trajectories spanning very different time scales
reaching from individual mutations to diverged homologs. Our in silico
evolution encompasses random nucleotide mutations, insertions and deletions,
and models selection using a fitness landscape, which is inferred via a
generative probabilistic model for protein families. We show that the proposed
framework accurately reproduces the sequence statistics of both short-time
(experimental) and long-time (natural) protein evolution, suggesting
applicability also to relatively data-poor intermediate evolutionary time
scales, which are currently inaccessible to evolution experiments. Our model
uncovers a highly collective nature of epistasis, gradually changing the
fitness effect of mutations in a diverging sequence context, rather than acting
via strong interactions between individual mutations. This collective nature
triggers the emergence of a long evolutionary time scale, separating fast
mutational processes inside a given sequence context, from the slow evolution
of the context itself. The model quantitatively reproduces the extent of
contingency and entrenchment, as well as the loss of predictability in protein
evolution observed in deep mutational scanning experiments of distant homologs.
It thereby deepens our understanding of the interplay between mutation and
selection in shaping protein diversity and novel functions, allows to
statistically forecast evolution, and challenges the prevailing
independent-site models of protein evolution, which are unable to capture the
fundamental importance of epistasis.",0
Open-Vocabulary Object Detection with Meta Prompt Representation and Instance Contrastive Optimization,2403.09433v1,http://arxiv.org/abs/2403.09433v1,2024-03-14 14:25:10+00:00,"Classical object detectors are incapable of detecting novel class objects
that are not encountered before. Regarding this issue, Open-Vocabulary Object
Detection (OVOD) is proposed, which aims to detect the objects in the candidate
class list. However, current OVOD models are suffering from overfitting on the
base classes, heavily relying on the large-scale extra data, and complex
training process. To overcome these issues, we propose a novel framework with
Meta prompt and Instance Contrastive learning (MIC) schemes. Firstly, we
simulate a novel-class-emerging scenario to help the prompt learner that learns
class and background prompts generalize to novel classes. Secondly, we design
an instance-level contrastive strategy to promote intra-class compactness and
inter-class separation, which benefits generalization of the detector to novel
class objects. Without using knowledge distillation, ensemble model or extra
training data during detector training, our proposed MIC outperforms previous
SOTA methods trained with these complex techniques on LVIS. Most importantly,
MIC shows great generalization ability on novel classes, e.g., with $+4.3\%$
and $+1.9\% \ \mathrm{AP}$ improvement compared with previous SOTA on COCO and
Objects365, respectively.",0
Reconstruction and Simulation of Elastic Objects with Spring-Mass 3D Gaussians,2403.09434v1,http://arxiv.org/abs/2403.09434v1,2024-03-14 14:25:10+00:00,"Reconstructing and simulating elastic objects from visual observations is
crucial for applications in computer vision and robotics. Existing methods,
such as 3D Gaussians, provide modeling for 3D appearance and geometry but lack
the ability to simulate physical properties or optimize parameters for
heterogeneous objects. We propose Spring-Gaus, a novel framework that
integrates 3D Gaussians with physics-based simulation for reconstructing and
simulating elastic objects from multi-view videos. Our method utilizes a 3D
Spring-Mass model, enabling the optimization of physical parameters at the
individual point level while decoupling the learning of physics and appearance.
This approach achieves great sample efficiency, enhances generalization, and
reduces sensitivity to the distribution of simulation particles. We evaluate
Spring-Gaus on both synthetic and real-world datasets, demonstrating accurate
reconstruction and simulation of elastic objects. This includes future
prediction and simulation under varying initial states and environmental
parameters. Project page: https://zlicheng.com/spring_gaus.",0
Efficient Transferability Assessment for Selection of Pre-trained Detectors,2403.09432v1,http://arxiv.org/abs/2403.09432v1,2024-03-14 14:23:23+00:00,"Large-scale pre-training followed by downstream fine-tuning is an effective
solution for transferring deep-learning-based models. Since finetuning all
possible pre-trained models is computational costly, we aim to predict the
transferability performance of these pre-trained models in a computational
efficient manner. Different from previous work that seek out suitable models
for downstream classification and segmentation tasks, this paper studies the
efficient transferability assessment of pre-trained object detectors. To this
end, we build up a detector transferability benchmark which contains a large
and diverse zoo of pre-trained detectors with various architectures, source
datasets and training schemes. Given this zoo, we adopt 7 target datasets from
5 diverse domains as the downstream target tasks for evaluation. Further, we
propose to assess classification and regression sub-tasks simultaneously in a
unified framework. Additionally, we design a complementary metric for
evaluating tasks with varying objects. Experimental results demonstrate that
our method outperforms other state-of-the-art approaches in assessing
transferability under different target domains while efficiently reducing
wall-clock time 32$\times$ and requires a mere 5.2\% memory footprint compared
to brute-force fine-tuning of all pre-trained detectors.",0
Variational Inference with Sequential Sample-Average Approximations,2403.09429v1,http://arxiv.org/abs/2403.09429v1,2024-03-14 14:20:22+00:00,"We present variational inference with sequential sample-average approximation
(VISA), a method for approximate inference in computationally intensive models,
such as those based on numerical simulations. VISA extends importance-weighted
forward-KL variational inference by employing a sequence of sample-average
approximations, which are considered valid inside a trust region. This makes it
possible to reuse model evaluations across multiple gradient steps, thereby
reducing computational cost. We perform experiments on high-dimensional
Gaussians, Lotka-Volterra dynamics, and a Pickover attractor, which demonstrate
that VISA can achieve comparable approximation accuracy to standard
importance-weighted forward-KL variational inference with computational savings
of a factor two or more for conservatively chosen learning rates.",0
Borrowing Treasures from Neighbors: In-Context Learning for Multimodal Learning with Missing Modalities and Data Scarcity,2403.09428v1,http://arxiv.org/abs/2403.09428v1,2024-03-14 14:19:48+00:00,"Multimodal machine learning with missing modalities is an increasingly
relevant challenge arising in various applications such as healthcare. This
paper extends the current research into missing modalities to the low-data
regime, i.e., a downstream task has both missing modalities and limited sample
size issues. This problem setting is particularly challenging and also
practical as it is often expensive to get full-modality data and sufficient
annotated training samples. We propose to use retrieval-augmented in-context
learning to address these two crucial issues by unleashing the potential of a
transformer's in-context learning ability. Diverging from existing methods,
which primarily belong to the parametric paradigm and often require sufficient
training samples, our work exploits the value of the available full-modality
data, offering a novel perspective on resolving the challenge. The proposed
data-dependent framework exhibits a higher degree of sample efficiency and is
empirically demonstrated to enhance the classification model's performance on
both full- and missing-modality data in the low-data regime across various
multimodal learning tasks. When only 1% of the training data are available, our
proposed method demonstrates an average improvement of 6.1% over a recent
strong baseline across various datasets and missing states. Notably, our method
also reduces the performance gap between full-modality and missing-modality
data compared with the baseline.",0
Low-coercive-field ferroelectric hafnia with mobile domain walls,2403.09426v1,http://arxiv.org/abs/2403.09426v1,2024-03-14 14:17:01+00:00,"The high coercive field ($\mathcal{E}_c$) of hafnia-based ferroelectrics
presents a major obstacle to their applications. The ferroelectric switching
mechanisms in hafnia that dictate $\mathcal{E}_c$, especially those related to
nucleation-and-growth at the domain wall (DW), have remained elusive. Through
deep-learning-assisted multiscale simulations, we determine the
finite-temperature thermodynamics and switching mechanisms for diverse types of
180$^\circ$ DWs, revealing a complex, stress-sensitive mobility landscape. The
propagation velocities for mobile DW types under various thermal conditions can
be characterized with a single creep equation, featuring a creep exponent of 2.
This unconventional critical exponent results from the nucleation of a
half-unit-cell-thin, elliptically-shaped critical nucleus. Our multiscale
approach not only reproduces the experimental thickness ($d$) scaling,
$\mathcal{E}_c\propto d^{-\frac{2}{3}}$, but also predicts that $\mathcal{E}_c$
of HfO$_2$ can be engineered to $\approx$0.1 MV/cm, even lower than perovskite
ferroelectrics. The theoretical lower bound of $\mathcal{E}_c$ afforded by
ferroelectric hafnia offers opportunities to realize power-efficient,
high-fidelity ferroelectric nanoelectronics.",0
RoDUS: Robust Decomposition of Static and Dynamic Elements in Urban Scenes,2403.09419v1,http://arxiv.org/abs/2403.09419v1,2024-03-14 14:08:59+00:00,"The task of separating dynamic objects from static environments using NeRFs
has been widely studied in recent years. However, capturing large-scale scenes
still poses a challenge due to their complex geometric structures and
unconstrained dynamics. Without the help of 3D motion cues, previous methods
often require simplified setups with slow camera motion and only a few/single
dynamic actors, leading to suboptimal solutions in most urban setups. To
overcome such limitations, we present RoDUS, a pipeline for decomposing static
and dynamic elements in urban scenes, with thoughtfully separated NeRF models
for moving and non-moving components. Our approach utilizes a robust
kernel-based initialization coupled with 4D semantic information to selectively
guide the learning process. This strategy enables accurate capturing of the
dynamics in the scene, resulting in reduced artifacts caused by NeRF on
background reconstruction, all by using self-supervision. Notably, experimental
evaluations on KITTI-360 and Pandaset datasets demonstrate the effectiveness of
our method in decomposing challenging urban scenes into precise static and
dynamic components.",0
GPT on a Quantum Computer,2403.09418v1,http://arxiv.org/abs/2403.09418v1,2024-03-14 14:07:31+00:00,"Large Language Models (LLMs) such as ChatGPT have transformed how we interact
with and understand the capabilities of Artificial Intelligence (AI). However,
the intersection of LLMs with the burgeoning field of Quantum Machine Learning
(QML) is only in its nascent stages. This paper presents an exploration of this
niche by detailing a comprehensive framework for implementing the foundational
Transformer architecture -- integral to ChatGPT -- within a quantum computing
paradigm. We meticulously design quantum circuits that implement adapted
versions of the transformer's core components and the generative pre-training
phase. By integrating quantum computing with LLMs, we aspire to open new
avenues for research in QML and contribute to the ongoing evolution of AI
technologies.",0
Constrained and Vanishing Expressivity of Quantum Fourier Models,2403.09417v1,http://arxiv.org/abs/2403.09417v1,2024-03-14 14:05:24+00:00,"In this work, we highlight an unforeseen behavior of the expressivity of
Parameterized Quantum Circuits (PQC) for machine learning. A large class of
these models, seen as Fourier Series which frequencies are derived from the
encoding gates, were thought to have their Fourier coefficients mostly
determined by the trainable gates. Here, we demonstrate a new correlation
between the Fourier coefficients of the quantum model and its encoding gates.
In addition, we display a phenomenon of vanishing expressivity in certain
settings, where some Fourier coefficients vanish exponentially when the number
of qubits grows. These two behaviors imply novel forms of constraints which
limit the expressivity of PQCs, and therefore imply a new inductive bias for
Quantum models. The key concept in this work is the notion of a frequency
redundancy in the Fourier series spectrum, which determines its importance.
Those theoretical behaviours are observed in numerical simulations.",0
Scalability of Metropolis-within-Gibbs schemes for high-dimensional Bayesian models,2403.09416v1,http://arxiv.org/abs/2403.09416v1,2024-03-14 14:04:44+00:00,"We study general coordinate-wise MCMC schemes (such as
Metropolis-within-Gibbs samplers), which are commonly used to fit Bayesian
non-conjugate hierarchical models. We relate their convergence properties to
the ones of the corresponding (potentially not implementable) Gibbs sampler
through the notion of conditional conductance. This allows us to study the
performances of popular Metropolis-within-Gibbs schemes for non-conjugate
hierarchical models, in high-dimensional regimes where both number of
datapoints and parameters increase. Given random data-generating assumptions,
we establish dimension-free convergence results, which are in close accordance
with numerical evidences. Applications to Bayesian models for binary regression
with unknown hyperparameters and discretely observed diffusions are also
discussed. Motivated by such statistical applications, auxiliary results of
independent interest on approximate conductances and perturbation of Markov
operators are provided.",0
User Identification via Free Roaming Eye Tracking Data,2403.09415v1,http://arxiv.org/abs/2403.09415v1,2024-03-14 14:04:37+00:00,"We present a new dataset of ""free roaming"" (FR) and ""targeted roaming"" (TR):
a pool of 41 participants is asked to walk around a university campus (FR) or
is asked to find a particular room within a library (TR). Eye movements are
recorded using a commodity wearable eye tracker (Pupil Labs Neon at 200Hz). On
this dataset we investigate the accuracy of user identification using a
previously known machine learning pipeline where a Radial Basis Function
Network (RBFN) is used as classifier. Our highest accuracies are 87.3% for FR
and 89.4% for TR. This should be compared to 95.3% which is the (corresponding)
highest accuracy we are aware of (achieved in a laboratory setting using the
""RAN"" stimulus of the BioEye 2015 competition dataset). To the best of our
knowledge, our results are the first that study user identification in a non
laboratory setting; such settings are often more feasible than laboratory
settings and may include further advantages. The minimum duration of each
recording is 263s for FR and 154s for TR. Our best accuracies are obtained when
restricting to 120s and 140s for FR and TR respectively, always cut from the
end of the trajectories (both for the training and testing sessions). If we cut
the same length from the beginning, then accuracies are 12.2% lower for FR and
around 6.4% lower for TR. On the full trajectories accuracies are lower by 5%
and 52% for FR and TR. We also investigate the impact of including higher order
velocity derivatives (such as acceleration, jerk, or jounce).",0
Region-based U-net for accelerated training and enhanced precision in deep brain segmentation,2403.09414v1,http://arxiv.org/abs/2403.09414v1,2024-03-14 14:04:29+00:00,"Segmentation of brain structures on MRI is the primary step for further
quantitative analysis of brain diseases. Manual segmentation is still
considered the gold standard in terms of accuracy; however, such data is
extremely time-consuming to generate. This paper presents a deep learning-based
segmentation approach for 12 deep-brain structures, utilizing multiple
region-based U-Nets. The brain is divided into three focal regions of interest
that encompass the brainstem, the ventricular system, and the striatum. Next,
three region-based U-nets are run in parallel to parcellate these larger
structures into their respective four substructures. This approach not only
greatly reduces the training and processing times but also significantly
enhances the segmentation accuracy, compared to segmenting the entire MRI image
at once. Our approach achieves remarkable accuracy with an average Dice
Similarity Coefficient (DSC) of 0.901 and 95% Hausdorff Distance (HD95) of
1.155 mm. The method was compared with state-of-the-art segmentation
approaches, demonstrating a high level of accuracy and robustness of the
proposed method.",0
XCoOp: Explainable Prompt Learning for Computer-Aided Diagnosis via Concept-guided Context Optimization,2403.09410v1,http://arxiv.org/abs/2403.09410v1,2024-03-14 14:02:01+00:00,"Utilizing potent representations of the large vision-language models (VLMs)
to accomplish various downstream tasks has attracted increasing attention.
Within this research field, soft prompt learning has become a representative
approach for efficiently adapting VLMs such as CLIP, to tasks like image
classification. However, most existing prompt learning methods learn text
tokens that are unexplainable, which cannot satisfy the stringent
interpretability requirements of Explainable Artificial Intelligence (XAI) in
high-stakes scenarios like healthcare. To address this issue, we propose a
novel explainable prompt learning framework that leverages medical knowledge by
aligning the semantics of images, learnable prompts, and clinical
concept-driven prompts at multiple granularities. Moreover, our framework
addresses the lack of valuable concept annotations by eliciting knowledge from
large language models and offers both visual and textual explanations for the
prompts. Extensive experiments and explainability analyses conducted on various
datasets, with and without concept labels, demonstrate that our method
simultaneously achieves superior diagnostic performance, flexibility, and
interpretability, shedding light on the effectiveness of foundation models in
facilitating XAI. The code will be made publically available.",0
LM2D: Lyrics- and Music-Driven Dance Synthesis,2403.09407v1,http://arxiv.org/abs/2403.09407v1,2024-03-14 13:59:04+00:00,"Dance typically involves professional choreography with complex movements
that follow a musical rhythm and can also be influenced by lyrical content. The
integration of lyrics in addition to the auditory dimension, enriches the
foundational tone and makes motion generation more amenable to its semantic
meanings. However, existing dance synthesis methods tend to model motions only
conditioned on audio signals. In this work, we make two contributions to bridge
this gap. First, we propose LM2D, a novel probabilistic architecture that
incorporates a multimodal diffusion model with consistency distillation,
designed to create dance conditioned on both music and lyrics in one diffusion
generation step. Second, we introduce the first 3D dance-motion dataset that
encompasses both music and lyrics, obtained with pose estimation technologies.
We evaluate our model against music-only baseline models with objective metrics
and human evaluations, including dancers and choreographers. The results
demonstrate LM2D is able to produce realistic and diverse dance matching both
lyrics and music. A video summary can be accessed at:
https://youtu.be/4XCgvYookvA.",0
Heuristic Reasoning in AI: Instrumental Use and Mimetic Absorption,2403.09404v1,http://arxiv.org/abs/2403.09404v1,2024-03-14 13:53:05+00:00,"We propose a novel program of heuristic reasoning within artificial
intelligence (AI) systems. Through a series of innovative experiments,
including variations of the classic Linda problem and a novel application of
the Beauty Contest game, we uncover trade-offs between accuracy maximization
and effort reduction that shape the conditions under which AIs transition
between exhaustive logical processing and the use of cognitive shortcuts
(heuristics). We distinguish between the 'instrumental' use of heuristics to
match resources with objectives, and 'mimetic absorption,' whereby heuristics
are learned from humans, and manifest randomly and universally. We provide
evidence that AI, despite lacking intrinsic goals or self-awareness, manifests
an adaptive balancing of precision and efficiency, consistent with principles
of resource-rational human cognition as explicated in classical theories of
bounded rationality and dual-process theory.",0
Unsupervised Modality-Transferable Video Highlight Detection with Representation Activation Sequence Learning,2403.09401v1,http://arxiv.org/abs/2403.09401v1,2024-03-14 13:52:03+00:00,"Identifying highlight moments of raw video materials is crucial for improving
the efficiency of editing videos that are pervasive on internet platforms.
However, the extensive work of manually labeling footage has created obstacles
to applying supervised methods to videos of unseen categories. The absence of
an audio modality that contains valuable cues for highlight detection in many
videos also makes it difficult to use multimodal strategies. In this paper, we
propose a novel model with cross-modal perception for unsupervised highlight
detection. The proposed model learns representations with visual-audio level
semantics from image-audio pair data via a self-reconstruction task. To achieve
unsupervised highlight detection, we investigate the latent representations of
the network and propose the representation activation sequence learning (RASL)
module with k-point contrastive learning to learn significant representation
activations. To connect the visual modality with the audio modality, we use the
symmetric contrastive learning (SCL) module to learn the paired visual and
audio representations. Furthermore, an auxiliary task of masked feature vector
sequence (FVS) reconstruction is simultaneously conducted during pretraining
for representation enhancement. During inference, the cross-modal pretrained
model can generate representations with paired visual-audio semantics given
only the visual modality. The RASL module is used to output the highlight
scores. The experimental results show that the proposed framework achieves
superior performance compared to other state-of-the-art approaches.",0
ConDiSR: Contrastive Disentanglement and Style Regularization for Single Domain Generalization,2403.09400v1,http://arxiv.org/abs/2403.09400v1,2024-03-14 13:50:44+00:00,"Medical data often exhibits distribution shifts, which cause test-time
performance degradation for deep learning models trained using standard
supervised learning pipelines. This challenge is addressed in the field of
Domain Generalization (DG) with the sub-field of Single Domain Generalization
(SDG) being specifically interesting due to the privacy- or logistics-related
issues often associated with medical data. Existing disentanglement-based SDG
methods heavily rely on structural information embedded in segmentation masks,
however classification labels do not provide such dense information. This work
introduces a novel SDG method aimed at medical image classification that
leverages channel-wise contrastive disentanglement. It is further enhanced with
reconstruction-based style regularization to ensure extraction of distinct
style and structure feature representations. We evaluate our method on the
complex task of multicenter histopathology image classification, comparing it
against state-of-the-art (SOTA) SDG baselines. Results demonstrate that our
method surpasses the SOTA by a margin of 1% in average accuracy while also
showing more stable performance. This study highlights the importance and
challenges of exploring SDG frameworks in the context of the classification
task. The code is publicly available at
https://github.com/BioMedIA-MBZUAI/ConDiSR",0
Learning to optimize with convergence guarantees using nonlinear system theory,2403.09389v1,http://arxiv.org/abs/2403.09389v1,2024-03-14 13:40:26+00:00,"The increasing reliance on numerical methods for controlling dynamical
systems and training machine learning models underscores the need to devise
algorithms that dependably and efficiently navigate complex optimization
landscapes. Classical gradient descent methods offer strong theoretical
guarantees for convex problems; however, they demand meticulous hyperparameter
tuning for non-convex ones. The emerging paradigm of learning to optimize (L2O)
automates the discovery of algorithms with optimized performance leveraging
learning models and data - yet, it lacks a theoretical framework to analyze
convergence and robustness of the learned algorithms. In this paper, we fill
this gap by harnessing nonlinear system theory. Specifically, we propose an
unconstrained parametrization of all convergent algorithms for smooth
non-convex objective functions. Notably, our framework is directly compatible
with automatic differentiation tools, ensuring convergence by design while
learning to optimize.",0
Pantypes: Diverse Representatives for Self-Explainable Models,2403.09383v1,http://arxiv.org/abs/2403.09383v1,2024-03-14 13:34:30+00:00,"Prototypical self-explainable classifiers have emerged to meet the growing
demand for interpretable AI systems. These classifiers are designed to
incorporate high transparency in their decisions by basing inference on
similarity with learned prototypical objects. While these models are designed
with diversity in mind, the learned prototypes often do not sufficiently
represent all aspects of the input distribution, particularly those in low
density regions. Such lack of sufficient data representation, known as
representation bias, has been associated with various detrimental properties
related to machine learning diversity and fairness. In light of this, we
introduce pantypes, a new family of prototypical objects designed to capture
the full diversity of the input distribution through a sparse set of objects.
We show that pantypes can empower prototypical self-explainable models by
occupying divergent regions of the latent space and thus fostering high
diversity, interpretability and fairness.",0
Introducing Routing Functions to Vision-Language Parameter-Efficient Fine-Tuning with Low-Rank Bottlenecks,2403.09377v1,http://arxiv.org/abs/2403.09377v1,2024-03-14 13:27:42+00:00,"Mainstream parameter-efficient fine-tuning (PEFT) methods, such as LoRA or
Adapter, project a model's hidden states to a lower dimension, allowing
pre-trained models to adapt to new data through this low-rank bottleneck.
However, PEFT tasks involving multiple modalities, like vision-language (VL)
tasks, require not only adaptation to new data but also learning the
relationship between different modalities. Targeting at VL PEFT tasks, we
propose a family of operations, called routing functions, to enhance VL
alignment in the low-rank bottlenecks. The routing functions adopt linear
operations and do not introduce new trainable parameters. In-depth analyses are
conducted to study their behavior. In various VL PEFT settings, the routing
functions significantly improve performance of the original PEFT methods,
achieving over 20% improvement on VQAv2
($\text{RoBERTa}_{\text{large}}$+ViT-L/16) and 30% on COCO Captioning
(GPT2-medium+ViT-L/16). Also when fine-tuning a pre-trained multimodal model
such as CLIP-BART, we observe smaller but consistent improvements across a
range of VL PEFT tasks.",0
Machine Learning Processes as Sources of Ambiguity: Insights from AI Art,2403.09374v1,http://arxiv.org/abs/2403.09374v1,2024-03-14 13:21:48+00:00,"Ongoing efforts to turn Machine Learning (ML) into a design material have
encountered limited success. This paper examines the burgeoning area of AI art
to understand how artists incorporate ML in their creative work. Drawing upon
related HCI theories, we investigate how artists create ambiguity by analyzing
nine AI artworks that use computer vision and image synthesis. Our analysis
shows that, in addition to the established types of ambiguity, artists worked
closely with the ML process (dataset curation, model training, and application)
and developed various techniques to evoke the ambiguity of processes. Our
finding indicates that the current conceptualization of ML as a design material
needs to reframe the ML process as design elements, instead of technical
details. Finally, this paper offers reflections on commonly held assumptions in
HCI about ML uncertainty, dependability, and explainability, and advocates to
supplement the artifact-centered design perspective of ML with a
process-centered one.",0
Noise-aware neural network for stochastic dynamics simulation,2403.09370v1,http://arxiv.org/abs/2403.09370v1,2024-03-14 13:17:37+00:00,"In the presence of system-environment coupling, classical complex systems
undergo stochastic dynamics, where rich phenomena can emerge at large
spatio-temporal scales. To investigate these phenomena, numerical approaches
for simulating stochastic dynamics are indispensable and can be computationally
expensive. In light of the recent fast development in machine learning
techniques, here, we establish a generic machine learning approach to simulate
the stochastic dynamics, dubbed the noise-aware neural network (NANN). One key
feature of this approach is its ability to generate the long-time stochastic
dynamics of complex large-scale systems by just training NANN with the one-step
dynamics of smaller-scale systems, thus reducing the computational cost.
Furthermore, this NANN based approach is quite generic. Case-by-case special
design of the architecture of NANN is not necessary when it is employed to
investigate different stochastic complex systems. Using the noisy Kuramoto
model and the Vicsek model as concrete examples, we demonstrate its capability
in simulating stochastic dynamics. We believe that this novel machine learning
approach can be a useful tool in investigating the large spatio-temporal
scaling behavior of complex systems subjected to the influences of the
environmental noise.",0
DF4LCZ: A SAM-Empowered Data Fusion Framework for Scene-Level Local Climate Zone Classification,2403.09367v1,http://arxiv.org/abs/2403.09367v1,2024-03-14 13:15:46+00:00,"Recent advancements in remote sensing (RS) technologies have shown their
potential in accurately classifying local climate zones (LCZs). However,
traditional scene-level methods using convolutional neural networks (CNNs)
often struggle to integrate prior knowledge of ground objects effectively.
Moreover, commonly utilized data sources like Sentinel-2 encounter difficulties
in capturing detailed ground object information. To tackle these challenges, we
propose a data fusion method that integrates ground object priors extracted
from high-resolution Google imagery with Sentinel-2 multispectral imagery. The
proposed method introduces a novel Dual-stream Fusion framework for LCZ
classification (DF4LCZ), integrating instance-based location features from
Google imagery with the scene-level spatial-spectral features extracted from
Sentinel-2 imagery. The framework incorporates a Graph Convolutional Network
(GCN) module empowered by the Segment Anything Model (SAM) to enhance feature
extraction from Google imagery. Simultaneously, the framework employs a 3D-CNN
architecture to learn the spectral-spatial features of Sentinel-2 imagery.
Experiments are conducted on a multi-source remote sensing image dataset
specifically designed for LCZ classification, validating the effectiveness of
the proposed DF4LCZ. The related code and dataset are available at
https://github.com/ctrlovefly/DF4LCZ.",0
Sentinel-Guided Zero-Shot Learning: A Collaborative Paradigm without Real Data Exposure,2403.09363v1,http://arxiv.org/abs/2403.09363v1,2024-03-14 13:12:49+00:00,"With increasing concerns over data privacy and model copyrights, especially
in the context of collaborations between AI service providers and data owners,
an innovative SG-ZSL paradigm is proposed in this work. SG-ZSL is designed to
foster efficient collaboration without the need to exchange models or sensitive
data. It consists of a teacher model, a student model and a generator that
links both model entities. The teacher model serves as a sentinel on behalf of
the data owner, replacing real data, to guide the student model at the AI
service provider's end during training. Considering the disparity of knowledge
space between the teacher and student, we introduce two variants of the teacher
model: the omniscient and the quasi-omniscient teachers. Under these teachers'
guidance, the student model seeks to match the teacher model's performance and
explores domains that the teacher has not covered. To trade off between privacy
and performance, we further introduce two distinct security-level training
protocols: white-box and black-box, enhancing the paradigm's adaptability.
Despite the inherent challenges of real data absence in the SG-ZSL paradigm, it
consistently outperforms in ZSL and GZSL tasks, notably in the white-box
protocol. Our comprehensive evaluation further attests to its robustness and
efficiency across various setups, including stringent black-box training
protocol.",0
D3T: Distinctive Dual-Domain Teacher Zigzagging Across RGB-Thermal Gap for Domain-Adaptive Object Detection,2403.09359v1,http://arxiv.org/abs/2403.09359v1,2024-03-14 13:05:43+00:00,"Domain adaptation for object detection typically entails transferring
knowledge from one visible domain to another visible domain. However, there are
limited studies on adapting from the visible to the thermal domain, because the
domain gap between the visible and thermal domains is much larger than
expected, and traditional domain adaptation can not successfully facilitate
learning in this situation. To overcome this challenge, we propose a
Distinctive Dual-Domain Teacher (D3T) framework that employs distinct training
paradigms for each domain. Specifically, we segregate the source and target
training sets for building dual-teachers and successively deploy exponential
moving average to the student model to individual teachers of each domain. The
framework further incorporates a zigzag learning method between dual teachers,
facilitating a gradual transition from the visible to thermal domains during
training. We validate the superiority of our method through newly designed
experimental protocols with well-known thermal datasets, i.e., FLIR and KAIST.
Source code is available at https://github.com/EdwardDo69/D3T .",0
JAXbind: Bind any function to JAX,2403.08847v1,http://arxiv.org/abs/2403.08847v1,2024-03-13 16:50:04+00:00,"JAX is widely used in machine learning and scientific computing, the latter
of which often relies on existing high-performance code that we would ideally
like to incorporate into JAX. Reimplementing the existing code in JAX is often
impractical and the existing interface in JAX for binding custom code requires
deep knowledge of JAX and its C++ backend. The goal of JAXbind is to
drastically reduce the effort required to bind custom functions implemented in
other programming languages to JAX. Specifically, JAXbind provides an
easy-to-use Python interface for defining custom so-called JAX primitives that
support arbitrary JAX transformations.",0
Digital Twin-assisted Reinforcement Learning for Resource-aware Microservice Offloading in Edge Computing,2403.08687v1,http://arxiv.org/abs/2403.08687v1,2024-03-13 16:44:36+00:00,"Collaborative edge computing (CEC) has emerged as a promising paradigm,
enabling edge nodes to collaborate and execute microservices from end devices.
Microservice offloading, a fundamentally important problem, decides when and
where microservices are executed upon the arrival of services. However, the
dynamic nature of the real-world CEC environment often leads to inefficient
microservice offloading strategies, resulting in underutilized resources and
network congestion. To address this challenge, we formulate an online joint
microservice offloading and bandwidth allocation problem, JMOBA, to minimize
the average completion time of services. In this paper, we introduce a novel
microservice offloading algorithm, DTDRLMO, which leverages deep reinforcement
learning (DRL) and digital twin technology. Specifically, we employ digital
twin techniques to predict and adapt to changing edge node loads and network
conditions of CEC in real-time. Furthermore, this approach enables the
generation of an efficient offloading plan, selecting the most suitable edge
node for each microservice. Simulation results on real-world and synthetic
datasets demonstrate that DTDRLMO outperforms heuristic and learning-based
methods in average service completion time.",0
KnowCoder: Coding Structured Knowledge into LLMs for Universal Information Extraction,2403.07969v2,http://arxiv.org/abs/2403.07969v2,2024-03-12 14:56:34+00:00,"In this paper, we propose KnowCoder, a Large Language Model (LLM) to conduct
Universal Information Extraction (UIE) via code generation. KnowCoder aims to
develop a kind of unified schema representation that LLMs can easily understand
and an effective learning framework that encourages LLMs to follow schemas and
extract structured knowledge accurately. To achieve these, KnowCoder introduces
a code-style schema representation method to uniformly transform different
schemas into Python classes, with which complex schema information, such as
constraints among tasks in UIE, can be captured in an LLM-friendly manner. We
further construct a code-style schema library covering over $\textbf{30,000}$
types of knowledge, which is the largest one for UIE, to the best of our
knowledge. To ease the learning process of LLMs, KnowCoder contains a two-phase
learning framework that enhances its schema understanding ability via code
pretraining and its schema following ability via instruction tuning. After code
pretraining on around $1.5$B automatically constructed data, KnowCoder already
attains remarkable generalization ability and achieves relative improvements by
$\textbf{49.8%}$ F1, compared to LLaMA2, under the few-shot setting. After
instruction tuning, KnowCoder further exhibits strong generalization ability on
unseen schemas and achieves up to $\textbf{12.5%}$ and $\textbf{21.9%}$,
compared to sota baselines, under the zero-shot setting and the low resource
setting, respectively. Additionally, based on our unified schema
representations, various human-annotated datasets can simultaneously be
utilized to refine KnowCoder, which achieves significant improvements up to
$\textbf{7.5%}$ under the supervised setting.",0
On the resolution of the sign of gluon polarization in the proton,2403.08117v1,http://arxiv.org/abs/2403.08117v1,2024-03-12 22:57:38+00:00,"Recently the possible existence of negative gluon helicity, $\Delta g$, has
been observed to be compatible with existing empirical constraints, including
from jet production in polarized proton-proton collisions at RHIC, and lattice
QCD data on polarized gluon Ioffe time distributions. We perform a new global
analysis of polarized parton distributions in the proton with new constraints
from the high-$x$ region of deep-inelastic scattering (DIS). A dramatic
reduction in the quality of the fit for the negative $\Delta g$ replicas
compared to those with positive $\Delta g$ suggest that the negative $\Delta g$
solution cannot simultaneously account for high-$x$ polarized DIS data along
with lattice and polarized jet data.",0
NeRF-Supervised Feature Point Detection and Description,2403.08156v1,http://arxiv.org/abs/2403.08156v1,2024-03-13 00:43:10+00:00,"Feature point detection and description is the backbone for various computer
vision applications, such as Structure-from-Motion, visual SLAM, and visual
place recognition. While learning-based methods have surpassed traditional
handcrafted techniques, their training often relies on simplistic
homography-based simulations of multi-view perspectives, limiting model
generalisability. This paper introduces a novel approach leveraging neural
radiance fields (NeRFs) for realistic multi-view training data generation. We
create a diverse multi-view dataset using NeRFs, consisting of indoor and
outdoor scenes. Our proposed methodology adapts state-of-the-art feature
detectors and descriptors to train on NeRF-synthesised views supervised by
perspective projective geometry. Our experiments demonstrate that the proposed
methods achieve competitive or superior performance on standard benchmarks for
relative pose estimation, point cloud registration, and homography estimation
while requiring significantly less training data compared to existing
approaches.",0
The Effect of Different Optimization Strategies to Physics-Constrained Deep Learning for Soil Moisture Estimation,2403.08154v1,http://arxiv.org/abs/2403.08154v1,2024-03-13 00:32:30+00:00,"Soil moisture is a key hydrological parameter that has significant importance
to human society and the environment. Accurate modeling and monitoring of soil
moisture in crop fields, especially in the root zone (top 100 cm of soil), is
essential for improving agricultural production and crop yield with the help of
precision irrigation and farming tools. Realizing the full sensor data
potential depends greatly on advanced analytical and predictive domain-aware
models. In this work, we propose a physics-constrained deep learning (P-DL)
framework to integrate physics-based principles on water transport and water
sensing signals for effective reconstruction of the soil moisture dynamics. We
adopt three different optimizers, namely Adam, RMSprop, and GD, to minimize the
loss function of P-DL during the training process. In the illustrative case
study, we demonstrate the empirical convergence of Adam optimizers outperforms
the other optimization methods in both mini-batch and full-batch training.",0
Multi-Fidelity Reinforcement Learning for Time-Optimal Quadrotor Re-planning,2403.08152v1,http://arxiv.org/abs/2403.08152v1,2024-03-13 00:30:09+00:00,"High-speed online trajectory planning for UAVs poses a significant challenge
due to the need for precise modeling of complex dynamics while also being
constrained by computational limitations. This paper presents a multi-fidelity
reinforcement learning method (MFRL) that aims to effectively create a
realistic dynamics model and simultaneously train a planning policy that can be
readily deployed in real-time applications. The proposed method involves the
co-training of a planning policy and a reward estimator; the latter predicts
the performance of the policy's output and is trained efficiently through
multi-fidelity Bayesian optimization. This optimization approach models the
correlation between different fidelity levels, thereby constructing a
high-fidelity model based on a low-fidelity foundation, which enables the
accurate development of the reward model with limited high-fidelity
experiments. The framework is further extended to include real-world flight
experiments in reinforcement learning training, allowing the reward model to
precisely reflect real-world constraints and broadening the policy's
applicability to real-world scenarios. We present rigorous evaluations by
training and testing the planning policy in both simulated and real-world
environments. The resulting trained policy not only generates faster and more
reliable trajectories compared to the baseline snap minimization method, but it
also achieves trajectory updates in 2 ms on average, while the baseline method
takes several minutes.",0
Measuring the Energy Consumption and Efficiency of Deep Neural Networks: An Empirical Analysis and Design Recommendations,2403.08151v1,http://arxiv.org/abs/2403.08151v1,2024-03-13 00:27:19+00:00,"Addressing the so-called ``Red-AI'' trend of rising energy consumption by
large-scale neural networks, this study investigates the actual energy
consumption, as measured by node-level watt-meters, of training various fully
connected neural network architectures. We introduce the BUTTER-E dataset, an
augmentation to the BUTTER Empirical Deep Learning dataset, containing energy
consumption and performance data from 63,527 individual experimental runs
spanning 30,582 distinct configurations: 13 datasets, 20 sizes (number of
trainable parameters), 8 network ``shapes'', and 14 depths on both CPU and GPU
hardware collected using node-level watt-meters. This dataset reveals the
complex relationship between dataset size, network structure, and energy use,
and highlights the impact of cache effects. We propose a straightforward and
effective energy model that accounts for network size, computing, and memory
hierarchy. Our analysis also uncovers a surprising, hardware-mediated
non-linear relationship between energy efficiency and network design,
challenging the assumption that reducing the number of parameters or FLOPs is
the best way to achieve greater energy efficiency. Highlighting the need for
cache-considerate algorithm development, we suggest a combined approach to
energy efficient network, algorithm, and hardware design. This work contributes
to the fields of sustainable computing and Green AI, offering practical
guidance for creating more energy-efficient neural networks and promoting
sustainable AI.",0
Representing Molecules as Random Walks Over Interpretable Grammars,2403.08147v1,http://arxiv.org/abs/2403.08147v1,2024-03-13 00:19:06+00:00,"Recent research in molecular discovery has primarily been devoted to small,
drug-like molecules, leaving many similarly important applications in material
design without adequate technology. These applications often rely on more
complex molecular structures with fewer examples that are carefully designed
using known substructures. We propose a data-efficient and interpretable model
for representing and reasoning over such molecules in terms of graph grammars
that explicitly describe the hierarchical design space featuring motifs to be
the design basis. We present a novel representation in the form of random walks
over the design space, which facilitates both molecule generation and property
prediction. We demonstrate clear advantages over existing methods in terms of
performance, efficiency, and synthesizability of predicted molecules, and we
provide detailed insights into the method's chemical interpretability.",0
Algorithmic Information Disclosure in Optimal Auctions,2403.08145v1,http://arxiv.org/abs/2403.08145v1,2024-03-13 00:11:27+00:00,"This paper studies a joint design problem where a seller can design both the
signal structures for the agents to learn their values, and the allocation and
payment rules for selling the item. In his seminal work, Myerson (1981) shows
how to design the optimal auction with exogenous signals. We show that the
problem becomes NP-hard when the seller also has the ability to design the
signal structures. Our main result is a polynomial-time approximation scheme
(PTAS) for computing the optimal joint design with at most an $\epsilon$
multiplicative loss in expected revenue. Moreover, we show that in our joint
design problem, the seller can significantly reduce the information rent of the
agents by providing partial information, which ensures a revenue that is at
least $1 - \frac{1}{e}$ of the optimal welfare for all valuation distributions.",0
"Prosody for Intuitive Robotic Interface Design: It's Not What You Said, It's How You Said It",2403.08144v1,http://arxiv.org/abs/2403.08144v1,2024-03-13 00:10:18+00:00,"In this paper, we investigate the use of 'prosody' (the musical elements of
speech) as a communicative signal for intuitive human-robot interaction
interfaces. Our approach, rooted in Research through Design (RtD), examines the
application of prosody in directing a quadruped robot navigation. We involved
ten team members in an experiment to command a robot through an obstacle course
using natural interaction. A human operator, serving as the robot's sensory and
processing proxy, translated human communication into a basic set of navigation
commands, effectively simulating an intuitive interface. During our analysis of
interaction videos, when lexical and visual cues proved insufficient for
accurate command interpretation, we turned to non-verbal auditory cues.
Qualitative evidence suggests that participants intuitively relied on prosody
to control robot navigation. We highlight specific distinct prosodic constructs
that emerged from this preliminary exploration and discuss their pragmatic
functions. This work contributes a discussion on the broader potential of
prosody as a multifunctional communicative signal for designing future
intuitive robotic interfaces, enabling lifelong learning and personalization in
human-robot interaction.",0
BAGEL: Bootstrapping Agents by Guiding Exploration with Language,2403.08140v1,http://arxiv.org/abs/2403.08140v1,2024-03-12 23:59:15+00:00,"Following natural language instructions by executing actions in digital
environments (e.g. web-browsers and REST APIs) is a challenging task for
language model (LM) agents. Unfortunately, LM agents often fail to generalize
to new environments without human demonstrations. This work presents BAGEL, a
method for bootstrapping LM agents without human supervision. BAGEL converts a
seed set of randomly explored trajectories or synthetic instructions, into
demonstrations, via round-trips between two noisy LM components: an LM labeler
which converts a trajectory into a synthetic instruction, and a zero-shot LM
agent which maps the synthetic instruction into a refined trajectory. By
performing these round-trips iteratively, BAGEL quickly converts the initial
distribution of trajectories towards those that are well-described by natural
language. We use BAGEL demonstrations to adapt a zero shot LM agent at test
time via in-context learning over retrieved demonstrations, and find
improvements of over 2-13% absolute on ToolQA and MiniWob++, with up to 13x
reduction in execution failures.",0
Physics-Inspired Deep Learning Anti-Aliasing Framework in Efficient Channel State Feedback,2403.08133v1,http://arxiv.org/abs/2403.08133v1,2024-03-12 23:40:51+00:00,"Acquiring downlink channel state information (CSI) at the base station is
vital for optimizing performance in massive Multiple input multiple output
(MIMO) Frequency-Division Duplexing (FDD) systems. While deep learning
architectures have been successful in facilitating UE-side CSI feedback and
gNB-side recovery, the undersampling issue prior to CSI feedback is often
overlooked. This issue, which arises from low density pilot placement in
current standards, results in significant aliasing effects in outdoor channels
and consequently limits CSI recovery performance. To this end, this work
introduces a new CSI upsampling framework at the gNB as a post-processing
solution to address the gaps caused by undersampling. Leveraging the physical
principles of discrete Fourier transform shifting theorem and multipath
reciprocity, our framework effectively uses uplink CSI to mitigate aliasing
effects. We further develop a learning-based method that integrates the
proposed algorithm with the Iterative Shrinkage-Thresholding Algorithm Net
(ISTA-Net) architecture, enhancing our approach for non-uniform sampling
recovery. Our numerical results show that both our rule-based and deep learning
methods significantly outperform traditional interpolation techniques and
current state-of-the-art approaches in terms of performance.",0
Cost-Effective Methodology for Complex Tuning Searches in HPC: Navigating Interdependencies and Dimensionality,2403.08131v1,http://arxiv.org/abs/2403.08131v1,2024-03-12 23:39:43+00:00,"Tuning searches are pivotal in High-Performance Computing (HPC), addressing
complex optimization challenges in computational applications. The complexity
arises not only from finely tuning parameters within routines but also
potential interdependencies among them, rendering traditional optimization
methods inefficient. Instead of scrutinizing interdependencies among parameters
and routines, practitioners often face the dilemma of conducting independent
tuning searches for each routine, thereby overlooking interdependence, or
pursuing a more resource-intensive joint search for all routines. This decision
is driven by the consideration that some interdependence analysis and
high-dimensional decomposition techniques in literature may be prohibitively
expensive in HPC tuning searches. Our methodology adapts and refines these
methods to ensure computational feasibility while maximizing performance gains
in real-world scenarios. Our methodology leverages a cost-effective
interdependence analysis to decide whether to merge several tuning searches
into a joint search or conduct orthogonal searches. Tested on synthetic
functions with varying levels of parameter interdependence, our methodology
efficiently explores the search space. In comparison to
Bayesian-optimization-based full independent or fully joint searches, our
methodology suggested an optimized breakdown of independent and merged searches
that led to final configurations up to 8% more accurate, reducing the search
time by up to 95%. When applied to GPU-offloaded Real-Time Time-Dependent
Density Functional Theory (RT-TDDFT), an application in computational materials
science that challenges modern HPC autotuners, our methodology achieved an
effective tuning search. Its adaptability and efficiency extend beyond
RT-TDDFT, making it valuable for related applications in HPC.",0
Towards Independence Criterion in Machine Unlearning of Features and Labels,2403.08124v1,http://arxiv.org/abs/2403.08124v1,2024-03-12 23:21:09+00:00,"This work delves into the complexities of machine unlearning in the face of
distributional shifts, particularly focusing on the challenges posed by
non-uniform feature and label removal. With the advent of regulations like the
GDPR emphasizing data privacy and the right to be forgotten, machine learning
models face the daunting task of unlearning sensitive information without
compromising their integrity or performance. Our research introduces a novel
approach that leverages influence functions and principles of distributional
independence to address these challenges. By proposing a comprehensive
framework for machine unlearning, we aim to ensure privacy protection while
maintaining model performance and adaptability across varying distributions.
Our method not only facilitates efficient data removal but also dynamically
adjusts the model to preserve its generalization capabilities. Through
extensive experimentation, we demonstrate the efficacy of our approach in
scenarios characterized by significant distributional shifts, making
substantial contributions to the field of machine unlearning. This research
paves the way for developing more resilient and adaptable unlearning
techniques, ensuring models remain robust and accurate in the dynamic landscape
of data privacy and machine learning.",0
Early Directional Convergence in Deep Homogeneous Neural Networks for Small Initializations,2403.08121v1,http://arxiv.org/abs/2403.08121v1,2024-03-12 23:17:32+00:00,"This paper studies the gradient flow dynamics that arise when training deep
homogeneous neural networks, starting with small initializations. The present
work considers neural networks that are assumed to have locally Lipschitz
gradients and an order of homogeneity strictly greater than two. This paper
demonstrates that for sufficiently small initializations, during the early
stages of training, the weights of the neural network remain small in norm and
approximately converge in direction along the Karush-Kuhn-Tucker (KKT) points
of the neural correlation function introduced in [1]. Additionally, for square
loss and under a separability assumption on the weights of neural networks, a
similar directional convergence of gradient flow dynamics is shown near certain
saddle points of the loss function.",0
Characterising harmful data sources when constructing multi-fidelity surrogate models,2403.08118v1,http://arxiv.org/abs/2403.08118v1,2024-03-12 22:57:53+00:00,"Surrogate modelling techniques have seen growing attention in recent years
when applied to both modelling and optimisation of industrial design problems.
These techniques are highly relevant when assessing the performance of a
particular design carries a high cost, as the overall cost can be mitigated via
the construction of a model to be queried in lieu of the available high-cost
source. The construction of these models can sometimes employ other sources of
information which are both cheaper and less accurate. The existence of these
sources however poses the question of which sources should be used when
constructing a model. Recent studies have attempted to characterise harmful
data sources to guide practitioners in choosing when to ignore a certain
source. These studies have done so in a synthetic setting, characterising
sources using a large amount of data that is not available in practice. Some of
these studies have also been shown to potentially suffer from bias in the
benchmarks used in the analysis. In this study, we present a characterisation
of harmful low-fidelity sources using only the limited data available to train
a surrogate model. We employ recently developed benchmark filtering techniques
to conduct a bias-free assessment, providing objectively varied benchmark
suites of different sizes for future research. Analysing one of these benchmark
suites with the technique known as Instance Space Analysis, we provide an
intuitive visualisation of when a low-fidelity source should be used and use
this analysis to provide guidelines that can be used in an applied industrial
setting.",0
VANP: Learning Where to See for Navigation with Self-Supervised Vision-Action Pre-Training,2403.08109v1,http://arxiv.org/abs/2403.08109v1,2024-03-12 22:33:08+00:00,"Humans excel at efficiently navigating through crowds without collision by
focusing on specific visual regions relevant to navigation. However, most
robotic visual navigation methods rely on deep learning models pre-trained on
vision tasks, which prioritize salient objects -- not necessarily relevant to
navigation and potentially misleading. Alternative approaches train specialized
navigation models from scratch, requiring significant computation. On the other
hand, self-supervised learning has revolutionized computer vision and natural
language processing, but its application to robotic navigation remains
underexplored due to the difficulty of defining effective self-supervision
signals. Motivated by these observations, in this work, we propose a
Self-Supervised Vision-Action Model for Visual Navigation Pre-Training (VANP).
Instead of detecting salient objects that are beneficial for tasks such as
classification or detection, VANP learns to focus only on specific visual
regions that are relevant to the navigation task. To achieve this, VANP uses a
history of visual observations, future actions, and a goal image for
self-supervision, and embeds them using two small Transformer Encoders. Then,
VANP maximizes the information between the embeddings by using a mutual
information maximization objective function. We demonstrate that most
VANP-extracted features match with human navigation intuition. VANP achieves
comparable performance as models learned end-to-end with half the training time
and models trained on a large-scale, fully supervised dataset, i.e., ImageNet,
with only 0.08% data.",0
Continuous Object State Recognition for Cooking Robots Using Pre-Trained Vision-Language Models and Black-box Optimization,2403.08239v1,http://arxiv.org/abs/2403.08239v1,2024-03-13 04:45:40+00:00,"The state recognition of the environment and objects by robots is generally
based on the judgement of the current state as a classification problem. On the
other hand, state changes of food in cooking happen continuously and need to be
captured not only at a certain time point but also continuously over time. In
addition, the state changes of food are complex and cannot be easily described
by manual programming. Therefore, we propose a method to recognize the
continuous state changes of food for cooking robots through the spoken language
using pre-trained large-scale vision-language models. By using models that can
compute the similarity between images and texts continuously over time, we can
capture the state changes of food while cooking. We also show that by adjusting
the weighting of each text prompt based on fitting the similarity changes to a
sigmoid function and then performing black-box optimization, more accurate and
robust continuous state recognition can be achieved. We demonstrate the
effectiveness and limitations of this method by performing the recognition of
water boiling, butter melting, egg cooking, and onion stir-frying.",0
TaskCLIP: Extend Large Vision-Language Model for Task Oriented Object Detection,2403.08108v1,http://arxiv.org/abs/2403.08108v1,2024-03-12 22:33:02+00:00,"Task-oriented object detection aims to find objects suitable for
accomplishing specific tasks. As a challenging task, it requires simultaneous
visual data processing and reasoning under ambiguous semantics. Recent
solutions are mainly all-in-one models. However, the object detection backbones
are pre-trained without text supervision. Thus, to incorporate task
requirements, their intricate models undergo extensive learning on a highly
imbalanced and scarce dataset, resulting in capped performance, laborious
training, and poor generalizability. In contrast, we propose TaskCLIP, a more
natural two-stage design composed of general object detection and task-guided
object selection. Particularly for the latter, we resort to the recently
successful large Vision-Language Models (VLMs) as our backbone, which provides
rich semantic knowledge and a uniform embedding space for images and texts.
Nevertheless, the naive application of VLMs leads to sub-optimal quality, due
to the misalignment between embeddings of object images and their visual
attributes, which are mainly adjective phrases. To this end, we design a
transformer-based aligner after the pre-trained VLMs to re-calibrate both
embeddings. Finally, we employ a trainable score function to post-process the
VLM matching results for object selection. Experimental results demonstrate
that our TaskCLIP outperforms the state-of-the-art DETR-based model TOIST by
3.5% and only requires a single NVIDIA RTX 4090 for both training and
inference.",0
Efficient Language Model Architectures for Differentially Private Federated Learning,2403.08100v1,http://arxiv.org/abs/2403.08100v1,2024-03-12 22:21:48+00:00,"Cross-device federated learning (FL) is a technique that trains a model on
data distributed across typically millions of edge devices without data leaving
the devices. SGD is the standard client optimizer for on device training in
cross-device FL, favored for its memory and computational efficiency. However,
in centralized training of neural language models, adaptive optimizers are
preferred as they offer improved stability and performance. In light of this,
we ask if language models can be modified such that they can be efficiently
trained with SGD client optimizers and answer this affirmatively.
  We propose a scale-invariant Coupled Input Forget Gate (SI CIFG) recurrent
network by modifying the sigmoid and tanh activations in the recurrent cell and
show that this new model converges faster and achieves better utility than the
standard CIFG recurrent model in cross-device FL in large scale experiments. We
further show that the proposed scale invariant modification also helps in
federated learning of larger transformer models. Finally, we demonstrate the
scale invariant modification is also compatible with other non-adaptive
algorithms. Particularly, our results suggest an improved privacy utility
trade-off in federated learning with differential privacy.",0
Mechanics of Next Token Prediction with Self-Attention,2403.08081v1,http://arxiv.org/abs/2403.08081v1,2024-03-12 21:15:38+00:00,"Transformer-based language models are trained on large datasets to predict
the next token given an input sequence. Despite this simple training objective,
they have led to revolutionary advances in natural language processing.
Underlying this success is the self-attention mechanism. In this work, we ask:
$\textit{What}$ $\textit{does}$ $\textit{a}$ $\textit{single}$
$\textit{self-attention}$ $\textit{layer}$ $\textit{learn}$ $\textit{from}$
$\textit{next-token}$ $\textit{prediction?}$ We show that training
self-attention with gradient descent learns an automaton which generates the
next token in two distinct steps: $\textbf{(1)}$ $\textbf{Hard}$
$\textbf{retrieval:}$ Given input sequence, self-attention precisely selects
the $\textit{high-priority}$ $\textit{input}$ $\textit{tokens}$ associated with
the last input token. $\textbf{(2)}$ $\textbf{Soft}$ $\textbf{composition:}$ It
then creates a convex combination of the high-priority tokens from which the
next token can be sampled. Under suitable conditions, we rigorously
characterize these mechanics through a directed graph over tokens extracted
from the training data. We prove that gradient descent implicitly discovers the
strongly-connected components (SCC) of this graph and self-attention learns to
retrieve the tokens that belong to the highest-priority SCC available in the
context window. Our theory relies on decomposing the model weights into a
directional component and a finite component that correspond to hard retrieval
and soft composition steps respectively. This also formalizes a related
implicit bias formula conjectured in [Tarzanagh et al. 2023]. We hope that
these findings shed light on how self-attention processes sequential data and
pave the path toward demystifying more complex architectures.",0
A Multimodal Intermediate Fusion Network with Manifold Learning for Stress Detection,2403.08077v1,http://arxiv.org/abs/2403.08077v1,2024-03-12 21:06:19+00:00,"Multimodal deep learning methods capture synergistic features from multiple
modalities and have the potential to improve accuracy for stress detection
compared to unimodal methods. However, this accuracy gain typically comes from
high computational cost due to the high-dimensional feature spaces, especially
for intermediate fusion. Dimensionality reduction is one way to optimize
multimodal learning by simplifying data and making the features more amenable
to processing and analysis, thereby reducing computational complexity. This
paper introduces an intermediate multimodal fusion network with manifold
learning-based dimensionality reduction. The multimodal network generates
independent representations from biometric signals and facial landmarks through
1D-CNN and 2D-CNN. Finally, these features are fused and fed to another 1D-CNN
layer, followed by a fully connected dense layer. We compared various
dimensionality reduction techniques for different variations of unimodal and
multimodal networks. We observe that the intermediate-level fusion with the
Multi-Dimensional Scaling (MDS) manifold method showed promising results with
an accuracy of 96.00\% in a Leave-One-Subject-Out Cross-Validation (LOSO-CV)
paradigm over other dimensional reduction methods. MDS had the highest
computational cost among manifold learning methods. However, while
outperforming other networks, it managed to reduce the computational cost of
the proposed networks by 25\% when compared to six well-known conventional
feature selection methods used in the preprocessing step.",0
FluoroSAM: A Language-aligned Foundation Model for X-ray Image Segmentation,2403.08059v1,http://arxiv.org/abs/2403.08059v1,2024-03-12 20:11:38+00:00,"Automated X-ray image segmentation would accelerate research and development
in diagnostic and interventional precision medicine. Prior efforts have
contributed task-specific models capable of solving specific image analysis
problems, but the utility of these models is restricted to their particular
task domain, and expanding to broader use requires additional data, labels, and
retraining efforts. Recently, foundation models (FMs) -- machine learning
models trained on large amounts of highly variable data thus enabling broad
applicability -- have emerged as promising tools for automated image analysis.
Existing FMs for medical image analysis focus on scenarios and modalities where
objects are clearly defined by visually apparent boundaries, such as surgical
tool segmentation in endoscopy. X-ray imaging, by contrast, does not generally
offer such clearly delineated boundaries or structure priors. During X-ray
image formation, complex 3D structures are projected in transmission onto the
imaging plane, resulting in overlapping features of varying opacity and shape.
To pave the way toward an FM for comprehensive and automated analysis of
arbitrary medical X-ray images, we develop FluoroSAM, a language-aligned
variant of the Segment-Anything Model, trained from scratch on 1.6M synthetic
X-ray images. FluoroSAM is trained on data including masks for 128 organ types
and 464 non-anatomical objects, such as tools and implants. In real X-ray
images of cadaveric specimens, FluoroSAM is able to segment bony anatomical
structures based on text-only prompting with 0.51 and 0.79 DICE with
point-based refinement, outperforming competing SAM variants for all
structures. FluoroSAM is also capable of zero-shot generalization to segmenting
classes beyond the training set thanks to its language alignment, which we
demonstrate for full lung segmentation on real chest X-rays.",0
CHAI: Clustered Head Attention for Efficient LLM Inference,2403.08058v1,http://arxiv.org/abs/2403.08058v1,2024-03-12 20:10:04+00:00,"Large Language Models (LLMs) with hundreds of billions of parameters have
transformed the field of machine learning. However, serving these models at
inference time is both compute and memory intensive, where a single request can
require multiple GPUs and tens of Gigabytes of memory. Multi-Head Attention is
one of the key components of LLMs, which can account for over 50% of LLMs
memory and compute requirement. We observe that there is a high amount of
redundancy across heads on which tokens they pay attention to. Based on this
insight, we propose Clustered Head Attention (CHAI). CHAI combines heads with a
high amount of correlation for self-attention at runtime, thus reducing both
memory and compute. In our experiments, we show that CHAI is able to reduce the
memory requirements for storing K,V cache by up to 21.4% and inference time
latency by up to 1.73x without any fine-tuning required. CHAI achieves this
with a maximum 3.2% deviation in accuracy across 3 different models (i.e.
OPT-66B, LLAMA-7B, LLAMA-33B) and 5 different evaluation datasets.",0
DrivAerNet: A Parametric Car Dataset for Data-Driven Aerodynamic Design and Graph-Based Drag Prediction,2403.08055v1,http://arxiv.org/abs/2403.08055v1,2024-03-12 20:02:39+00:00,"This study introduces DrivAerNet, a large-scale high-fidelity CFD dataset of
3D industry-standard car shapes, and RegDGCNN, a dynamic graph convolutional
neural network model, both aimed at aerodynamic car design through machine
learning. DrivAerNet, with its 4000 detailed 3D car meshes using 0.5 million
surface mesh faces and comprehensive aerodynamic performance data comprising of
full 3D pressure, velocity fields, and wall-shear stresses, addresses the
critical need for extensive datasets to train deep learning models in
engineering applications. It is 60\% larger than the previously available
largest public dataset of cars, and is the only open-source dataset that also
models wheels and underbody. RegDGCNN leverages this large-scale dataset to
provide high-precision drag estimates directly from 3D meshes, bypassing
traditional limitations such as the need for 2D image rendering or Signed
Distance Fields (SDF). By enabling fast drag estimation in seconds, RegDGCNN
facilitates rapid aerodynamic assessments, offering a substantial leap towards
integrating data-driven methods in automotive design. Together, DrivAerNet and
RegDGCNN promise to accelerate the car design process and contribute to the
development of more efficient vehicles. To lay the groundwork for future
innovations in the field, the dataset and code used in our study are publicly
accessible at \url{https://github.com/Mohamedelrefaie/DrivAerNet}",0
Learning-based Prescribed-Time Safety for Control of Unknown Systems with Control Barrier Functions,2403.08054v1,http://arxiv.org/abs/2403.08054v1,2024-03-12 20:01:36+00:00,"In many control system applications, state constraint satisfaction needs to
be guaranteed within a prescribed time. While this issue has been partially
addressed for systems with known dynamics, it remains largely unaddressed for
systems with unknown dynamics. In this paper, we propose a Gaussian
process-based time-varying control method that leverages backstepping and
control barrier functions to achieve safety requirements within prescribed time
windows. It can be used to keep a system within a safe region or to make it
return to a safe region within a limited time window. These properties are
cemented by rigorous theoretical results. The effectiveness of the proposed
controller is demonstrated in a simulation of a robotic manipulator.",0
TutoAI: A Cross-domain Framework for AI-assisted Mixed-media Tutorial Creation on Physical Tasks,2403.08049v1,http://arxiv.org/abs/2403.08049v1,2024-03-12 19:46:59+00:00,"Mixed-media tutorials, which integrate videos, images, text, and diagrams to
teach procedural skills, offer more browsable alternatives than timeline-based
videos. However, manually creating such tutorials is tedious, and existing
automated solutions are often restricted to a particular domain. While AI
models hold promise, it is unclear how to effectively harness their powers,
given the multi-modal data involved and the vast landscape of models. We
present TutoAI, a cross-domain framework for AI-assisted mixed-media tutorial
creation on physical tasks. First, we distill common tutorial components by
surveying existing work; then, we present an approach to identify, assemble,
and evaluate AI models for component extraction; finally, we propose guidelines
for designing user interfaces (UI) that support tutorial creation based on
AI-generated components. We show that TutoAI has achieved higher or similar
quality compared to a baseline model in preliminary user studies.",0
Authorship Style Transfer with Policy Optimization,2403.08043v1,http://arxiv.org/abs/2403.08043v1,2024-03-12 19:34:54+00:00,"Authorship style transfer aims to rewrite a given text into a specified
target while preserving the original meaning in the source. Existing approaches
rely on the availability of a large number of target style exemplars for model
training. However, these overlook cases where a limited number of target style
examples are available. The development of parameter-efficient transfer
learning techniques and policy optimization (PO) approaches suggest lightweight
PO is a feasible approach to low-resource style transfer. In this work, we
propose a simple two step tune-and-optimize technique for low-resource textual
style transfer. We apply our technique to authorship transfer as well as a
larger-data native language style task and in both cases find it outperforms
state-of-the-art baseline models.",0
CT evaluation of 2D and 3D holistic deep learning methods for the volumetric segmentation of airway lesions,2403.08042v1,http://arxiv.org/abs/2403.08042v1,2024-03-12 19:34:50+00:00,"This research embarked on a comparative exploration of the holistic
segmentation capabilities of Convolutional Neural Networks (CNNs) in both 2D
and 3D formats, focusing on cystic fibrosis (CF) lesions. The study utilized
data from two CF reference centers, covering five major CF structural changes.
Initially, it compared the 2D and 3D models, highlighting the 3D model's
superior capability in capturing complex features like mucus plugs and
consolidations. To improve the 2D model's performance, a loss adapted to fine
structures segmentation was implemented and evaluated, significantly enhancing
its accuracy, though not surpassing the 3D model's performance. The models
underwent further validation through external evaluation against pulmonary
function tests (PFTs), confirming the robustness of the findings. Moreover,
this study went beyond comparing metrics; it also included comprehensive
assessments of the models' interpretability and reliability, providing valuable
insights for their clinical application.",0
MicroT: Low-Energy and Adaptive Models for MCUs,2403.08040v1,http://arxiv.org/abs/2403.08040v1,2024-03-12 19:23:13+00:00,"We propose MicroT, a low-energy, multi-task adaptive model framework for
resource-constrained MCUs. We divide the original model into a feature
extractor and a classifier. The feature extractor is obtained through
self-supervised knowledge distillation and further optimized into part and full
models through model splitting and joint training. These models are then
deployed on MCUs, with classifiers added and trained on local tasks, ultimately
performing stage-decision for joint inference. In this process, the part model
initially processes the sample, and if the confidence score falls below the set
threshold, the full model will resume and continue the inference. We evaluate
MicroT on two models, three datasets, and two MCU boards. Our experimental
evaluation shows that MicroT effectively improves model performance and reduces
energy consumption when dealing with multiple local tasks. Compared to the
unoptimized feature extractor, MicroT can improve accuracy by up to 9.87%. On
MCUs, compared to the standard full model inference, MicroT can save up to
about 29.13% in energy consumption. MicroT also allows users to adaptively
adjust the stage-decision ratio as needed, better balancing model performance
and energy consumption. Under the standard stage-decision ratio configuration,
MicroT can increase accuracy by 5.91% and save about 14.47% of energy
consumption.",0
Giant radio galaxies in the LOFAR deep fields,2403.08037v1,http://arxiv.org/abs/2403.08037v1,2024-03-12 19:18:47+00:00,"In this study, we compare the radio, optical and environmental properties of
GRGs with those of a control sample of smaller RGs we found in the three
LOw-Frequency ARray (LOFAR) deep fields, namely the Bootes, ELAIS-N1, Lockman
Hole, for a total area of about 95 deg^2. We inspected the LOFAR deep fields
and created a catalogue of 1609 extended radio galaxies (ERGs). By visual
inspection, we identified their host galaxies and spectroscopically or
photometrically classified 280 of these as GRGs. We studied their properties,
such as their accretion state, stellar mass and star formation rate (SFR) using
deep optical and infrared survey data. Moreover, we explored the environment in
terms of the surface number density of neighbouring galaxies within these
surveys. Integrated flux densities and radio luminosities were also determined
for a subset of ERGs through available survey images at 50, 150, 610, and 1400
MHz to compute integrated spectral indices. Considering the fraction of GRGs
displaying an FRII morphology alongside the host galaxy properties, we suggest
that GRGs consistently possess sufficient power to overcome jet frustration
caused by the interstellar medium. Moreover, clear differences emerge in the
environmental densities between GRGs and smaller RGs, using the number of
neighbouring galaxies within 10 Mpc from the host galaxy as a proxy. GRGs
preferentially reside in sparser environments compared to their smaller
counterparts. In particular, only 3.6% of the GRGs reside within a 3D comoving
distance of 5 Mpc from a previously reported galaxy cluster. We found that
larger sources exhibit steeper integrated spectral indices, suggesting that
GRGs are late-stage versions of RGs. These results suggest that GRGs are
amongst the oldest radio sources with the most stable nuclear activity that
reside in sparse environments.",0
Multiscale Low-Frequency Memory Network for Improved Feature Extraction in Convolutional Neural Networks,2403.08157v1,http://arxiv.org/abs/2403.08157v1,2024-03-13 00:48:41+00:00,"Deep learning and Convolutional Neural Networks (CNNs) have driven major
transformations in diverse research areas. However, their limitations in
handling low-frequency information present obstacles in certain tasks like
interpreting global structures or managing smooth transition images. Despite
the promising performance of transformer structures in numerous tasks, their
intricate optimization complexities highlight the persistent need for refined
CNN enhancements using limited resources. Responding to these complexities, we
introduce a novel framework, the Multiscale Low-Frequency Memory (MLFM)
Network, with the goal to harness the full potential of CNNs while keeping
their complexity unchanged. The MLFM efficiently preserves low-frequency
information, enhancing performance in targeted computer vision tasks. Central
to our MLFM is the Low-Frequency Memory Unit (LFMU), which stores various
low-frequency data and forms a parallel channel to the core network. A key
advantage of MLFM is its seamless compatibility with various prevalent
networks, requiring no alterations to their original core structure. Testing on
ImageNet demonstrated substantial accuracy improvements in multiple 2D CNNs,
including ResNet, MobileNet, EfficientNet, and ConvNeXt. Furthermore, we
showcase MLFM's versatility beyond traditional image classification by
successfully integrating it into image-to-image translation tasks, specifically
in semantic segmentation networks like FCN and U-Net. In conclusion, our work
signifies a pivotal stride in the journey of optimizing the efficacy and
efficiency of CNNs with limited resources. This research builds upon the
existing CNN foundations and paves the way for future advancements in computer
vision. Our codes are available at https://github.com/AlphaWuSeu/ MLFM.",0
Asymptotics of Random Feature Regression Beyond the Linear Scaling Regime,2403.08160v1,http://arxiv.org/abs/2403.08160v1,2024-03-13 00:59:25+00:00,"Recent advances in machine learning have been achieved by using
overparametrized models trained until near interpolation of the training data.
It was shown, e.g., through the double descent phenomenon, that the number of
parameters is a poor proxy for the model complexity and generalization
capabilities. This leaves open the question of understanding the impact of
parametrization on the performance of these models. How does model complexity
and generalization depend on the number of parameters $p$? How should we choose
$p$ relative to the sample size $n$ to achieve optimal test error?
  In this paper, we investigate the example of random feature ridge regression
(RFRR). This model can be seen either as a finite-rank approximation to kernel
ridge regression (KRR), or as a simplified model for neural networks trained in
the so-called lazy regime. We consider covariates uniformly distributed on the
$d$-dimensional sphere and compute sharp asymptotics for the RFRR test error in
the high-dimensional polynomial scaling, where $p,n,d \to \infty$ while $p/
d^{\kappa_1}$ and $n / d^{\kappa_2}$ stay constant, for all $\kappa_1 ,
\kappa_2 \in \mathbb{R}_{>0}$. These asymptotics precisely characterize the
impact of the number of random features and regularization parameter on the
test performance. In particular, RFRR exhibits an intuitive trade-off between
approximation and generalization power. For $n = o(p)$, the sample size $n$ is
the bottleneck and RFRR achieves the same performance as KRR (which is
equivalent to taking $p = \infty$). On the other hand, if $p = o(n)$, the
number of random features $p$ is the limiting factor and RFRR test error
matches the approximation error of the random feature model class (akin to
taking $n = \infty$). Finally, a double descent appears at $n= p$, a phenomenon
that was previously only characterized in the linear scaling $\kappa_1 =
\kappa_2 = 1$.",0
LAFS: Landmark-based Facial Self-supervised Learning for Face Recognition,2403.08161v1,http://arxiv.org/abs/2403.08161v1,2024-03-13 01:07:55+00:00,"In this work we focus on learning facial representations that can be adapted
to train effective face recognition models, particularly in the absence of
labels. Firstly, compared with existing labelled face datasets, a vastly larger
magnitude of unlabeled faces exists in the real world. We explore the learning
strategy of these unlabeled facial images through self-supervised pretraining
to transfer generalized face recognition performance. Moreover, motivated by
one recent finding, that is, the face saliency area is critical for face
recognition, in contrast to utilizing random cropped blocks of images for
constructing augmentations in pretraining, we utilize patches localized by
extracted facial landmarks. This enables our method - namely LAndmark-based
Facial Self-supervised learning LAFS), to learn key representation that is more
critical for face recognition. We also incorporate two landmark-specific
augmentations which introduce more diversity of landmark information to further
regularize the learning. With learned landmark-based facial representations, we
further adapt the representation for face recognition with regularization
mitigating variations in landmark positions. Our method achieves significant
improvement over the state-of-the-art on multiple face recognition benchmarks,
especially on more challenging few-shot scenarios.",0
Iterative Learning for Joint Image Denoising and Motion Artifact Correction of 3D Brain MRI,2403.08162v1,http://arxiv.org/abs/2403.08162v1,2024-03-13 01:18:55+00:00,"Image noise and motion artifacts greatly affect the quality of brain MRI and
negatively influence downstream medical image analysis. Previous studies often
focus on 2D methods that process each volumetric MR image slice-by-slice, thus
losing important 3D anatomical information. Additionally, these studies
generally treat image denoising and artifact correction as two standalone
tasks, without considering their potential relationship, especially on
low-quality images where severe noise and motion artifacts occur
simultaneously. To address these issues, we propose a Joint image Denoising and
motion Artifact Correction (JDAC) framework via iterative learning to handle
noisy MRIs with motion artifacts, consisting of an adaptive denoising model and
an anti-artifact model. In the adaptive denoising model, we first design a
novel noise level estimation strategy, and then adaptively reduce the noise
through a U-Net backbone with feature normalization conditioning on the
estimated noise variance. The anti-artifact model employs another U-Net for
eliminating motion artifacts, incorporating a novel gradient-based loss
function designed to maintain the integrity of brain anatomy during the motion
correction process. These two models are iteratively employed for joint image
denoising and artifact correction through an iterative learning framework. An
early stopping strategy depending on noise level estimation is applied to
accelerate the iteration process. The denoising model is trained with 9,544
T1-weighted MRIs with manually added Gaussian noise as supervision. The
anti-artifact model is trained on 552 T1-weighted MRIs with motion artifacts
and paired motion-free images. Experimental results on a public dataset and a
clinical study suggest the effectiveness of JDAC in both tasks of denoising and
motion artifact correction, compared with several state-of-the-art methods.",0
Point Cloud Compression via Constrained Optimal Transport,2403.08236v1,http://arxiv.org/abs/2403.08236v1,2024-03-13 04:36:24+00:00,"This paper presents a novel point cloud compression method COT-PCC by
formulating the task as a constrained optimal transport (COT) problem. COT-PCC
takes the bitrate of compressed features as an extra constraint of optimal
transport (OT) which learns the distribution transformation between original
and reconstructed points. Specifically, the formulated COT is implemented with
a generative adversarial network (GAN) and a bitrate loss for training. The
discriminator measures the Wasserstein distance between input and reconstructed
points, and a generator calculates the optimal mapping between distributions of
input and reconstructed point cloud. Moreover, we introduce a learnable
sampling module for downsampling in the compression procedure. Extensive
results on both sparse and dense point cloud datasets demonstrate that COT-PCC
outperforms state-of-the-art methods in terms of both CD and PSNR metrics.
Source codes are available at \url{https://github.com/cognaclee/PCC-COT}.",0
Robust Decision Aggregation with Adversarial Experts,2403.08222v1,http://arxiv.org/abs/2403.08222v1,2024-03-13 03:47:08+00:00,"We consider a binary decision aggregation problem in the presence of both
truthful and adversarial experts. The truthful experts will report their
private signals truthfully with proper incentive, while the adversarial experts
can report arbitrarily. The decision maker needs to design a robust aggregator
to forecast the true state of the world based on the reports of experts. The
decision maker does not know the specific information structure, which is a
joint distribution of signals, states, and strategies of adversarial experts.
We want to find the optimal aggregator minimizing regret under the worst
information structure. The regret is defined by the difference in expected loss
between the aggregator and a benchmark who makes the optimal decision given the
joint distribution and reports of truthful experts.
  We prove that when the truthful experts are symmetric and adversarial experts
are not too numerous, the truncated mean is optimal, which means that we remove
some lowest reports and highest reports and take averaging among the left
reports. Moreover, for many settings, the optimal aggregators are in the family
of piecewise linear functions. The regret is independent of the total number of
experts but only depends on the ratio of adversaries. We evaluate our
aggregators by numerical experiment in an ensemble learning task. We also
obtain some negative results for the aggregation problem with adversarial
experts under some more general information structures and experts' report
space.",0
Efficient geometric Markov chain Monte Carlo for nonlinear Bayesian inversion enabled by derivative-informed neural operators,2403.08220v1,http://arxiv.org/abs/2403.08220v1,2024-03-13 03:45:14+00:00,"We propose an operator learning approach to accelerate geometric Markov chain
Monte Carlo (MCMC) for solving infinite-dimensional nonlinear Bayesian inverse
problems. While geometric MCMC employs high-quality proposals that adapt to
posterior local geometry, it requires computing local gradient and Hessian
information of the log-likelihood, incurring a high cost when the
parameter-to-observable (PtO) map is defined through expensive model
simulations. We consider a delayed-acceptance geometric MCMC method driven by a
neural operator surrogate of the PtO map, where the proposal is designed to
exploit fast surrogate approximations of the log-likelihood and,
simultaneously, its gradient and Hessian. To achieve a substantial speedup, the
surrogate needs to be accurate in predicting both the observable and its
parametric derivative (the derivative of the observable with respect to the
parameter). Training such a surrogate via conventional operator learning using
input--output samples often demands a prohibitively large number of model
simulations. In this work, we present an extension of derivative-informed
operator learning [O'Leary-Roseberry et al., J. Comput. Phys., 496 (2024)]
using input--output--derivative training samples. Such a learning method leads
to derivative-informed neural operator (DINO) surrogates that accurately
predict the observable and its parametric derivative at a significantly lower
training cost than the conventional method. Cost and error analysis for reduced
basis DINO surrogates are provided. Numerical studies on PDE-constrained
Bayesian inversion demonstrate that DINO-driven MCMC generates effective
posterior samples 3--9 times faster than geometric MCMC and 60--97 times faster
than prior geometry-based MCMC. Furthermore, the training cost of DINO
surrogates breaks even after collecting merely 10--25 effective posterior
samples compared to geometric MCMC.",0
SpaceOctopus: An Octopus-inspired Motion Planning Framework for Multi-arm Space Robot,2403.08219v1,http://arxiv.org/abs/2403.08219v1,2024-03-13 03:34:00+00:00,"Space robots have played a critical role in autonomous maintenance and space
junk removal. Multi-arm space robots can efficiently complete the target
capture and base reorientation tasks due to their flexibility and the
collaborative capabilities between the arms. However, the complex coupling
properties arising from both the multiple arms and the free-floating base
present challenges to the motion planning problems of multi-arm space robots.
We observe that the octopus elegantly achieves similar goals when grabbing prey
and escaping from danger. Inspired by the distributed control of octopuses'
limbs, we develop a multi-level decentralized motion planning framework to
manage the movement of different arms of space robots. This motion planning
framework integrates naturally with the multi-agent reinforcement learning
(MARL) paradigm. The results indicate that our method outperforms the previous
method (centralized training). Leveraging the flexibility of the decentralized
framework, we reassemble policies trained for different tasks, enabling the
space robot to complete trajectory planning tasks while adjusting the base
attitude without further learning. Furthermore, our experiments confirm the
superior robustness of our method in the face of external disturbances,
changing base masses, and even the failure of one arm.",0
Research on the Application of Deep Learning-based BERT Model in Sentiment Analysis,2403.08217v1,http://arxiv.org/abs/2403.08217v1,2024-03-13 03:31:26+00:00,"This paper explores the application of deep learning techniques, particularly
focusing on BERT models, in sentiment analysis. It begins by introducing the
fundamental concept of sentiment analysis and how deep learning methods are
utilized in this domain. Subsequently, it delves into the architecture and
characteristics of BERT models. Through detailed explanation, it elucidates the
application effects and optimization strategies of BERT models in sentiment
analysis, supported by experimental validation. The experimental findings
indicate that BERT models exhibit robust performance in sentiment analysis
tasks, with notable enhancements post fine-tuning. Lastly, the paper concludes
by summarizing the potential applications of BERT models in sentiment analysis
and suggests directions for future research and practical implementations.",0
PaddingFlow: Improving Normalizing Flows with Padding-Dimensional Noise,2403.08216v1,http://arxiv.org/abs/2403.08216v1,2024-03-13 03:28:39+00:00,"Normalizing flow is a generative modeling approach with efficient sampling.
However, Flow-based models suffer two issues, which are manifold and discrete
data. If the target distribution is a manifold, which means the dimension of
the latent target distribution and the dimension of the data distribution are
unmatched, flow-based models might perform badly. Discrete data makes
flow-based models collapse into a degenerate mixture of point masses. In this
paper, to sidestep such two issues we propose PaddingFlow, a novel
dequantization method, which improves normalizing flows with
padding-dimensional noise. PaddingFlow is easy to implement, computationally
cheap, widely suitable for various tasks, and generates samples that are
unbiased estimations of the data. Especially, our method can overcome the
limitation of existing dequantization methods that have to change the data
distribution, which might degrade performance. We validate our method on the
main benchmarks of unconditional density estimation, including five tabular
datasets and four image datasets for VAE models, and the IK experiments which
are conditional density estimation. The results show that PaddingFlow can
provide improvement on all tasks in this paper.",0
LIX: Implicitly Infusing Spatial Geometric Prior Knowledge into Visual Semantic Segmentation for Autonomous Driving,2403.08215v1,http://arxiv.org/abs/2403.08215v1,2024-03-13 03:24:36+00:00,"Despite the impressive performance achieved by data-fusion networks with
duplex encoders for visual semantic segmentation, they become ineffective when
spatial geometric data are not available. Implicitly infusing the spatial
geometric prior knowledge acquired by a duplex-encoder teacher model into a
single-encoder student model is a practical, albeit less explored research
avenue. This paper delves into this topic and resorts to knowledge distillation
approaches to address this problem. We introduce the Learning to Infuse ""X""
(LIX) framework, with novel contributions in both logit distillation and
feature distillation aspects. We present a mathematical proof that underscores
the limitation of using a single fixed weight in decoupled knowledge
distillation and introduce a logit-wise dynamic weight controller as a solution
to this issue. Furthermore, we develop an adaptively-recalibrated feature
distillation algorithm, including two technical novelties: feature
recalibration via kernel regression and in-depth feature consistency
quantification via centered kernel alignment. Extensive experiments conducted
with intermediate-fusion and late-fusion networks across various public
datasets provide both quantitative and qualitative evaluations, demonstrating
the superior performance of our LIX framework when compared to other
state-of-the-art approaches.",0
"P2LHAP:Wearable sensor-based human activity recognition, segmentation and forecast through Patch-to-Label Seq2Seq Transformer",2403.08214v1,http://arxiv.org/abs/2403.08214v1,2024-03-13 03:23:50+00:00,"Traditional deep learning methods struggle to simultaneously segment,
recognize, and forecast human activities from sensor data. This limits their
usefulness in many fields such as healthcare and assisted living, where
real-time understanding of ongoing and upcoming activities is crucial. This
paper introduces P2LHAP, a novel Patch-to-Label Seq2Seq framework that tackles
all three tasks in a efficient single-task model. P2LHAP divides sensor data
streams into a sequence of ""patches"", served as input tokens, and outputs a
sequence of patch-level activity labels including the predicted future
activities. A unique smoothing technique based on surrounding patch labels, is
proposed to identify activity boundaries accurately. Additionally, P2LHAP
learns patch-level representation by sensor signal channel-independent
Transformer encoders and decoders. All channels share embedding and Transformer
weights across all sequences. Evaluated on three public datasets, P2LHAP
significantly outperforms the state-of-the-art in all three tasks,
demonstrating its effectiveness and potential for real-world applications.",0
Advancing Security in AI Systems: A Novel Approach to Detecting Backdoors in Deep Neural Networks,2403.08208v1,http://arxiv.org/abs/2403.08208v1,2024-03-13 03:10:11+00:00,"In the rapidly evolving landscape of communication and network security, the
increasing reliance on deep neural networks (DNNs) and cloud services for data
processing presents a significant vulnerability: the potential for backdoors
that can be exploited by malicious actors. Our approach leverages advanced
tensor decomposition algorithms Independent Vector Analysis (IVA), Multiset
Canonical Correlation Analysis (MCCA), and Parallel Factor Analysis (PARAFAC2)
to meticulously analyze the weights of pre-trained DNNs and distinguish between
backdoored and clean models effectively. The key strengths of our method lie in
its domain independence, adaptability to various network architectures, and
ability to operate without access to the training data of the scrutinized
models. This not only ensures versatility across different application
scenarios but also addresses the challenge of identifying backdoors without
prior knowledge of the specific triggers employed to alter network behavior. We
have applied our detection pipeline to three distinct computer vision datasets,
encompassing both image classification and object detection tasks. The results
demonstrate a marked improvement in both accuracy and efficiency over existing
backdoor detection methods. This advancement enhances the security of deep
learning and AI in networked systems, providing essential cybersecurity against
evolving threats in emerging technologies.",0
BG-HGNN: Toward Scalable and Efficient Heterogeneous Graph Neural Network,2403.08207v1,http://arxiv.org/abs/2403.08207v1,2024-03-13 03:03:40+00:00,"Many computer vision and machine learning problems are modelled as learning
tasks on heterogeneous graphs, featuring a wide array of relations from diverse
types of nodes and edges. Heterogeneous graph neural networks (HGNNs) stand out
as a promising neural model class designed for heterogeneous graphs. Built on
traditional GNNs, existing HGNNs employ different parameter spaces to model the
varied relationships. However, the practical effectiveness of existing HGNNs is
often limited to simple heterogeneous graphs with few relation types. This
paper first highlights and demonstrates that the standard approach employed by
existing HGNNs inevitably leads to parameter explosion and relation collapse,
making HGNNs less effective or impractical for complex heterogeneous graphs
with numerous relation types. To overcome this issue, we introduce a novel
framework, Blend&Grind-HGNN (BG-HGNN), which effectively tackles the challenges
by carefully integrating different relations into a unified feature space
manageable by a single set of parameters. This results in a refined HGNN method
that is more efficient and effective in learning from heterogeneous graphs,
especially when the number of relations grows. Our empirical studies illustrate
that BG-HGNN significantly surpasses existing HGNNs in terms of parameter
efficiency (up to 28.96 $\times$), training throughput (up to 8.12 $\times$),
and accuracy (up to 1.07 $\times$).",0
Discrete Semantic Tokenization for Deep CTR Prediction,2403.08206v1,http://arxiv.org/abs/2403.08206v1,2024-03-13 03:03:15+00:00,"Incorporating item content information into click-through rate (CTR)
prediction models remains a challenge, especially with the time and space
constraints of industrial scenarios. The content-encoding paradigm, which
integrates user and item encoders directly into CTR models, prioritizes space
over time. In contrast, the embedding-based paradigm transforms item and user
semantics into latent embeddings and then caches them, prioritizes space over
time. In this paper, we introduce a new semantic-token paradigm and propose a
discrete semantic tokenization approach, namely UIST, for user and item
representation. UIST facilitates swift training and inference while maintaining
a conservative memory footprint. Specifically, UIST quantizes dense embedding
vectors into discrete tokens with shorter lengths and employs a hierarchical
mixture inference module to weigh the contribution of each user--item token
pair. Our experimental results on news recommendation showcase the
effectiveness and efficiency (about 200-fold space compression) of UIST for CTR
prediction.",0
AutoDFP: Automatic Data-Free Pruning via Channel Similarity Reconstruction,2403.08204v1,http://arxiv.org/abs/2403.08204v1,2024-03-13 02:56:31+00:00,"Structured pruning methods are developed to bridge the gap between the
massive scale of neural networks and the limited hardware resources. Most
current structured pruning methods rely on training datasets to fine-tune the
compressed model, resulting in high computational burdens and being
inapplicable for scenarios with stringent requirements on privacy and security.
As an alternative, some data-free methods have been proposed, however, these
methods often require handcraft parameter tuning and can only achieve
inflexible reconstruction. In this paper, we propose the Automatic Data-Free
Pruning (AutoDFP) method that achieves automatic pruning and reconstruction
without fine-tuning. Our approach is based on the assumption that the loss of
information can be partially compensated by retaining focused information from
similar channels. Specifically, We formulate data-free pruning as an
optimization problem, which can be effectively addressed through reinforcement
learning. AutoDFP assesses the similarity of channels for each layer and
provides this information to the reinforcement learning agent, guiding the
pruning and reconstruction process of the network. We evaluate AutoDFP with
multiple networks on multiple datasets, achieving impressive compression
results. For instance, on the CIFAR-10 dataset, AutoDFP demonstrates a 2.87\%
reduction in accuracy loss compared to the recently proposed data-free pruning
method DFPC with fewer FLOPs on VGG-16. Furthermore, on the ImageNet dataset,
AutoDFP achieves 43.17\% higher accuracy than the SOTA method with the same
80\% preserved ratio on MobileNet-V1.",0
Learnable Community-Aware Transformer for Brain Connectome Analysis with Token Clustering,2403.08203v1,http://arxiv.org/abs/2403.08203v1,2024-03-13 02:55:27+00:00,"Neuroscientific research has revealed that the complex brain network can be
organized into distinct functional communities, each characterized by a
cohesive group of regions of interest (ROIs) with strong interconnections.
These communities play a crucial role in comprehending the functional
organization of the brain and its implications for neurological conditions,
including Autism Spectrum Disorder (ASD) and biological differences, such as in
gender. Traditional models have been constrained by the necessity of predefined
community clusters, limiting their flexibility and adaptability in deciphering
the brain's functional organization. Furthermore, these models were restricted
by a fixed number of communities, hindering their ability to accurately
represent the brain's dynamic nature. In this study, we present a token
clustering brain transformer-based model ($\texttt{TC-BrainTF}$) for joint
community clustering and classification. Our approach proposes a novel token
clustering (TC) module based on the transformer architecture, which utilizes
learnable prompt tokens with orthogonal loss where each ROI embedding is
projected onto the prompt embedding space, effectively clustering ROIs into
communities and reducing the dimensions of the node representation via merging
with communities. Our results demonstrate that our learnable community-aware
model $\texttt{TC-BrainTF}$ offers improved accuracy in identifying ASD and
classifying genders through rigorous testing on ABIDE and HCP datasets.
Additionally, the qualitative analysis on $\texttt{TC-BrainTF}$ has
demonstrated the effectiveness of the designed TC module and its relevance to
neuroscience interpretations.",0
Prototyping and Experimental Results for Environment-Aware Millimeter Wave Beam Alignment via Channel Knowledge Map,2403.08200v1,http://arxiv.org/abs/2403.08200v1,2024-03-13 02:54:59+00:00,"Channel knowledge map (CKM), which aims to directly reflect the intrinsic
channel properties of the local wireless environment, is a novel technique for
achieving environmentaware communication. In this paper, to alleviate the large
training overhead in millimeter wave (mmWave) beam alignment, an
environment-aware and training-free beam alignment prototype is established
based on a typical CKM, termed beam index map (BIM). To this end, a general CKM
construction method is first presented, and an indoor BIM is constructed
offline to learn the candidate transmit and receive beam index pairs for each
grid in the experimental area. Furthermore, based on the location information
of the receiver (or the dynamic obstacles) from the ultra-wide band (UWB)
positioning system, the established BIM is used to achieve training-free beam
alignment by directly providing the beam indexes for the transmitter and
receiver. Three typical scenarios are considered in the experiment, including
quasi-static environment with line-of-sight (LoS) link, quasistatic environment
without LoS link and dynamic environment. Besides, the receiver orientation
measured from the gyroscope is also used to help CKM predict more accurate beam
indexes. The experiment results show that compared with the benchmark
location-based beam alignment strategy, the CKM-based beam alignment strategy
can achieve much higher received power, which is close to that achieved by
exhaustive beam search, but with significantly reduced training overhead.",0
Deep Submodular Peripteral Network,2403.08199v1,http://arxiv.org/abs/2403.08199v1,2024-03-13 02:53:52+00:00,"Submodular functions, crucial for various applications, often lack practical
learning methods for their acquisition. Seemingly unrelated, learning a scaling
from oracles offering graded pairwise preferences (GPC) is underexplored,
despite a rich history in psychometrics. In this paper, we introduce deep
submodular peripteral networks (DSPNs), a novel parametric family of submodular
functions, and methods for their training using a contrastive-learning inspired
GPC-ready strategy to connect and then tackle both of the above challenges. We
introduce newly devised GPC-style ""peripteral"" loss which leverages numerically
graded relationships between pairs of objects (sets in our case). Unlike
traditional contrastive learning, our method utilizes graded comparisons,
extracting more nuanced information than just binary-outcome comparisons, and
contrasts sets of any size (not just two). We also define a novel suite of
automatic sampling strategies for training, including active-learning inspired
submodular feedback. We demonstrate DSPNs' efficacy in learning submodularity
from a costly target submodular function showing superiority in downstream
tasks such as experimental design and streaming applications.",0
PAGE: Domain-Incremental Adaptation with Past-Agnostic Generative Replay for Smart Healthcare,2403.08197v1,http://arxiv.org/abs/2403.08197v1,2024-03-13 02:44:33+00:00,"We propose PAGE, a domain-incremental adaptation strategy with past-agnostic
generative replay for smart healthcare. PAGE enables generative replay without
the aid of any preserved data or information from prior domains. When adapting
to a new domain, it exploits real data from the new distribution and the
current model to generate synthetic data that retain the learned knowledge of
previous domains. By replaying the synthetic data with the new real data during
training, PAGE achieves a good balance between domain adaptation and knowledge
retention. In addition, we incorporate an extended inductive conformal
prediction (EICP) method into PAGE to produce a confidence score and a
credibility value for each detection result. This makes the predictions
interpretable and provides statistical guarantees for disease detection in
smart healthcare applications. We demonstrate PAGE's effectiveness in
domain-incremental disease detection with three distinct disease datasets
collected from commercially available WMSs. PAGE achieves highly competitive
performance against state-of-the-art with superior scalability, data privacy,
and feasibility. Furthermore, PAGE can enable up to 75% reduction in clinical
workload with the help of EICP.",0
SpeechColab Leaderboard: An Open-Source Platform for Automatic Speech Recognition Evaluation,2403.08196v1,http://arxiv.org/abs/2403.08196v1,2024-03-13 02:41:53+00:00,"In the wake of the surging tide of deep learning over the past decade,
Automatic Speech Recognition (ASR) has garnered substantial attention, leading
to the emergence of numerous publicly accessible ASR systems that are actively
being integrated into our daily lives. Nonetheless, the impartial and
replicable evaluation of these ASR systems encounters challenges due to various
crucial subtleties. In this paper we introduce the SpeechColab Leaderboard, a
general-purpose, open-source platform designed for ASR evaluation. With this
platform: (i) We report a comprehensive benchmark, unveiling the current
state-of-the-art panorama for ASR systems, covering both open-source models and
industrial commercial services. (ii) We quantize how distinct nuances in the
scoring pipeline influence the final benchmark outcomes. These include nuances
related to capitalization, punctuation, interjection, contraction, synonym
usage, compound words, etc. These issues have gained prominence in the context
of the transition towards an End-to-End future. (iii) We propose a practical
modification to the conventional Token-Error-Rate (TER) evaluation metric, with
inspirations from Kolmogorov complexity and Normalized Information Distance
(NID). This adaptation, called modified-TER (mTER), achieves proper
normalization and symmetrical treatment of reference and hypothesis. By
leveraging this platform as a large-scale testing ground, this study
demonstrates the robustness and backward compatibility of mTER when compared to
TER. The SpeechColab Leaderboard is accessible at
https://github.com/SpeechColab/Leaderboard",0
Unsupervised Learning of Hybrid Latent Dynamics: A Learn-to-Identify Framework,2403.08194v1,http://arxiv.org/abs/2403.08194v1,2024-03-13 02:33:57+00:00,"Modern applications increasingly require unsupervised learning of latent
dynamics from high-dimensional time-series. This presents a significant
challenge of identifiability: many abstract latent representations may
reconstruct observations, yet do they guarantee an adequate identification of
the governing dynamics? This paper investigates this challenge from two angles:
the use of physics inductive bias specific to the data being modeled, and a
learn-to-identify strategy that separates forecasting objectives from the data
used for the identification. We combine these two strategies in a novel
framework for unsupervised meta-learning of hybrid latent dynamics (Meta-HyLaD)
with: 1) a latent dynamic function that hybridize known mathematical
expressions of prior physics with neural functions describing its unknown
errors, and 2) a meta-learning formulation to learn to separately identify both
components of the hybrid dynamics. Through extensive experiments on five
physics and one biomedical systems, we provide strong evidence for the benefits
of Meta-HyLaD to integrate rich prior knowledge while identifying their gap to
observed data.",0
Learning-driven Physically-aware Large-scale Circuit Gate Sizing,2403.08193v1,http://arxiv.org/abs/2403.08193v1,2024-03-13 02:33:28+00:00,"Gate sizing plays an important role in timing optimization after physical
design. Existing machine learning-based gate sizing works cannot optimize
timing on multiple timing paths simultaneously and neglect the physical
constraint on layouts. They cause sub-optimal sizing solutions and
low-efficiency issues when compared with commercial gate sizing tools. In this
work, we propose a learning-driven physically-aware gate sizing framework to
optimize timing performance on large-scale circuits efficiently. In our
gradient descent optimization-based work, for obtaining accurate gradients, a
multi-modal gate sizing-aware timing model is achieved via learning timing
information on multiple timing paths and physical information on
multiple-scaled layouts jointly. Then, gradient generation based on the
sizing-oriented estimator and adaptive back-propagation are developed to update
gate sizes. Our results demonstrate that our work achieves higher timing
performance improvements in a faster way compared with the commercial gate
sizing tool.",0
Synchronized Dual-arm Rearrangement via Cooperative mTSP,2403.08191v1,http://arxiv.org/abs/2403.08191v1,2024-03-13 02:26:15+00:00,"Synchronized dual-arm rearrangement is widely studied as a common scenario in
industrial applications. It often faces scalability challenges due to the
computational complexity of robotic arm rearrangement and the high-dimensional
nature of dual-arm planning. To address these challenges, we formulated the
problem as cooperative mTSP, a variant of mTSP where agents share cooperative
costs, and utilized reinforcement learning for its solution. Our approach
involved representing rearrangement tasks using a task state graph that
captured spatial relationships and a cooperative cost matrix that provided
details about action costs. Taking these representations as observations, we
designed an attention-based network to effectively combine them and provide
rational task scheduling. Furthermore, a cost predictor is also introduced to
directly evaluate actions during both training and planning, significantly
expediting the planning process. Our experimental results demonstrate that our
approach outperforms existing methods in terms of both performance and planning
efficiency.",0
Perceive With Confidence: Statistical Safety Assurances for Navigation with Learning-Based Perception,2403.08185v1,http://arxiv.org/abs/2403.08185v1,2024-03-13 02:18:33+00:00,"Rapid advances in perception have enabled large pre-trained models to be used
out of the box for processing high-dimensional, noisy, and partial observations
of the world into rich geometric representations (e.g., occupancy predictions).
However, safe integration of these models onto robots remains challenging due
to a lack of reliable performance in unfamiliar environments. In this work, we
present a framework for rigorously quantifying the uncertainty of pre-trained
perception models for occupancy prediction in order to provide end-to-end
statistical safety assurances for navigation. We build on techniques from
conformal prediction for producing a calibrated perception system that lightly
processes the outputs of a pre-trained model while ensuring generalization to
novel environments and robustness to distribution shifts in states when
perceptual outputs are used in conjunction with a planner. The calibrated
system can be used in combination with any safe planner to provide an
end-to-end statistical assurance on safety in a new environment with a
user-specified threshold $1-\epsilon$. We evaluate the resulting approach -
which we refer to as Perceive with Confidence (PwC) - with experiments in
simulation and on hardware where a quadruped robot navigates through indoor
environments containing objects unseen during training or calibration. These
experiments validate the safety assurances provided by PwC and demonstrate
significant improvements in empirical safety rates compared to baselines.",0
SeCG: Semantic-Enhanced 3D Visual Grounding via Cross-modal Graph Attention,2403.08182v1,http://arxiv.org/abs/2403.08182v1,2024-03-13 02:11:04+00:00,"3D visual grounding aims to automatically locate the 3D region of the
specified object given the corresponding textual description. Existing works
fail to distinguish similar objects especially when multiple referred objects
are involved in the description. Experiments show that direct matching of
language and visual modal has limited capacity to comprehend complex
referential relationships in utterances. It is mainly due to the interference
caused by redundant visual information in cross-modal alignment. To strengthen
relation-orientated mapping between different modalities, we propose SeCG, a
semantic-enhanced relational learning model based on a graph network with our
designed memory graph attention layer. Our method replaces original
language-independent encoding with cross-modal encoding in visual analysis.
More text-related feature expressions are obtained through the guidance of
global semantics and implicit relationships. Experimental results on ReferIt3D
and ScanRefer benchmarks show that the proposed method outperforms the existing
state-of-the-art methods, particularly improving the localization performance
for the multi-relation challenges.",0
Learning Barrier-Certified Polynomial Dynamical Systems for Obstacle Avoidance with Robots,2403.08178v1,http://arxiv.org/abs/2403.08178v1,2024-03-13 02:04:57+00:00,"Established techniques that enable robots to learn from demonstrations are
based on learning a stable dynamical system (DS). To increase the robots'
resilience to perturbations during tasks that involve static obstacle
avoidance, we propose incorporating barrier certificates into an optimization
problem to learn a stable and barrier-certified DS. Such optimization problem
can be very complex or extremely conservative when the traditional linear
parameter-varying formulation is used. Thus, different from previous approaches
in the literature, we propose to use polynomial representations for DSs, which
yields an optimization problem that can be tackled by sum-of-squares
techniques. Finally, our approach can handle obstacle shapes that fall outside
the scope of assumptions typically found in the literature concerning obstacle
avoidance within the DS learning framework. Supplementary material can be found
at the project webpage: https://martinschonger.github.io/abc-ds",0
Tractable Local Equilibria in Non-Concave Games,2403.08171v1,http://arxiv.org/abs/2403.08171v1,2024-03-13 01:51:30+00:00,"While Online Gradient Descent and other no-regret learning procedures are
known to efficiently converge to coarse correlated equilibrium in games where
each agent's utility is concave in their own strategy, this is not the case
when the utilities are non-concave, a situation that is common in machine
learning applications where the agents' strategies are parameterized by deep
neural networks, or the agents' utilities are computed by a neural network, or
both. Indeed, non-concave games present a host of game-theoretic and
optimization challenges: (i) Nash equilibria may fail to exist; (ii) local Nash
equilibria exist but are intractable; and (iii) mixed Nash, correlated, and
coarse correlated equilibria have infinite support in general, and are
intractable. To sidestep these challenges we propose a new solution concept,
termed $(\varepsilon, \Phi(\delta))$-local equilibrium, which generalizes local
Nash equilibrium in non-concave games, as well as (coarse) correlated
equilibrium in concave games. Importantly, we show that two instantiations of
this solution concept capture the convergence guarantees of Online Gradient
Descent and no-regret learning, which we show efficiently converge to this type
of equilibrium in non-concave games with smooth utilities.",0
"MolBind: Multimodal Alignment of Language, Molecules, and Proteins",2403.08167v1,http://arxiv.org/abs/2403.08167v1,2024-03-13 01:38:42+00:00,"Recent advancements in biology and chemistry have leveraged multi-modal
learning, integrating molecules and their natural language descriptions to
enhance drug discovery. However, current pre-training frameworks are limited to
two modalities, and designing a unified network to process different modalities
(e.g., natural language, 2D molecular graphs, 3D molecular conformations, and
3D proteins) remains challenging due to inherent gaps among them. In this work,
we propose MolBind, a framework that trains encoders for multiple modalities
through contrastive learning, mapping all modalities to a shared feature space
for multi-modal semantic alignment. To facilitate effective pre-training of
MolBind on multiple modalities, we also build and collect a high-quality
dataset with four modalities, MolBind-M4, including graph-language,
conformation-language, graph-conformation, and conformation-protein paired
data. MolBind shows superior zero-shot learning performance across a wide range
of tasks, demonstrating its strong capability of capturing the underlying
semantics of multiple modalities.",0
"SN 2023zaw: an ultra-stripped, nickel-poor supernova from a low-mass progenitor",2403.08165v1,http://arxiv.org/abs/2403.08165v1,2024-03-13 01:31:36+00:00,"We present SN 2023zaw $-$ a sub-luminous ($\mathrm{M_r} = -16.7$ mag) and
rapidly-evolving supernova ($\mathrm{t_{1/2,r}} = 4.9$ days), with the lowest
nickel mass ($\approx0.002$ $\mathrm{M_\odot}$) measured among all
stripped-envelope supernovae discovered to date. The photospheric spectra are
dominated by broad He I and Ca NIR emission lines with velocities of $\sim10\
000 - 12\ 000$ $\mathrm{km\ s^{-1}}$. The late-time spectra show prominent
narrow He I emission lines at $\sim$1000$\ \mathrm{km\ s^{-1}}$, indicative of
interaction with He-rich circumstellar material. SN 2023zaw is located in the
spiral arm of a star-forming galaxy. We perform radiation-hydrodynamical and
analytical modeling of the lightcurve by fitting with a combination of
shock-cooling emission and nickel decay. The progenitor has a best-fit envelope
mass of $\approx0.2$ $\mathrm{M_\odot}$ and an envelope radius of $\approx50$
$\mathrm{R_\odot}$. The extremely low nickel mass and low ejecta mass
($\approx0.5$ $\mathrm{M_\odot}$) suggest an ultra-stripped SN, which
originates from a mass-losing low mass He-star (ZAMS mass $<$ 10
$\mathrm{M_\odot}$) in a close binary system. This is a channel to form double
neutron star systems, whose merger is detectable with LIGO. SN 2023zaw
underscores the existence of a previously undiscovered population of extremely
low nickel mass ($< 0.005$ $\mathrm{M_\odot}$) stripped-envelope supernovae,
which can be explored with deep and high-cadence transient surveys.",0
EM-TTS: Efficiently Trained Low-Resource Mongolian Lightweight Text-to-Speech,2403.08164v1,http://arxiv.org/abs/2403.08164v1,2024-03-13 01:27:57+00:00,"Recently, deep learning-based Text-to-Speech (TTS) systems have achieved
high-quality speech synthesis results. Recurrent neural networks have become a
standard modeling technique for sequential data in TTS systems and are widely
used. However, training a TTS model which includes RNN components requires
powerful GPU performance and takes a long time. In contrast, CNN-based sequence
synthesis techniques can significantly reduce the parameters and training time
of a TTS model while guaranteeing a certain performance due to their high
parallelism, which alleviate these economic costs of training. In this paper,
we propose a lightweight TTS system based on deep convolutional neural
networks, which is a two-stage training end-to-end TTS model and does not
employ any recurrent units. Our model consists of two stages: Text2Spectrum and
SSRN. The former is used to encode phonemes into a coarse mel spectrogram and
the latter is used to synthesize the complete spectrum from the coarse mel
spectrogram. Meanwhile, we improve the robustness of our model by a series of
data augmentations, such as noise suppression, time warping, frequency masking
and time masking, for solving the low resource mongolian problem. Experiments
show that our model can reduce the training time and parameters while ensuring
the quality and naturalness of the synthesized speech compared to using
mainstream TTS models. Our method uses NCMMSC2022-MTTSC Challenge dataset for
validation, which significantly reduces training time while maintaining a
certain accuracy.",0
LG-Traj: LLM Guided Pedestrian Trajectory Prediction,2403.08032v1,http://arxiv.org/abs/2403.08032v1,2024-03-12 19:06:23+00:00,"Accurate pedestrian trajectory prediction is crucial for various
applications, and it requires a deep understanding of pedestrian motion
patterns in dynamic environments. However, existing pedestrian trajectory
prediction methods still need more exploration to fully leverage these motion
patterns. This paper investigates the possibilities of using Large Language
Models (LLMs) to improve pedestrian trajectory prediction tasks by inducing
motion cues. We introduce LG-Traj, a novel approach incorporating LLMs to
generate motion cues present in pedestrian past/observed trajectories. Our
approach also incorporates motion cues present in pedestrian future
trajectories by clustering future trajectories of training data using a mixture
of Gaussians. These motion cues, along with pedestrian coordinates, facilitate
a better understanding of the underlying representation. Furthermore, we
utilize singular value decomposition to augment the observed trajectories,
incorporating them into the model learning process to further enhance
representation learning. Our method employs a transformer-based architecture
comprising a motion encoder to model motion patterns and a social decoder to
capture social interactions among pedestrians. We demonstrate the effectiveness
of our approach on popular pedestrian trajectory prediction benchmarks, namely
ETH-UCY and SDD, and present various ablation experiments to validate our
approach.",0
HOLISMOKES -- XII. Time-delay Measurements of Strongly Lensed Type Ia Supernovae using a Long Short-Term Memory Network,2403.08029v1,http://arxiv.org/abs/2403.08029v1,2024-03-12 19:01:20+00:00,"Strongly lensed Type Ia supernovae (LSNe Ia) are a promising probe to measure
the Hubble constant ($H_0$) directly. To use LSNe Ia for cosmography, a
time-delay measurement between the multiple images, a lens-mass model, and a
mass reconstruction along the line of sight are required. In this work, we
present the machine learning network LSTM-FCNN which is a combination of a Long
Short-Term Memory Network (LSTM) and a fully-connected neural network (FCNN).
The LSTM-FCNN is designed to measure time delays on a sample of LSNe Ia
spanning a broad range of properties, which we expect to find with the upcoming
Rubin Observatory Legacy Survey of Space and Time (LSST) and for which
follow-up observations are planned. With follow-up observations in $i$ band
(cadence of one to three days with a single-epoch $5\sigma$ depth of 24.5 mag),
we reach a bias-free delay measurement with a precision around 0.7 days over a
large sample of LSNe Ia. The LSTM-FCNN is far more general than previous
machine learning approaches such as the Random Forest (RF), where a RF has to
be trained for each observational pattern separately, and yet the LSTM-FCNN
outperforms the RF by a factor of roughly three. Therefore, the LSTM-FCNN is a
very promising approach to achieve robust time delays in LSNe Ia, which is
important for a precise and accurate constraint on $H_0$",0
McCatch: Scalable Microcluster Detection in Dimensional and Nondimensional Datasets,2403.08027v1,http://arxiv.org/abs/2403.08027v1,2024-03-12 18:55:23+00:00,"How could we have an outlier detector that works even with nondimensional
data, and ranks together both singleton microclusters ('one-off' outliers) and
nonsingleton microclusters by their anomaly scores? How to obtain scores that
are principled in one scalable and 'hands-off' manner? Microclusters of
outliers indicate coalition or repetition in fraud activities, etc.; their
identification is thus highly desirable. This paper presents McCatch: a new
algorithm that detects microclusters by leveraging our proposed 'Oracle' plot
(1NN Distance versus Group 1NN Distance). We study 31 real and synthetic
datasets with up to 1M data elements to show that McCatch is the only method
that answers both of the questions above; and, it outperforms 11 other methods,
especially when the data has nonsingleton microclusters or is nondimensional.
We also showcase McCatch's ability to detect meaningful microclusters in
graphs, fingerprints, logs of network connections, text data, and satellite
imagery. For example, it found a 30-elements microcluster of confirmed 'Denial
of Service' attacks in the network logs, taking only ~3 minutes for 222K data
elements on a stock desktop.",0
Chronos: Learning the Language of Time Series,2403.07815v1,http://arxiv.org/abs/2403.07815v1,2024-03-12 16:53:54+00:00,"We introduce Chronos, a simple yet effective framework for pretrained
probabilistic time series models. Chronos tokenizes time series values using
scaling and quantization into a fixed vocabulary and trains existing
transformer-based language model architectures on these tokenized time series
via the cross-entropy loss. We pretrained Chronos models based on the T5 family
(ranging from 20M to 710M parameters) on a large collection of publicly
available datasets, complemented by a synthetic dataset that we generated via
Gaussian processes to improve generalization. In a comprehensive benchmark
consisting of 42 datasets, and comprising both classical local models and deep
learning methods, we show that Chronos models: (a) significantly outperform
other methods on datasets that were part of the training corpus; and (b) have
comparable and occasionally superior zero-shot performance on new datasets,
relative to methods that were trained specifically on them. Our results
demonstrate that Chronos models can leverage time series data from diverse
domains to improve zero-shot accuracy on unseen forecasting tasks, positioning
pretrained models as a viable tool to greatly simplify forecasting pipelines.",0
A Stochastic GDA Method With Backtracking For Solving Nonconvex (Strongly) Concave Minimax Problems,2403.07806v1,http://arxiv.org/abs/2403.07806v1,2024-03-12 16:43:55+00:00,"We propose a stochastic GDA (gradient descent ascent) method with
backtracking (SGDA-B) to solve nonconvex-(strongly) concave (NCC) minimax
problems $\min_x \max_y \sum_{i=1}^N g_i(x_i)+f(x,y)-h(y)$, where $h$ and $g_i$
for $i = 1, \ldots, N$ are closed, convex functions, $f$ is $L$-smooth and
$\mu$-strongly concave in $y$ for some $\mu\geq 0$. We consider two scenarios:
(i) the deterministic setting where we assume one can compute $\nabla f$
exactly, and (ii) the stochastic setting where we have only access to $\nabla
f$ through an unbiased stochastic oracle with a finite variance. While most of
the existing methods assume knowledge of the Lipschitz constant $L$, SGDA-B is
agnostic to $L$. Moreover, SGDA-B can support random block-coordinate updates.
In the deterministic setting, SGDA-B can compute an $\epsilon$-stationary point
within $\mathcal{O}(L\kappa^2/\epsilon^2)$ and $\mathcal{O}(L^3/\epsilon^4)$
gradient calls when $\mu>0$ and $\mu=0$, respectively, where $\kappa=L/\mu$. In
the stochastic setting, for any $p \in (0, 1)$ and $\epsilon >0$, it can
compute an $\epsilon$-stationary point with high probability, which requires
$\mathcal{O}(L\kappa^3\epsilon^{-4}\log(1/p))$ and
$\tilde{\mathcal{O}}(L^4\epsilon^{-7}\log(1/p))$ stochastic oracle calls, with
probability at least $1-p$, when $\mu>0$ and $\mu=0$, respectively. To our
knowledge, SGDA-B is the first GDA-type method with backtracking to solve NCC
minimax problems and achieves the best complexity among the methods that are
agnostic to $L$. We also provide numerical results for SGDA-B on a
distributionally robust learning problem illustrating the potential performance
gains that can be achieved by SGDA-B.",0
Boosting keyword spotting through on-device learnable user speech characteristics,2403.07802v1,http://arxiv.org/abs/2403.07802v1,2024-03-12 16:41:31+00:00,"Keyword spotting systems for always-on TinyML-constrained applications
require on-site tuning to boost the accuracy of offline trained classifiers
when deployed in unseen inference conditions. Adapting to the speech
peculiarities of target users requires many in-domain samples, often
unavailable in real-world scenarios. Furthermore, current on-device learning
techniques rely on computationally intensive and memory-hungry backbone update
schemes, unfit for always-on, battery-powered devices. In this work, we propose
a novel on-device learning architecture, composed of a pretrained backbone and
a user-aware embedding learning the user's speech characteristics. The
so-generated features are fused and used to classify the input utterance. For
domain shifts generated by unseen speakers, we measure error rate reductions of
up to 19% from 30.1% to 24.3% based on the 35-class problem of the Google
Speech Commands dataset, through the inexpensive update of the user
projections. We moreover demonstrate the few-shot learning capabilities of our
proposed architecture in sample- and class-scarce learning conditions. With
23.7 kparameters and 1 MFLOP per epoch required for on-device training, our
system is feasible for TinyML applications aimed at battery-powered
microcontrollers.",0
BraSyn 2023 challenge: Missing MRI synthesis and the effect of different learning objectives,2403.07800v1,http://arxiv.org/abs/2403.07800v1,2024-03-12 16:36:27+00:00,"This work is addressing the Brain Magnetic Resonance Image Synthesis for
Tumor Segmentation (BraSyn) challenge which was hosted as part of the Brain
Tumor Segmentation challenge (BraTS) 2023. In this challenge researchers are
invited to work on synthesizing a missing magnetic resonance image sequence
given other available sequences to facilitate tumor segmentation pipelines
trained on complete sets of image sequences. This problem can be addressed
using deep learning in the framework of paired images-to-image translation. In
this work, we proposed to investigate the effectiveness of a commonly-used deep
learning framework such as Pix2Pix trained under supervision of different
image-quality loss functions. Our results indicate that using different loss
functions significantly affects the synthesis quality. We systematically study
the impact of different loss functions in the multi-sequence MR image synthesis
setting of the BraSyn challenge. Furthermore, we show how image synthesis
performance can be optimized by beneficially combining different learning
objectives.",0
Joint Selection: Adaptively Incorporating Public Information for Private Synthetic Data,2403.07797v1,http://arxiv.org/abs/2403.07797v1,2024-03-12 16:34:07+00:00,"Mechanisms for generating differentially private synthetic data based on
marginals and graphical models have been successful in a wide range of
settings. However, one limitation of these methods is their inability to
incorporate public data. Initializing a data generating model by pre-training
on public data has shown to improve the quality of synthetic data, but this
technique is not applicable when model structure is not determined a priori. We
develop the mechanism jam-pgm, which expands the adaptive measurements
framework to jointly select between measuring public data and private data.
This technique allows for public data to be included in a graphical-model-based
mechanism. We show that jam-pgm is able to outperform both publicly assisted
and non publicly assisted synthetic data generation mechanisms even when the
public data distribution is biased.",0
Second gadolinium loading to Super-Kamiokande,2403.07796v1,http://arxiv.org/abs/2403.07796v1,2024-03-12 16:34:03+00:00,"The first loading of gadolinium (Gd) into Super-Kamiokande in 2020 was
successful, and the neutron capture efficiency on Gd reached 50\%. To further
increase the Gd neutron capture efficiency to 75\%, 26.1 tons of $\rm Gd_2(\rm
SO_4)_3\cdot \rm 8H_2O$ was additionally loaded into Super-Kamiokande (SK) from
May 31 to July 4, 2022. As the amount of loaded $\rm Gd_2(\rm SO_4)_3\cdot \rm
8H_2O$ was doubled compared to the first loading, the capacity of the powder
dissolving system was doubled. We also developed new batches of gadolinium
sulfate with even further reduced radioactive impurities. In addition, a more
efficient screening method was devised and implemented to evaluate these new
batches of $\rm Gd_2(\rm SO_4)_3\cdot \rm 8H_2O$. Following the second loading,
the Gd concentration in SK was measured to be $333.5\pm2.5$ ppm via an Atomic
Absorption Spectrometer (AAS). From the mean neutron capture time constant of
neutrons from an Am/Be calibration source, the Gd concentration was
independently measured to be 332.7 $\pm$ 6.8(sys.) $\pm$ 1.1(stat.) ppm,
consistent with the AAS result. Furthermore, during the loading the Gd
concentration was monitored continually using the capture time constant of each
spallation neutron produced by cosmic-ray muons,and the final neutron capture
efficiency was shown to become 1.5 times higher than that of the first loaded
phase, as expected.",0
Fine-tuning Neural Network Quantum States,2403.07795v1,http://arxiv.org/abs/2403.07795v1,2024-03-12 16:33:32+00:00,"Recent progress in the design and optimization of Neural Network Quantum
States (NNQS) have made them an effective method to investigate ground-state
properties of quantum many-body systems. In contrast to the standard approach
of training a separate NNQS from scratch at every point of the phase diagram,
we demonstrate that the optimization at a highly expressive point of the phase
diagram (i.e., close to a phase transition) yields interpretable features that
can be reused to accurately describe a wide region across the transition. We
demonstrate the feasibility of our approach on different systems in one and two
dimensions by initially pretraining a NNQS at a given point of the phase
diagram, followed by fine-tuning only the output layer for all other points.
Notably, the computational cost of the fine-tuning step is very low compared to
the pretraining stage. We argue that the reduced cost of this paradigm has
significant potential to advance the exploration of condensed matter systems
using NNQS, mirroring the success of fine-tuning in machine learning and
natural language processing.",0
DexCap: Scalable and Portable Mocap Data Collection System for Dexterous Manipulation,2403.07788v1,http://arxiv.org/abs/2403.07788v1,2024-03-12 16:23:49+00:00,"Imitation learning from human hand motion data presents a promising avenue
for imbuing robots with human-like dexterity in real-world manipulation tasks.
Despite this potential, substantial challenges persist, particularly with the
portability of existing hand motion capture (mocap) systems and the difficulty
of translating mocap data into effective control policies. To tackle these
issues, we introduce DexCap, a portable hand motion capture system, alongside
DexIL, a novel imitation algorithm for training dexterous robot skills directly
from human hand mocap data. DexCap offers precise, occlusion-resistant tracking
of wrist and finger motions based on SLAM and electromagnetic field together
with 3D observations of the environment. Utilizing this rich dataset, DexIL
employs inverse kinematics and point cloud-based imitation learning to
replicate human actions with robot hands. Beyond learning from human motion,
DexCap also offers an optional human-in-the-loop correction mechanism to refine
and further improve robot performance. Through extensive evaluation across six
dexterous manipulation tasks, our approach not only demonstrates superior
performance but also showcases the system's capability to effectively learn
from in-the-wild mocap data, paving the way for future data collection methods
for dexterous manipulation. More details can be found at
https://dex-cap.github.io",0
Generative deep learning-enabled ultra-large field-of-view lens-free imaging,2403.07786v1,http://arxiv.org/abs/2403.07786v1,2024-03-12 16:20:27+00:00,"Advancements in high-throughput biomedical applications necessitate
real-time, large field-of-view (FOV) imaging capabilities. Conventional
lens-free imaging (LFI) systems, while addressing the limitations of physical
lenses, have been constrained by dynamic, hard-to-model optical fields,
resulting in a limited one-shot FOV of approximately 20 $mm^2$. This
restriction has been a major bottleneck in applications like live-cell imaging
and automation of microfluidic systems for biomedical research. Here, we
present a deep-learning(DL)-based imaging framework -- GenLFI -- leveraging
generative artificial intelligence (AI) for holographic image reconstruction.
We demonstrate that GenLFI can achieve a real-time FOV over 550 $mm^2$,
surpassing the current LFI system by more than 20-fold, and even larger than
the world's largest confocal microscope by 1.76 times. The resolution is at the
sub-pixel level of 5.52 $\mu m$, without the need for a shifting light source.
The unsupervised learning-based reconstruction does not require optical field
modeling, making imaging dynamic 3D samples (e.g., droplet-based microfluidics
and 3D cell models) in complex optical fields possible. This GenLFI framework
unlocks the potential of LFI systems, offering a robust tool to tackle new
frontiers in high-throughput biomedical applications such as drug discovery.",0
FairRR: Pre-Processing for Group Fairness through Randomized Response,2403.07780v1,http://arxiv.org/abs/2403.07780v1,2024-03-12 16:08:47+00:00,"The increasing usage of machine learning models in consequential
decision-making processes has spurred research into the fairness of these
systems. While significant work has been done to study group fairness in the
in-processing and post-processing setting, there has been little that
theoretically connects these results to the pre-processing domain. This paper
proposes that achieving group fairness in downstream models can be formulated
as finding the optimal design matrix in which to modify a response variable in
a Randomized Response framework. We show that measures of group fairness can be
directly controlled for with optimal model utility, proposing a pre-processing
algorithm called FairRR that yields excellent downstream model utility and
fairness.",0
SemCity: Semantic Scene Generation with Triplane Diffusion,2403.07773v2,http://arxiv.org/abs/2403.07773v2,2024-03-12 15:59:08+00:00,"We present ""SemCity,"" a 3D diffusion model for semantic scene generation in
real-world outdoor environments. Most 3D diffusion models focus on generating a
single object, synthetic indoor scenes, or synthetic outdoor scenes, while the
generation of real-world outdoor scenes is rarely addressed. In this paper, we
concentrate on generating a real-outdoor scene through learning a diffusion
model on a real-world outdoor dataset. In contrast to synthetic data,
real-outdoor datasets often contain more empty spaces due to sensor
limitations, causing challenges in learning real-outdoor distributions. To
address this issue, we exploit a triplane representation as a proxy form of
scene distributions to be learned by our diffusion model. Furthermore, we
propose a triplane manipulation that integrates seamlessly with our triplane
diffusion model. The manipulation improves our diffusion model's applicability
in a variety of downstream tasks related to outdoor scene generation such as
scene inpainting, scene outpainting, and semantic scene completion refinements.
In experimental results, we demonstrate that our triplane diffusion model shows
meaningful generation results compared with existing work in a real-outdoor
dataset, SemanticKITTI. We also show our triplane manipulation facilitates
seamlessly adding, removing, or modifying objects within a scene. Further, it
also enables the expansion of scenes toward a city-level scale. Finally, we
evaluate our method on semantic scene completion refinements where our
diffusion model enhances predictions of semantic scene completion networks by
learning scene distribution. Our code is available at
https://github.com/zoomin-lee/SemCity.",0
Beyond the Labels: Unveiling Text-Dependency in Paralinguistic Speech Recognition Datasets,2403.07767v1,http://arxiv.org/abs/2403.07767v1,2024-03-12 15:54:32+00:00,"Paralinguistic traits like cognitive load and emotion are increasingly
recognized as pivotal areas in speech recognition research, often examined
through specialized datasets like CLSE and IEMOCAP. However, the integrity of
these datasets is seldom scrutinized for text-dependency. This paper critically
evaluates the prevalent assumption that machine learning models trained on such
datasets genuinely learn to identify paralinguistic traits, rather than merely
capturing lexical features. By examining the lexical overlap in these datasets
and testing the performance of machine learning models, we expose significant
text-dependency in trait-labeling. Our results suggest that some machine
learning models, especially large pre-trained models like HuBERT, might
inadvertently focus on lexical characteristics rather than the intended
paralinguistic features. The study serves as a call to action for the research
community to reevaluate the reliability of existing datasets and methodologies,
ensuring that machine learning models genuinely learn what they are designed to
recognize.",0
Emerging Technologies for 6G Non-Terrestrial-Networks: From Academia to Industrial Applications,2403.07763v1,http://arxiv.org/abs/2403.07763v1,2024-03-12 15:51:38+00:00,"Terrestrial networks form the fundamental infrastructure of modern
communication systems, serving more than 4 billion users globally. However,
terrestrial networks are facing a wide range of challenges, from coverage and
reliability to interference and congestion. As the demands of the 6G era are
expected to be much higher, it is crucial to address these challenges to ensure
a robust and efficient communication infrastructure for the future. To address
these problems, Non-terrestrial Network (NTN) has emerged to be a promising
solution. NTNs are communication networks that leverage airborne (e.g.,
unmanned aerial vehicles) and spaceborne vehicles (e.g., satellites) to
facilitate ultra-reliable communications and connectivity with high data rates
and low latency over expansive regions. This article aims to provide a
comprehensive survey on the utilization of network slicing, Artificial
Intelligence/Machine Learning (AI/ML), and Open Radio Access Network (ORAN) to
address diverse challenges of NTNs from the perspectives of both academia and
industry. Particularly, we first provide an in-depth tutorial on NTN and the
key enabling technologies including network slicing, AI/ML, and ORAN. Then, we
provide a comprehensive survey on how network slicing and AI/ML have been
leveraged to overcome the challenges that NTNs are facing. Moreover, we present
how ORAN can be utilized for NTNs. Finally, we highlight important challenges,
open issues, and future research directions of NTN in the 6G era.",0
Supporting Annotators with Affordances for Efficiently Labeling Conversational Data,2403.07762v1,http://arxiv.org/abs/2403.07762v1,2024-03-12 15:51:10+00:00,"Without well-labeled ground truth data, machine learning-based systems would
not be as ubiquitous as they are today, but these systems rely on substantial
amounts of correctly labeled data. Unfortunately, crowdsourced labeling is time
consuming and expensive. To address the concerns of effort and tedium, we
designed CAL, a novel interface to aid in data labeling. We made several key
design decisions for CAL, which include preventing inapt labels from being
selected, guiding users in selecting an appropriate label when they need
assistance, incorporating labeling documentation into the interface, and
providing an efficient means to view previous labels. We implemented a
production-quality implementation of CAL and report a user-study evaluation
that compares CAL to a standard spreadsheet. Key findings of our study include
users using CAL reported lower cognitive load, did not increase task time,
users rated CAL to be easier to use, and users preferred CAL over the
spreadsheet.",0
Probabilistic Easy Variational Causal Effect,2403.07745v1,http://arxiv.org/abs/2403.07745v1,2024-03-12 15:28:21+00:00,"Let $X$ and $Z$ be random vectors, and $Y=g(X,Z)$. In this paper, on the one
hand, for the case that $X$ and $Z$ are continuous, by using the ideas from the
total variation and the flux of $g$, we develop a point of view in causal
inference capable of dealing with a broad domain of causal problems. Indeed, we
focus on a function, called Probabilistic Easy Variational Causal Effect
(PEACE), which can measure the direct causal effect of $X$ on $Y$ with respect
to continuously and interventionally changing the values of $X$ while keeping
the value of $Z$ constant. PEACE is a function of $d\ge 0$, which is a degree
managing the strengths of probability density values $f(x|z)$. On the other
hand, we generalize the above idea for the discrete case and show its
compatibility with the continuous case. Further, we investigate some properties
of PEACE using measure theoretical concepts. Furthermore, we provide some
identifiability criteria and several examples showing the generic capability of
PEACE. We note that PEACE can deal with the causal problems for which
micro-level or just macro-level changes in the value of the input variables are
important. Finally, PEACE is stable under small changes in $\partial
g_{in}/\partial x$ and the joint distribution of $X$ and $Z$, where $g_{in}$ is
obtained from $g$ by removing all functional relationships defining $X$ and
$Z$.",0
Equipping Computational Pathology Systems with Artifact Processing Pipelines: A Showcase for Computation and Performance Trade-offs,2403.07743v2,http://arxiv.org/abs/2403.07743v2,2024-03-12 15:22:05+00:00,"Histopathology is a gold standard for cancer diagnosis under a microscopic
examination. However, histological tissue processing procedures result in
artifacts, which are ultimately transferred to the digitized version of glass
slides, known as whole slide images (WSIs). Artifacts are diagnostically
irrelevant areas and may result in wrong deep learning (DL) algorithms
predictions. Therefore, detecting and excluding artifacts in the computational
pathology (CPATH) system is essential for reliable automated diagnosis. In this
paper, we propose a mixture of experts (MoE) scheme for detecting five notable
artifacts, including damaged tissue, blur, folded tissue, air bubbles, and
histologically irrelevant blood from WSIs. First, we train independent binary
DL models as experts to capture particular artifact morphology. Then, we
ensemble their predictions using a fusion mechanism. We apply probabilistic
thresholding over the final probability distribution to improve the sensitivity
of the MoE. We developed DL pipelines using two MoEs and two multiclass models
of state-of-the-art deep convolutional neural networks (DCNNs) and vision
transformers (ViTs). DCNNs-based MoE and ViTs-based MoE schemes outperformed
simpler multiclass models and were tested on datasets from different hospitals
and cancer types, where MoE using DCNNs yielded the best results. The proposed
MoE yields 86.15% F1 and 97.93% sensitivity scores on unseen data, retaining
less computational cost for inference than MoE using ViTs. This best
performance of MoEs comes with relatively higher computational trade-offs than
multiclass models. The proposed artifact detection pipeline will not only
ensure reliable CPATH predictions but may also provide quality control.",0
Uncertainty Quantification with Deep Ensembles for 6D Object Pose Estimation,2403.07741v1,http://arxiv.org/abs/2403.07741v1,2024-03-12 15:19:25+00:00,"The estimation of 6D object poses is a fundamental task in many computer
vision applications. Particularly, in high risk scenarios such as human-robot
interaction, industrial inspection, and automation, reliable pose estimates are
crucial. In the last years, increasingly accurate and robust
deep-learning-based approaches for 6D object pose estimation have been
proposed. Many top-performing methods are not end-to-end trainable but consist
of multiple stages. In the context of deep uncertainty quantification, deep
ensembles are considered as state of the art since they have been proven to
produce well-calibrated and robust uncertainty estimates. However, deep
ensembles can only be applied to methods that can be trained end-to-end. In
this work, we propose a method to quantify the uncertainty of multi-stage 6D
object pose estimation approaches with deep ensembles. For the implementation,
we choose SurfEmb as representative, since it is one of the top-performing 6D
object pose estimation approaches in the BOP Challenge 2022. We apply
established metrics and concepts for deep uncertainty quantification to
evaluate the results. Furthermore, we propose a novel uncertainty calibration
score for regression tasks to quantify the quality of the estimated
uncertainty.",0
The Minimax Rate of HSIC Estimation for Translation-Invariant Kernels,2403.07735v1,http://arxiv.org/abs/2403.07735v1,2024-03-12 15:13:21+00:00,"Kernel techniques are among the most influential approaches in data science
and statistics. Under mild conditions, the reproducing kernel Hilbert space
associated to a kernel is capable of encoding the independence of $M\ge 2$
random variables. Probably the most widespread independence measure relying on
kernels is the so-called Hilbert-Schmidt independence criterion (HSIC; also
referred to as distance covariance in the statistics literature). Despite
various existing HSIC estimators designed since its introduction close to two
decades ago, the fundamental question of the rate at which HSIC can be
estimated is still open. In this work, we prove that the minimax optimal rate
of HSIC estimation on $\mathbb R^d$ for Borel measures containing the Gaussians
with continuous bounded translation-invariant characteristic kernels is
$\mathcal O\!\left(n^{-1/2}\right)$. Specifically, our result implies the
optimality in the minimax sense of many of the most-frequently used estimators
(including the U-statistic, the V-statistic, and the Nystr\""om-based one) on
$\mathbb R^d$.",0
DSEG-LIME - Improving Image Explanation by Hierarchical Data-Driven Segmentation,2403.07733v1,http://arxiv.org/abs/2403.07733v1,2024-03-12 15:13:12+00:00,"Explainable Artificial Intelligence is critical in unraveling decision-making
processes in complex machine learning models. LIME (Local Interpretable
Model-agnostic Explanations) is a well-known XAI framework for image analysis.
It utilizes image segmentation to create features to identify relevant areas
for classification. Consequently, poor segmentation can compromise the
consistency of the explanation and undermine the importance of the segments,
affecting the overall interpretability. Addressing these challenges, we
introduce DSEG-LIME (Data-Driven Segmentation LIME), featuring: i) a
data-driven segmentation for human-recognized feature generation, and ii) a
hierarchical segmentation procedure through composition. We benchmark DSEG-LIME
on pre-trained models with images from the ImageNet dataset - scenarios without
domain-specific knowledge. The analysis includes a quantitative evaluation
using established XAI metrics, complemented by a qualitative assessment through
a user study. Our findings demonstrate that DSEG outperforms in most of the XAI
metrics and enhances the alignment of explanations with human-recognized
concepts, significantly improving interpretability. The code is available
under: https://github. com/patrick-knab/DSEG-LIME",0
Performance Analysis of Matrix Multiplication for Deep Learning on the Edge,2403.07731v1,http://arxiv.org/abs/2403.07731v1,2024-03-12 15:11:47+00:00,"The devices designed for the Internet-of-Things encompass a large variety of
distinct processor architectures, forming a highly heterogeneous zoo. In order
to tackle this, we employ a simulator to estimate the performance of the
matrix-matrix multiplication (GEMM) kernel on processors designed to operate at
the edge. Our simulator adheres to the modern implementations of GEMM,
advocated by GotoBLAS2, BLIS, OpenBLAS, etc., to carefully account for the
amount of data transfers across the memory hierarchy of different algorithmic
variants of the kernel. %Armed with this tool, A small collection of
experiments provide the necessary data to calibrate the simulator and deliver
highly accurate estimations of the execution time for a given processor
architecture.",0
CAS: A General Algorithm for Online Selective Conformal Prediction with FCR Control,2403.07728v1,http://arxiv.org/abs/2403.07728v1,2024-03-12 15:07:20+00:00,"We study the problem of post-selection predictive inference in an online
fashion. To avoid devoting resources to unimportant units, a preliminary
selection of the current individual before reporting its prediction interval is
common and meaningful in online predictive tasks. Since the online selection
causes a temporal multiplicity in the selected prediction intervals, it is
important to control the real-time false coverage-statement rate (FCR) to
measure the averaged miscoverage error. We develop a general framework named
CAS (Calibration after Adaptive Selection) that can wrap around any prediction
model and online selection rule to output post-selection prediction intervals.
If the current individual is selected, we first perform an adaptive selection
on historical data to construct a calibration set, then output a conformal
prediction interval for the unobserved label. We provide tractable
constructions for the calibration set for popular online selection rules. We
proved that CAS can achieve an exact selection-conditional coverage guarantee
in the finite-sample and distribution-free regimes. For the decision-driven
selection rule, including most online multiple-testing procedures, CAS can
exactly control the real-time FCR below the target level without any
distributional assumptions. For the online selection with symmetric thresholds,
we establish the error bound for the control gap of FCR under mild
distributional assumptions. To account for the distribution shift in online
data, we also embed CAS into some recent dynamic conformal prediction methods
and examine the long-run FCR control. Numerical results on both synthetic and
real data corroborate that CAS can effectively control FCR around the target
level and yield more narrowed prediction intervals over existing baselines
across various settings.",0
Balancing Fairness and Accuracy in Data-Restricted Binary Classification,2403.07724v1,http://arxiv.org/abs/2403.07724v1,2024-03-12 15:01:27+00:00,"Applications that deal with sensitive information may have restrictions
placed on the data available to a machine learning (ML) classifier. For
example, in some applications, a classifier may not have direct access to
sensitive attributes, affecting its ability to produce accurate and fair
decisions. This paper proposes a framework that models the trade-off between
accuracy and fairness under four practical scenarios that dictate the type of
data available for analysis. Prior works examine this trade-off by analyzing
the outputs of a scoring function that has been trained to implicitly learn the
underlying distribution of the feature vector, class label, and sensitive
attribute of a dataset. In contrast, our framework directly analyzes the
behavior of the optimal Bayesian classifier on this underlying distribution by
constructing a discrete approximation it from the dataset itself. This approach
enables us to formulate multiple convex optimization problems, which allow us
to answer the question: How is the accuracy of a Bayesian classifier affected
in different data restricting scenarios when constrained to be fair? Analysis
is performed on a set of fairness definitions that include group and individual
fairness. Experiments on three datasets demonstrate the utility of the proposed
framework as a tool for quantifying the trade-offs among different fairness
notions and their distributional dependencies.",0
On the Last-Iterate Convergence of Shuffling Gradient Methods,2403.07723v1,http://arxiv.org/abs/2403.07723v1,2024-03-12 15:01:17+00:00,"Shuffling gradient methods, which are also known as stochastic gradient
descent (SGD) without replacement, are widely implemented in practice,
particularly including three popular algorithms: Random Reshuffle (RR), Shuffle
Once (SO), and Incremental Gradient (IG). Compared to the empirical success,
the theoretical guarantee of shuffling gradient methods was not
well-understanding for a long time. Until recently, the convergence rates had
just been established for the average iterate for convex functions and the last
iterate for strongly convex problems (using squared distance as the metric).
However, when using the function value gap as the convergence criterion,
existing theories cannot interpret the good performance of the last iterate in
different settings (e.g., constrained optimization). To bridge this gap between
practice and theory, we prove last-iterate convergence rates for shuffling
gradient methods with respect to the objective value even without strong
convexity. Our new results either (nearly) match the existing last-iterate
lower bounds or are as fast as the previous best upper bounds for the average
iterate.",0
Visual Decoding and Reconstruction via EEG Embeddings with Guided Diffusion,2403.07721v3,http://arxiv.org/abs/2403.07721v3,2024-03-12 14:58:57+00:00,"How to decode human vision through neural signals has attracted a
long-standing interest in neuroscience and machine learning. Modern contrastive
learning and generative models improved the performance of fMRI-based visual
decoding and reconstruction. However, the high cost and low temporal resolution
of fMRI limit their applications in brain-computer interfaces (BCIs), prompting
a high need for EEG-based visual reconstruction. In this study, we present an
EEG-based visual reconstruction framework. It consists of a plug-and-play EEG
encoder called the Adaptive Thinking Mapper (ATM), which is aligned with image
embeddings, and a two-stage EEG guidance image generator that first transforms
EEG features into image priors and then reconstructs the visual stimuli with a
pre-trained image generator. Our approach allows EEG embeddings to achieve
superior performance in image classification and retrieval tasks. Our two-stage
image generation strategy vividly reconstructs images seen by humans.
Furthermore, we analyzed the impact of signals from different time windows and
brain regions on decoding and reconstruction. The versatility of our framework
is demonstrated in the magnetoencephalogram (MEG) data modality. We report that
EEG-based visual decoding achieves SOTA performance, highlighting the
portability, low cost, and high temporal resolution of EEG, enabling a wide
range of BCI applications. The code of ATM is available at
https://github.com/dongyangli-del/EEG_Image_decode.",0
Dynamic Graph Representation with Knowledge-aware Attention for Histopathology Whole Slide Image Analysis,2403.07719v1,http://arxiv.org/abs/2403.07719v1,2024-03-12 14:58:51+00:00,"Histopathological whole slide images (WSIs) classification has become a
foundation task in medical microscopic imaging processing. Prevailing
approaches involve learning WSIs as instance-bag representations, emphasizing
significant instances but struggling to capture the interactions between
instances. Additionally, conventional graph representation methods utilize
explicit spatial positions to construct topological structures but restrict the
flexible interaction capabilities between instances at arbitrary locations,
particularly when spatially distant. In response, we propose a novel dynamic
graph representation algorithm that conceptualizes WSIs as a form of the
knowledge graph structure. Specifically, we dynamically construct neighbors and
directed edge embeddings based on the head and tail relationships between
instances. Then, we devise a knowledge-aware attention mechanism that can
update the head node features by learning the joint attention score of each
neighbor and edge. Finally, we obtain a graph-level embedding through the
global pooling process of the updated head, serving as an implicit
representation for the WSI classification. Our end-to-end graph representation
learning approach has outperformed the state-of-the-art WSI analysis methods on
three TCGA benchmark datasets and in-house test sets. Our code is available at
https://github.com/WonderLandxD/WiKG.",0
WorkArena: How Capable Are Web Agents at Solving Common Knowledge Work Tasks?,2403.07718v1,http://arxiv.org/abs/2403.07718v1,2024-03-12 14:58:45+00:00,"We study the use of large language model-based agents for interacting with
software via web browsers. Unlike prior work, we focus on measuring the agents'
ability to perform tasks that span the typical daily work of knowledge workers
utilizing enterprise software systems. To this end, we propose WorkArena, a
remote-hosted benchmark of 29 tasks based on the widely-used ServiceNow
platform. We also introduce BrowserGym, an environment for the design and
evaluation of such agents, offering a rich set of actions as well as multimodal
observations. Our empirical evaluation reveals that while current agents show
promise on WorkArena, there remains a considerable gap towards achieving full
task automation. Notably, our analysis uncovers a significant performance
disparity between open and closed-source LLMs, highlighting a critical area for
future exploration and development in the field.",0
Intra-video Positive Pairs in Self-Supervised Learning for Ultrasound,2403.07715v1,http://arxiv.org/abs/2403.07715v1,2024-03-12 14:57:57+00:00,"Self-supervised learning (SSL) is one strategy for addressing the paucity of
labelled data in medical imaging by learning representations from unlabelled
images. Contrastive and non-contrastive SSL methods produce learned
representations that are similar for pairs of related images. Such pairs are
commonly constructed by randomly distorting the same image twice. The
videographic nature of ultrasound offers flexibility for defining the
similarity relationship between pairs of images. In this study, we investigated
the effect of utilizing proximal, distinct images from the same B-mode
ultrasound video as pairs for SSL. Additionally, we introduced a sample
weighting scheme that increases the weight of closer image pairs and
demonstrated how it can be integrated into SSL objectives. Named Intra-Video
Positive Pairs (IVPP), the method surpassed previous ultrasound-specific
contrastive learning methods' average test accuracy on COVID-19 classification
with the POCUS dataset by $\ge 1.3\%$. Detailed investigations of IVPP's
hyperparameters revealed that some combinations of IVPP hyperparameters can
lead to improved or worsened performance, depending on the downstream task.
Guidelines for practitioners were synthesized based on the results, such as the
merit of IVPP with task-specific hyperparameters, and the improved performance
of contrastive methods for ultrasound compared to non-contrastive counterparts.",0
StableToolBench: Towards Stable Large-Scale Benchmarking on Tool Learning of Large Language Models,2403.07714v2,http://arxiv.org/abs/2403.07714v2,2024-03-12 14:57:40+00:00,"Large Language Models (LLMs) have witnessed remarkable advancements in recent
years, prompting the exploration of tool learning, which integrates LLMs with
external tools to address diverse real-world challenges. Assessing the
capability of LLMs to utilise tools necessitates large-scale and stable
benchmarks. However, previous works relied on either hand-crafted online tools
with limited scale, or large-scale real online APIs suffering from instability
of API status. To address this problem, we introduce StableToolBench, a
benchmark evolving from ToolBench, proposing a virtual API server and stable
evaluation system. The virtual API server contains a caching system and API
simulators which are complementary to alleviate the change in API status.
Meanwhile, the stable evaluation system designs solvable pass and win rates
using GPT-4 as the automatic evaluator to eliminate the randomness during
evaluation. Experimental results demonstrate the stability of StableToolBench,
and further discuss the effectiveness of API simulators, the caching system,
and the evaluator system.",0
pyvene: A Library for Understanding and Improving PyTorch Models via Interventions,2403.07809v1,http://arxiv.org/abs/2403.07809v1,2024-03-12 16:46:54+00:00,"Interventions on model-internal states are fundamental operations in many
areas of AI, including model editing, steering, robustness, and
interpretability. To facilitate such research, we introduce $\textbf{pyvene}$,
an open-source Python library that supports customizable interventions on a
range of different PyTorch modules. $\textbf{pyvene}$ supports complex
intervention schemes with an intuitive configuration format, and its
interventions can be static or include trainable parameters. We show how
$\textbf{pyvene}$ provides a unified and extensible framework for performing
interventions on neural models and sharing the intervened upon models with
others. We illustrate the power of the library via interpretability analyses
using causal abstraction and knowledge localization. We publish our library
through Python Package Index (PyPI) and provide code, documentation, and
tutorials at https://github.com/stanfordnlp/pyvene.",0
Branch-Train-MiX: Mixing Expert LLMs into a Mixture-of-Experts LLM,2403.07816v1,http://arxiv.org/abs/2403.07816v1,2024-03-12 16:54:58+00:00,"We investigate efficient methods for training Large Language Models (LLMs) to
possess capabilities in multiple specialized domains, such as coding, math
reasoning and world knowledge. Our method, named Branch-Train-MiX (BTX), starts
from a seed model, which is branched to train experts in embarrassingly
parallel fashion with high throughput and reduced communication cost. After
individual experts are asynchronously trained, BTX brings together their
feedforward parameters as experts in Mixture-of-Expert (MoE) layers and
averages the remaining parameters, followed by an MoE-finetuning stage to learn
token-level routing. BTX generalizes two special cases, the Branch-Train-Merge
method, which does not have the MoE finetuning stage to learn routing, and
sparse upcycling, which omits the stage of training experts asynchronously.
Compared to alternative approaches, BTX achieves the best accuracy-efficiency
tradeoff.",0
xMLP: Revolutionizing Private Inference with Exclusive Square Activation,2403.08024v1,http://arxiv.org/abs/2403.08024v1,2024-03-12 18:46:56+00:00,"Private Inference (PI) enables deep neural networks (DNNs) to work on private
data without leaking sensitive information by exploiting cryptographic
primitives such as multi-party computation (MPC) and homomorphic encryption
(HE). However, the use of non-linear activations such as ReLU in DNNs can lead
to impractically high PI latency in existing PI systems, as ReLU requires the
use of costly MPC computations, such as Garbled Circuits. Since square
activations can be processed by Beaver's triples hundreds of times faster
compared to ReLU, they are more friendly to PI tasks, but using them leads to a
notable drop in model accuracy. This paper starts by exploring the reason for
such an accuracy drop after using square activations, and concludes that this
is due to an ""information compounding"" effect. Leveraging this insight, we
propose xMLP, a novel DNN architecture that uses square activations exclusively
while maintaining parity in both accuracy and efficiency with ReLU-based DNNs.
Our experiments on CIFAR-100 and ImageNet show that xMLP models consistently
achieve better performance than ResNet models with fewer activation layers and
parameters while maintaining consistent performance with its ReLU-based
variants. Remarkably, when compared to state-of-the-art PI Models, xMLP
demonstrates superior performance, achieving a 0.58% increase in accuracy with
7x faster PI speed. Moreover, it delivers a significant accuracy improvement of
4.96% while maintaining the same PI latency. When offloading PI to the GPU,
xMLP is up to 700x faster than the previous state-of-the-art PI model with
comparable accuracy.",0
Label Dropout: Improved Deep Learning Echocardiography Segmentation Using Multiple Datasets With Domain Shift and Partial Labelling,2403.07818v1,http://arxiv.org/abs/2403.07818v1,2024-03-12 16:57:56+00:00,"Echocardiography (echo) is the first imaging modality used when assessing
cardiac function. The measurement of functional biomarkers from echo relies
upon the segmentation of cardiac structures and deep learning models have been
proposed to automate the segmentation process. However, in order to translate
these tools to widespread clinical use it is important that the segmentation
models are robust to a wide variety of images (e.g. acquired from different
scanners, by operators with different levels of expertise etc.). To achieve
this level of robustness it is necessary that the models are trained with
multiple diverse datasets. A significant challenge faced when training with
multiple diverse datasets is the variation in label presence, i.e. the combined
data are often partially-labelled. Adaptations of the cross entropy loss
function have been proposed to deal with partially labelled data. In this paper
we show that training naively with such a loss function and multiple diverse
datasets can lead to a form of shortcut learning, where the model associates
label presence with domain characteristics, leading to a drop in performance.
To address this problem, we propose a novel label dropout scheme to break the
link between domain characteristics and the presence or absence of labels. We
demonstrate that label dropout improves echo segmentation Dice score by 62% and
25% on two cardiac structures when training using multiple diverse partially
labelled datasets.",0
MRC-Net: 6-DoF Pose Estimation with MultiScale Residual Correlation,2403.08019v1,http://arxiv.org/abs/2403.08019v1,2024-03-12 18:36:59+00:00,"We propose a single-shot approach to determining 6-DoF pose of an object with
available 3D computer-aided design (CAD) model from a single RGB image. Our
method, dubbed MRC-Net, comprises two stages. The first performs pose
classification and renders the 3D object in the classified pose. The second
stage performs regression to predict fine-grained residual pose within class.
Connecting the two stages is a novel multi-scale residual correlation (MRC)
layer that captures high-and-low level correspondences between the input image
and rendering from first stage. MRC-Net employs a Siamese network with shared
weights between both stages to learn embeddings for input and rendered images.
To mitigate ambiguity when predicting discrete pose class labels on symmetric
objects, we use soft probabilistic labels to define pose class in the first
stage. We demonstrate state-of-the-art accuracy, outperforming all competing
RGB-based methods on four challenging BOP benchmark datasets: T-LESS, LM-O,
YCB-V, and ITODD. Our method is non-iterative and requires no complex
post-processing.",0
Learning Data Association for Multi-Object Tracking using Only Coordinates,2403.08018v1,http://arxiv.org/abs/2403.08018v1,2024-03-12 18:36:18+00:00,"We propose a novel Transformer-based module to address the data association
problem for multi-object tracking. From detections obtained by a pretrained
detector, this module uses only coordinates from bounding boxes to estimate an
affinity score between pairs of tracks extracted from two distinct temporal
windows. This module, named TWiX, is trained on sets of tracks with the
objective of discriminating pairs of tracks coming from the same object from
those which are not. Our module does not use the intersection over union
measure, nor does it requires any motion priors or any camera motion
compensation technique. By inserting TWiX within an online cascade matching
pipeline, our tracker C-TWiX achieves state-of-the-art performance on the
DanceTrack and KITTIMOT datasets, and gets competitive results on the MOT17
dataset. The code will be made available upon publication.",0
Red Teaming Models for Hyperspectral Image Analysis Using Explainable AI,2403.08017v1,http://arxiv.org/abs/2403.08017v1,2024-03-12 18:28:32+00:00,"Remote sensing (RS) applications in the space domain demand machine learning
(ML) models that are reliable, robust, and quality-assured, making red teaming
a vital approach for identifying and exposing potential flaws and biases. Since
both fields advance independently, there is a notable gap in integrating red
teaming strategies into RS. This paper introduces a methodology for examining
ML models operating on hyperspectral images within the HYPERVIEW challenge,
focusing on soil parameters' estimation. We use post-hoc explanation methods
from the Explainable AI (XAI) domain to critically assess the best performing
model that won the HYPERVIEW challenge and served as an inspiration for the
model deployed on board the INTUITION-1 hyperspectral mission. Our approach
effectively red teams the model by pinpointing and validating key shortcomings,
constructing a model that achieves comparable performance using just 1% of the
input features and a mere up to 5% performance loss. Additionally, we propose a
novel way of visualizing explanations that integrate domain-specific
information about hyperspectral bands (wavelengths) and data transformations to
better suit interpreting models for hyperspectral image analysis.",0
Aedes aegypti Egg Counting with Neural Networks for Object Detection,2403.08016v1,http://arxiv.org/abs/2403.08016v1,2024-03-12 18:28:13+00:00,"Aedes aegypti is still one of the main concerns when it comes to disease
vectors. Among the many ways to deal with it, there are important protocols
that make use of egg numbers in ovitraps to calculate indices, such as the
LIRAa and the Breteau Index, which can provide information on predictable
outbursts and epidemics. Also, there are many research lines that require egg
numbers, specially when mass production of mosquitoes is needed. Egg counting
is a laborious and error-prone task that can be automated via computer
vision-based techniques, specially deep learning-based counting with object
detection. In this work, we propose a new dataset comprising field and
laboratory eggs, along with test results of three neural networks applied to
the task: Faster R-CNN, Side-Aware Boundary Localization and FoveaBox.",0
Supervised Time Series Classification for Anomaly Detection in Subsea Engineering,2403.08013v1,http://arxiv.org/abs/2403.08013v1,2024-03-12 18:25:10+00:00,"Time series classification is of significant importance in monitoring
structural systems. In this work, we investigate the use of supervised machine
learning classification algorithms on simulated data based on a physical system
with two states: Intact and Broken. We provide a comprehensive discussion of
the preprocessing of temporal data, using measures of statistical dispersion
and dimension reduction techniques. We present an intuitive baseline method and
discuss its efficiency. We conclude with a comparison of the various methods
based on different performance metrics, showing the advantage of using machine
learning techniques as a tool in decision making.",0
Gujarati-English Code-Switching Speech Recognition using ensemble prediction of spoken language,2403.08011v1,http://arxiv.org/abs/2403.08011v1,2024-03-12 18:21:20+00:00,"An important and difficult task in code-switched speech recognition is to
recognize the language, as lots of words in two languages can sound similar,
especially in some accents. We focus on improving performance of end-to-end
Automatic Speech Recognition models by conditioning transformer layers on
language ID of words and character in the output in an per layer supervised
manner. To this end, we propose two methods of introducing language specific
parameters and explainability in the multi-head attention mechanism, and
implement a Temporal Loss that helps maintain continuity in input alignment.
Despite being unable to reduce WER significantly, our method shows promise in
predicting the correct language from just spoken data. We introduce
regularization in the language prediction by dropping LID in the sequence,
which helps align long repeated output sequences.",0
IndicSTR12: A Dataset for Indic Scene Text Recognition,2403.08007v1,http://arxiv.org/abs/2403.08007v1,2024-03-12 18:14:48+00:00,"The importance of Scene Text Recognition (STR) in today's increasingly
digital world cannot be overstated. Given the significance of STR, data
intensive deep learning approaches that auto-learn feature mappings have
primarily driven the development of STR solutions. Several benchmark datasets
and substantial work on deep learning models are available for Latin languages
to meet this need. On more complex, syntactically and semantically, Indian
languages spoken and read by 1.3 billion people, there is less work and
datasets available. This paper aims to address the Indian space's lack of a
comprehensive dataset by proposing the largest and most comprehensive real
dataset - IndicSTR12 - and benchmarking STR performance on 12 major Indian
languages. A few works have addressed the same issue, but to the best of our
knowledge, they focused on a small number of Indian languages. The size and
complexity of the proposed dataset are comparable to those of existing Latin
contemporaries, while its multilingualism will catalyse the development of
robust text detection and recognition models. It was created specifically for a
group of related languages with different scripts. The dataset contains over
27000 word-images gathered from various natural scenes, with over 1000
word-images for each language. Unlike previous datasets, the images cover a
broader range of realistic conditions, including blur, illumination changes,
occlusion, non-iconic texts, low resolution, perspective text etc. Along with
the new dataset, we provide a high-performing baseline on three models -
PARSeq, CRNN, and STARNet.",0
"Motifs, Phrases, and Beyond: The Modelling of Structure in Symbolic Music Generation",2403.07995v1,http://arxiv.org/abs/2403.07995v1,2024-03-12 18:03:08+00:00,"Modelling musical structure is vital yet challenging for artificial
intelligence systems that generate symbolic music compositions. This literature
review dissects the evolution of techniques for incorporating coherent
structure, from symbolic approaches to foundational and transformative deep
learning methods that harness the power of computation and data across a wide
variety of training paradigms. In the later stages, we review an emerging
technique which we refer to as ""sub-task decomposition"" that involves
decomposing music generation into separate high-level structural planning and
content creation stages. Such systems incorporate some form of musical
knowledge or neuro-symbolic methods by extracting melodic skeletons or
structural templates to guide the generation. Progress is evident in capturing
motifs and repetitions across all three eras reviewed, yet modelling the
nuanced development of themes across extended compositions in the style of
human composers remains difficult. We outline several key future directions to
realize the synergistic benefits of combining approaches from all eras
examined.",0
Majority-of-Three: The Simplest Optimal Learner?,2403.08831v1,http://arxiv.org/abs/2403.08831v1,2024-03-12 18:01:30+00:00,"Developing an optimal PAC learning algorithm in the realizable setting, where
empirical risk minimization (ERM) is suboptimal, was a major open problem in
learning theory for decades. The problem was finally resolved by Hanneke a few
years ago. Unfortunately, Hanneke's algorithm is quite complex as it returns
the majority vote of many ERM classifiers that are trained on carefully
selected subsets of the data. It is thus a natural goal to determine the
simplest algorithm that is optimal. In this work we study the arguably simplest
algorithm that could be optimal: returning the majority vote of three ERM
classifiers. We show that this algorithm achieves the optimal in-expectation
bound on its error which is provably unattainable by a single ERM classifier.
Furthermore, we prove a near-optimal high-probability bound on this algorithm's
error. We conjecture that a better analysis will prove that this algorithm is
in fact optimal in the high-probability regime.",0
Do Agents Dream of Electric Sheep?: Improving Generalization in Reinforcement Learning through Generative Learning,2403.07979v1,http://arxiv.org/abs/2403.07979v1,2024-03-12 18:00:02+00:00,"The Overfitted Brain hypothesis suggests dreams happen to allow
generalization in the human brain. Here, we ask if the same is true for
reinforcement learning agents as well. Given limited experience in a real
environment, we use imagination-based reinforcement learning to train a policy
on dream-like episodes, where non-imaginative, predicted trajectories are
modified through generative augmentations. Experiments on four ProcGen
environments show that, compared to classic imagination and offline training on
collected experience, our method can reach a higher level of generalization
when dealing with sparsely rewarded environments.",0
OPEN TEACH: A Versatile Teleoperation System for Robotic Manipulation,2403.07870v1,http://arxiv.org/abs/2403.07870v1,2024-03-12 17:58:38+00:00,"Open-sourced, user-friendly tools form the bedrock of scientific advancement
across disciplines. The widespread adoption of data-driven learning has led to
remarkable progress in multi-fingered dexterity, bimanual manipulation, and
applications ranging from logistics to home robotics. However, existing data
collection platforms are often proprietary, costly, or tailored to specific
robotic morphologies. We present OPEN TEACH, a new teleoperation system
leveraging VR headsets to immerse users in mixed reality for intuitive robot
control. Built on the affordable Meta Quest 3, which costs $500, OPEN TEACH
enables real-time control of various robots, including multi-fingered hands and
bimanual arms, through an easy-to-use app. Using natural hand gestures and
movements, users can manipulate robots at up to 90Hz with smooth visual
feedback and interface widgets offering closeup environment views. We
demonstrate the versatility of OPEN TEACH across 38 tasks on different robots.
A comprehensive user study indicates significant improvement in teleoperation
capability over the AnyTeleop framework. Further experiments exhibit that the
collected data is compatible with policy learning on 10 dexterous and
contact-rich manipulation tasks. Currently supporting Franka, xArm, Jaco, and
Allegro platforms, OPEN TEACH is fully open-sourced to promote broader
adoption. Videos are available at https://open-teach.github.io/.",0
LiveCodeBench: Holistic and Contamination Free Evaluation of Large Language Models for Code,2403.07974v1,http://arxiv.org/abs/2403.07974v1,2024-03-12 17:58:04+00:00,"Large Language Models (LLMs) applied to code-related applications have
emerged as a prominent field, attracting significant interest from both
academia and industry. However, as new and improved LLMs are developed,
existing evaluation benchmarks (e.g., HumanEval, MBPP) are no longer sufficient
for assessing their capabilities. In this work, we propose LiveCodeBench, a
comprehensive and contamination-free evaluation of LLMs for code, which
continuously collects new problems over time from contests across three
competition platforms, namely LeetCode, AtCoder, and CodeForces. Notably, our
benchmark also focuses on a broader range of code related capabilities, such as
self-repair, code execution, and test output prediction, beyond just code
generation. Currently, LiveCodeBench hosts four hundred high-quality coding
problems that were published between May 2023 and February 2024. We have
evaluated 9 base LLMs and 20 instruction-tuned LLMs on LiveCodeBench. We
present empirical findings on contamination, holistic performance comparisons,
potential overfitting in existing benchmarks as well as individual model
comparisons. We will release all prompts and model completions for further
community analysis, along with a general toolkit for adding new scenarios and
model",0
TeleMoMa: A Modular and Versatile Teleoperation System for Mobile Manipulation,2403.07869v1,http://arxiv.org/abs/2403.07869v1,2024-03-12 17:58:01+00:00,"A critical bottleneck limiting imitation learning in robotics is the lack of
data. This problem is more severe in mobile manipulation, where collecting
demonstrations is harder than in stationary manipulation due to the lack of
available and easy-to-use teleoperation interfaces. In this work, we
demonstrate TeleMoMa, a general and modular interface for whole-body
teleoperation of mobile manipulators. TeleMoMa unifies multiple human
interfaces including RGB and depth cameras, virtual reality controllers,
keyboard, joysticks, etc., and any combination thereof. In its more accessible
version, TeleMoMa works using simply vision (e.g., an RGB-D camera), lowering
the entry bar for humans to provide mobile manipulation demonstrations. We
demonstrate the versatility of TeleMoMa by teleoperating several existing
mobile manipulators - PAL Tiago++, Toyota HSR, and Fetch - in simulation and
the real world. We demonstrate the quality of the demonstrations collected with
TeleMoMa by training imitation learning policies for mobile manipulation tasks
involving synchronized whole-body motion. Finally, we also show that TeleMoMa's
teleoperation channel enables teleoperation on site, looking at the robot, or
remote, sending commands and observations through a computer network, and
perform user studies to evaluate how easy it is for novice users to learn to
collect demonstrations with different combinations of human interfaces enabled
by our system. We hope TeleMoMa becomes a helpful tool for the community
enabling researchers to collect whole-body mobile manipulation demonstrations.
For more information and video results,
https://robin-lab.cs.utexas.edu/telemoma-web.",0
Exploring Safety Generalization Challenges of Large Language Models via Code,2403.07865v2,http://arxiv.org/abs/2403.07865v2,2024-03-12 17:55:38+00:00,"The rapid advancement of Large Language Models (LLMs) has brought about
remarkable capabilities in natural language processing but also raised concerns
about their potential misuse. While strategies like supervised fine-tuning and
reinforcement learning from human feedback have enhanced their safety, these
methods primarily focus on natural languages, which may not generalize to other
domains. This paper introduces CodeAttack, a framework that transforms natural
language inputs into code inputs, presenting a novel environment for testing
the safety generalization of LLMs. Our comprehensive studies on
state-of-the-art LLMs including GPT-4, Claude-2, and Llama-2 series reveal a
common safety vulnerability of these models against code input: CodeAttack
consistently bypasses the safety guardrails of all models more than 80% of the
time. Furthermore, we find that a larger distribution gap between CodeAttack
and natural language leads to weaker safety generalization, such as encoding
natural language input with data structures or using less popular programming
languages. These findings highlight new safety risks in the code domain and the
need for more robust safety alignment algorithms to match the code capabilities
of LLMs.",0
Low coordinate degree algorithms I: Universality of computational thresholds for hypothesis testing,2403.07862v1,http://arxiv.org/abs/2403.07862v1,2024-03-12 17:52:35+00:00,"We study when low coordinate degree functions (LCDF) -- linear combinations
of functions depending on small subsets of entries of a vector -- can
hypothesis test between high-dimensional probability measures. These functions
are a generalization, proposed in Hopkins' 2018 thesis but seldom studied
since, of low degree polynomials (LDP), a class widely used in recent
literature as a proxy for all efficient algorithms for tasks in statistics and
optimization. Instead of the orthogonal polynomial decompositions used in LDP
calculations, our analysis of LCDF is based on the Efron-Stein or ANOVA
decomposition, making it much more broadly applicable. By way of illustration,
we prove channel universality for the success of LCDF in testing for the
presence of sufficiently ""dilute"" random signals through noisy channels: the
efficacy of LCDF depends on the channel only through the scalar Fisher
information for a class of channels including nearly arbitrary additive i.i.d.
noise and nearly arbitrary exponential families. As applications, we extend
lower bounds against LDP for spiked matrix and tensor models under additive
Gaussian noise to lower bounds against LCDF under general noisy channels. We
also give a simple and unified treatment of the effect of censoring models by
erasing observations at random and of quantizing models by taking the sign of
the observations. These results are the first computational lower bounds
against any large class of algorithms for all of these models when the channel
is not one of a few special cases, and thereby give the first substantial
evidence for the universality of several statistical-to-computational gaps.",0
Fairness Feedback Loops: Training on Synthetic Data Amplifies Bias,2403.07857v1,http://arxiv.org/abs/2403.07857v1,2024-03-12 17:48:08+00:00,"Model-induced distribution shifts (MIDS) occur as previous model outputs
pollute new model training sets over generations of models. This is known as
model collapse in the case of generative models, and performative prediction or
unfairness feedback loops for supervised models. When a model induces a
distribution shift, it also encodes its mistakes, biases, and unfairnesses into
the ground truth of its data ecosystem. We introduce a framework that allows us
to track multiple MIDS over many generations, finding that they can lead to
loss in performance, fairness, and minoritized group representation, even in
initially unbiased datasets. Despite these negative consequences, we identify
how models might be used for positive, intentional, interventions in their data
ecosystems, providing redress for historical discrimination through a framework
called algorithmic reparation (AR). We simulate AR interventions by curating
representative training batches for stochastic gradient descent to demonstrate
how AR can improve upon the unfairnesses of models and data ecosystems subject
to other MIDS. Our work takes an important step towards identifying,
mitigating, and taking accountability for the unfair feedback loops enabled by
the idea that ML systems are inherently neutral and objective.",0
Quantum Support Vector Machine for Prostate Cancer Detection: A Performance Analysis,2403.07856v1,http://arxiv.org/abs/2403.07856v1,2024-03-12 17:46:38+00:00,"This study addresses the urgent need for improved prostate cancer detection
methods by harnessing the power of advanced technological solutions. We
introduce the application of Quantum Support Vector Machine (QSVM) to this
critical healthcare challenge, showcasing an enhancement in diagnostic
performance over the classical Support Vector Machine (SVM) approach. Our study
not only outlines the remarkable improvements in diagnostic performance made by
QSVM over the classic SVM technique, but it delves into the advancements
brought about by the quantum feature map architecture, which has been carefully
identified and evaluated, ensuring it aligns seamlessly with the unique
characteristics of our prostate cancer dataset. This architecture succeded in
creating a distinct feature space, enabling the detection of complex,
non-linear patterns in the data. The findings reveal not only a comparable
accuracy with classical SVM ($92\%$) but also a $7.14\%$ increase in
sensitivity and a notably high F1-Score ($93.33\%$). This study's important
combination of quantum computing in medical diagnostics marks a pivotal step
forward in cancer detection, offering promising implications for the future of
healthcare technology.",0
Distilling the Knowledge in Data Pruning,2403.07854v1,http://arxiv.org/abs/2403.07854v1,2024-03-12 17:44:45+00:00,"With the increasing size of datasets used for training neural networks, data
pruning becomes an attractive field of research. However, most current data
pruning algorithms are limited in their ability to preserve accuracy compared
to models trained on the full data, especially in high pruning regimes. In this
paper we explore the application of data pruning while incorporating knowledge
distillation (KD) when training on a pruned subset. That is, rather than
relying solely on ground-truth labels, we also use the soft predictions from a
teacher network pre-trained on the complete data. By integrating KD into
training, we demonstrate significant improvement across datasets, pruning
methods, and on all pruning fractions. We first establish a theoretical
motivation for employing self-distillation to improve training on pruned data.
Then, we empirically make a compelling and highly practical observation: using
KD, simple random pruning is comparable or superior to sophisticated pruning
methods across all pruning regimes. On ImageNet for example, we achieve
superior accuracy despite training on a random subset of only 50% of the data.
Additionally, we demonstrate a crucial connection between the pruning factor
and the optimal knowledge distillation weight. This helps mitigate the impact
of samples with noisy labels and low-quality images retained by typical pruning
algorithms. Finally, we make an intriguing observation: when using lower
pruning fractions, larger teachers lead to accuracy degradation, while
surprisingly, employing teachers with a smaller capacity than the student's may
improve results. Our code will be made available.",0
12 mJ per Class On-Device Online Few-Shot Class-Incremental Learning,2403.07851v1,http://arxiv.org/abs/2403.07851v1,2024-03-12 17:43:20+00:00,"Few-Shot Class-Incremental Learning (FSCIL) enables machine learning systems
to expand their inference capabilities to new classes using only a few labeled
examples, without forgetting the previously learned classes. Classical
backpropagation-based learning and its variants are often unsuitable for
battery-powered, memory-constrained systems at the extreme edge. In this work,
we introduce Online Few-Shot Class-Incremental Learning (O-FSCIL), based on a
lightweight model consisting of a pretrained and metalearned feature extractor
and an expandable explicit memory storing the class prototypes. The
architecture is pretrained with a novel feature orthogonality regularization
and metalearned with a multi-margin loss. For learning a new class, our
approach extends the explicit memory with novel class prototypes, while the
remaining architecture is kept frozen. This allows learning previously unseen
classes based on only a few examples with one single pass (hence online).
O-FSCIL obtains an average accuracy of 68.62% on the FSCIL CIFAR100 benchmark,
achieving state-of-the-art results. Tailored for ultra-low-power platforms, we
implement O-FSCIL on the 60 mW GAP9 microcontroller, demonstrating online
learning capabilities within just 12 mJ per new class.",0
Iterative Graph Neural Network Enhancement via Frequent Subgraph Mining of Explanations,2403.07849v1,http://arxiv.org/abs/2403.07849v1,2024-03-12 17:41:27+00:00,"We formulate an XAI-based model improvement approach for Graph Neural
Networks (GNNs) for node classification, called Explanation Enhanced Graph
Learning (EEGL). The goal is to improve predictive performance of GNN using
explanations. EEGL is an iterative self-improving algorithm, which starts with
a learned ""vanilla"" GNN, and repeatedly uses frequent subgraph mining to find
relevant patterns in explanation subgraphs. These patterns are then filtered
further to obtain application-dependent features corresponding to the presence
of certain subgraphs in the node neighborhoods. Giving an application-dependent
algorithm for such a subgraph-based extension of the Weisfeiler-Leman (1-WL)
algorithm has previously been posed as an open problem. We present experimental
evidence, with synthetic and real-world data, which show that EEGL outperforms
related approaches in predictive performance and that it has a
node-distinguishing power beyond that of vanilla GNNs. We also analyze EEGL's
training dynamics.",0
Hyper-density functional theory of soft matter,2403.07845v1,http://arxiv.org/abs/2403.07845v1,2024-03-12 17:37:41+00:00,"We present a scheme for investigating arbitrary thermal observables in
spatially inhomogeneous many-body systems. Extending the equilibrium ensemble
yields any given observable as an explicit hyper-density functional. Associated
local fluctuation profiles follow from an exact hyper-Ornstein-Zernike
equation. Simulation-based supervised machine learning trains neural networks
that act as hyper-direct correlation functionals which facilitate efficient and
accurate predictions. We exemplify the approach for the cluster statistics of
hard rods and square well particles. The theory provides access to complex
order parameters, as is impossible in standard density functional theory.",0
A Machine learning and Empirical Bayesian Approach for Predictive Buying in B2B E-commerce,2403.07843v1,http://arxiv.org/abs/2403.07843v1,2024-03-12 17:32:52+00:00,"In the context of developing nations like India, traditional business to
business (B2B) commerce heavily relies on the establishment of robust
relationships, trust, and credit arrangements between buyers and sellers.
Consequently, ecommerce enterprises frequently. Established in 2016 with a
vision to revolutionize trade in India through technology, Udaan is the
countrys largest business to business ecommerce platform. Udaan operates across
diverse product categories, including lifestyle, electronics, home and employ
telecallers to cultivate buyer relationships, streamline order placement
procedures, and promote special promotions. The accurate anticipation of buyer
order placement behavior emerges as a pivotal factor for attaining sustainable
growth, heightening competitiveness, and optimizing the efficiency of these
telecallers. To address this challenge, we have employed an ensemble approach
comprising XGBoost and a modified version of Poisson Gamma model to predict
customer order patterns with precision. This paper provides an in-depth
exploration of the strategic fusion of machine learning and an empirical
Bayesian approach, bolstered by the judicious selection of pertinent features.
This innovative approach has yielded a remarkable 3 times increase in customer
order rates, show casing its potential for transformative impact in the
ecommerce industry.",0
Quantifying and Mitigating Privacy Risks for Tabular Generative Models,2403.07842v1,http://arxiv.org/abs/2403.07842v1,2024-03-12 17:27:49+00:00,"Synthetic data from generative models emerges as the privacy-preserving
data-sharing solution. Such a synthetic data set shall resemble the original
data without revealing identifiable private information. The backbone
technology of tabular synthesizers is rooted in image generative models,
ranging from Generative Adversarial Networks (GANs) to recent diffusion models.
Recent prior work sheds light on the utility-privacy tradeoff on tabular data,
revealing and quantifying privacy risks on synthetic data. We first conduct an
exhaustive empirical analysis, highlighting the utility-privacy tradeoff of
five state-of-the-art tabular synthesizers, against eight privacy attacks, with
a special focus on membership inference attacks. Motivated by the observation
of high data quality but also high privacy risk in tabular diffusion, we
propose DP-TLDM, Differentially Private Tabular Latent Diffusion Model, which
is composed of an autoencoder network to encode the tabular data and a latent
diffusion model to synthesize the latent tables. Following the emerging f-DP
framework, we apply DP-SGD to train the auto-encoder in combination with batch
clipping and use the separation value as the privacy metric to better capture
the privacy gain from DP algorithms. Our empirical evaluation demonstrates that
DP-TLDM is capable of achieving a meaningful theoretical privacy guarantee
while also significantly enhancing the utility of synthetic data. Specifically,
compared to other DP-protected tabular generative models, DP-TLDM improves the
synthetic quality by an average of 35% in data resemblance, 15% in the utility
for downstream tasks, and 50% in data discriminability, all while preserving a
comparable level of privacy risk.",0
MPCPA: Multi-Center Privacy Computing with Predictions Aggregation based on Denoising Diffusion Probabilistic Model,2403.07838v1,http://arxiv.org/abs/2403.07838v1,2024-03-12 17:21:46+00:00,"Privacy-preserving computing is crucial for multi-center machine learning in
many applications such as healthcare and finance. In this paper a Multi-center
Privacy Computing framework with Predictions Aggregation (MPCPA) based on
denoising diffusion probabilistic model (DDPM) is proposed, in which
conditional diffusion model training, DDPM data generation, a classifier, and
strategy of prediction aggregation are included. Compared to federated
learning, this framework necessitates fewer communications and leverages
high-quality generated data to support robust privacy computing. Experimental
validation across multiple datasets demonstrates that the proposed framework
outperforms classic federated learning and approaches the performance of
centralized learning with original data. Moreover, our approach demonstrates
robust security, effectively addressing challenges such as image memorization
and membership inference attacks. Our experiments underscore the efficacy of
the proposed framework in the realm of privacy computing, with the code set to
be released soon.",0
When Eye-Tracking Meets Machine Learning: A Systematic Review on Applications in Medical Image Analysis,2403.07834v1,http://arxiv.org/abs/2403.07834v1,2024-03-12 17:17:20+00:00,"Eye-gaze tracking research offers significant promise in enhancing various
healthcare-related tasks, above all in medical image analysis and
interpretation. Eye tracking, a technology that monitors and records the
movement of the eyes, provides valuable insights into human visual attention
patterns. This technology can transform how healthcare professionals and
medical specialists engage with and analyze diagnostic images, offering a more
insightful and efficient approach to medical diagnostics. Hence, extracting
meaningful features and insights from medical images by leveraging eye-gaze
data improves our understanding of how radiologists and other medical experts
monitor, interpret, and understand images for diagnostic purposes. Eye-tracking
data, with intricate human visual attention patterns embedded, provides a
bridge to integrating artificial intelligence (AI) development and human
cognition. This integration allows novel methods to incorporate domain
knowledge into machine learning (ML) and deep learning (DL) approaches to
enhance their alignment with human-like perception and decision-making.
Moreover, extensive collections of eye-tracking data have also enabled novel
ML/DL methods to analyze human visual patterns, paving the way to a better
understanding of human vision, attention, and cognition. This systematic review
investigates eye-gaze tracking applications and methodologies for enhancing
ML/DL algorithms for medical image analysis in depth.",0
The Missing Piece in Model Editing: A Deep Dive into the Hidden Damage Brought By Model Editing,2403.07825v1,http://arxiv.org/abs/2403.07825v1,2024-03-12 17:04:28+00:00,"Large Language Models have revolutionized numerous tasks with their
remarkable efficacy.However, the editing of these models, crucial for
rectifying outdated or erroneous information, often leads to a complex issue
known as the ripple effect in the hidden space. This effect, while difficult to
detect, can significantly impede the efficacy of model editing tasks and
deteriorate model performance.This paper addresses this scientific challenge by
proposing a novel evaluation methodology, Graphical Outlier Relation based
Assessment(GORA), which quantitatively evaluates the adaptations of the model
and the subsequent impact of editing. Furthermore, we introduce the Selective
Outlier Re-Editing Approach(SORA), a model editing method designed to mitigate
this ripple effect. Our comprehensive evaluations reveal that the ripple effect
in the hidden space is a significant issue in all current model editing
methods. However, our proposed methods, GORA and SORA, effectively identify and
alleviate this issue, respectively, contributing to the advancement of LLM
editing techniques.",0
Fusing Climate Data Products using a Spatially Varying Autoencoder,2403.07822v1,http://arxiv.org/abs/2403.07822v1,2024-03-12 17:03:07+00:00,"Autoencoders are powerful machine learning models used to compress
information from multiple data sources. However, autoencoders, like all
artificial neural networks, are often unidentifiable and uninterpretable. This
research focuses on creating an identifiable and interpretable autoencoder that
can be used to meld and combine climate data products. The proposed autoencoder
utilizes a Bayesian statistical framework, allowing for probabilistic
interpretations while also varying spatially to capture useful spatial patterns
across the various data products. Constraints are placed on the autoencoder as
it learns patterns in the data, creating an interpretable consensus that
includes the important features from each input. We demonstrate the utility of
the autoencoder by combining information from multiple precipitation products
in High Mountain Asia.",0
A Novel Feature Learning-based Bio-inspired Neural Network for Real-time Collision-free Rescue of Multi-Robot Systems,2403.08238v1,http://arxiv.org/abs/2403.08238v1,2024-03-13 04:43:10+00:00,"Natural disasters and urban accidents drive the demand for rescue robots to
provide safer, faster, and more efficient rescue trajectories. In this paper, a
feature learning-based bio-inspired neural network (FLBBINN) is proposed to
quickly generate a heuristic rescue path in complex and dynamic environments,
as traditional approaches usually cannot provide a satisfactory solution to
real-time responses to sudden environmental changes. The neurodynamic model is
incorporated into the feature learning method that can use environmental
information to improve path planning strategies. Task assignment and
collision-free rescue trajectory are generated through robot poses and the
dynamic landscape of neural activity. A dual-channel scale filter, a neural
activity channel, and a secondary distance fusion are employed to extract and
filter feature neurons. After completion of the feature learning process, a
neurodynamics-based feature matrix is established to quickly generate the new
heuristic rescue paths with parameter-driven topological adaptability. The
proposed FLBBINN aims to reduce the computational complexity of the neural
network-based approach and enable the feature learning method to achieve
real-time responses to environmental changes. Several simulations and
experiments have been conducted to evaluate the performance of the proposed
FLBBINN. The results show that the proposed FLBBINN would significantly improve
the speed, efficiency, and optimality for rescue operations.",0
Scattered Mixture-of-Experts Implementation,2403.08245v1,http://arxiv.org/abs/2403.08245v1,2024-03-13 05:00:23+00:00,"We present ScatterMoE, an implementation of Sparse Mixture-of-Experts (SMoE)
on GPUs. ScatterMoE builds upon existing implementations, and overcoming some
of the limitations to improve inference and training speed, and memory
footprint. This implementation achieves this by avoiding padding and making
excessive copies of the input. We introduce ParallelLinear, the main component
we use to build our implementation and the various kernels used to speed up the
operation. We benchmark our implementation against Megablocks, and show that it
enables a higher throughput and lower memory footprint. We also show how
ParallelLinear enables extension of the Mixture-of-Experts concept by
demonstrating with an implementation of Mixture of Attention.",0
Elastic shape analysis computations for clustering left atrial appendage geometries of atrial fibrillation patients,2403.08685v1,http://arxiv.org/abs/2403.08685v1,2024-03-13 16:42:05+00:00,"Morphological variations in the left atrial appendage (LAA) are associated
with different levels of ischemic stroke risk for patients with atrial
fibrillation (AF). Studying LAA morphology can elucidate mechanisms behind this
association and lead to the development of advanced stroke risk stratification
tools. However, current categorical descriptions of LAA morphologies are
qualitative and inconsistent across studies, which impedes advancements in our
understanding of stroke pathogenesis in AF. To mitigate these issues, we
introduce a quantitative pipeline that combines elastic shape analysis with
unsupervised learning for the categorization of LAA morphology in AF patients.
As part of our pipeline, we compute pairwise elastic distances between LAA
meshes from a cohort of 20 AF patients, and leverage these distances to cluster
our shape data. We demonstrate that our method clusters LAA morphologies based
on distinctive shape features, overcoming the innate inconsistencies of current
LAA categorization systems, and paving the way for improved stroke risk metrics
using objective LAA shape groups.",0
3D Spectrum Mapping and Reconstruction under Multi-Radiation Source Scenarios,2403.08513v1,http://arxiv.org/abs/2403.08513v1,2024-03-13 13:23:37+00:00,"Spectrum map construction, which is crucial in cognitive radio (CR) system,
visualizes the invisible space of the electromagnetic spectrum for
spectrum-resource management and allocation. Traditional reconstruction methods
are generally for two-dimensional (2D) spectrum map and driven by abundant
sampling data. In this paper, we propose a data-model-knowledge-driven
reconstruction scheme to construct the three-dimensional (3D) spectrum map
under multi-radiation source scenarios. We firstly design a maximum and minimum
path loss difference (MMPLD) clustering algorithm to detect the number of
radiation sources in a 3D space. Then, we develop a joint location-power
estimation method based on the heuristic population evolutionary optimization
algorithm. Considering the variation of electromagnetic environment, we
self-learn the path loss (PL) model based on the sampling data. Finally, the 3D
spectrum is reconstructed according to the self-learned PL model and the
extracted knowledge of radiation sources. Simulations show that the proposed 3D
spectrum map reconstruction scheme not only has splendid adaptability to the
environment, but also achieves high spectrum construction accuracy even when
the sampling rate is very low.",0
Consistent Prompting for Rehearsal-Free Continual Learning,2403.08568v2,http://arxiv.org/abs/2403.08568v2,2024-03-13 14:24:09+00:00,"Continual learning empowers models to adapt autonomously to the ever-changing
environment or data streams without forgetting old knowledge. Prompt-based
approaches are built on frozen pre-trained models to learn the task-specific
prompts and classifiers efficiently. Existing prompt-based methods are
inconsistent between training and testing, limiting their effectiveness. Two
types of inconsistency are revealed. Test predictions are made from all
classifiers while training only focuses on the current task classifier without
holistic alignment, leading to Classifier inconsistency. Prompt inconsistency
indicates that the prompt selected during testing may not correspond to the one
associated with this task during training. In this paper, we propose a novel
prompt-based method, Consistent Prompting (CPrompt), for more aligned training
and testing. Specifically, all existing classifiers are exposed to prompt
training, resulting in classifier consistency learning. In addition, prompt
consistency learning is proposed to enhance prediction robustness and boost
prompt selection accuracy. Our Consistent Prompting surpasses its prompt-based
counterparts and achieves state-of-the-art performance on multiple continual
learning benchmarks. Detailed analysis shows that improvements come from more
consistent training and testing.",0
A Novel Implicit Neural Representation for Volume Data,2403.08566v1,http://arxiv.org/abs/2403.08566v1,2024-03-13 14:22:13+00:00,"The storage of medical images is one of the challenges in the medical imaging
field. There are variable works that use implicit neural representation (INR)
to compress volumetric medical images. However, there is room to improve the
compression rate for volumetric medical images. Most of the INR techniques need
a huge amount of GPU memory and a long training time for high-quality medical
volume rendering. In this paper, we present a novel implicit neural
representation to compress volume data using our proposed architecture, that
is, the Lanczos downsampling scheme, SIREN deep network, and SRDenseNet
high-resolution scheme. Our architecture can effectively reduce training time,
and gain a high compression rate while retaining the final rendering quality.
Moreover, it can save GPU memory in comparison with the existing works. The
experiments show that the quality of reconstructed images and training speed
using our architecture is higher than current works which use the SIREN only.
Besides, the GPU memory cost is evidently decreased",0
Deep Learning based Positioning with Multi-task Learning and Uncertainty-based Fusion,2403.08565v1,http://arxiv.org/abs/2403.08565v1,2024-03-13 14:19:38+00:00,"Deep learning (DL) methods have been shown to improve the performance of
several use cases for the fifth-generation (5G) New radio (NR) air interface.
In this paper we investigate user equipment (UE) positioning using the channel
state information (CSI) fingerprints between a UE and multiple base stations
(BSs). In such a setup, a single DL model can be trained for UE positioning
using the CSI fingerprints of the multiple BSs as input. Alternatively, based
on the CSI at each BS, a separate DL model can be trained at each BS and then
the output of the different models are combined to determine the UE's position.
In this work we compare these different fusion techniques and show that fusing
the output of separate models achieves higher positioning accuracy, especially
in a dynamic scenario. We also show that the fusion of multiple outputs further
benefits from considering the uncertainty of the output of the DL model at each
BS. For a more efficient training of the DL model across BSs, we additionally
propose a multi-task learning (MTL) scheme by sharing some parameters across
the models while jointly training all models. This method, not only improves
the accuracy of the individual models, but also of the final combined estimate.
Lastly, we evaluate the reliability of the uncertainty estimation to ascertain
which of the fusion methods provides the highest quality of uncertainty
estimates.",0
Distributed Deep Learning for Modulation Classification in 6G Cell-Free Wireless Networks,2403.08563v1,http://arxiv.org/abs/2403.08563v1,2024-03-13 14:17:34+00:00,"In the evolution of 6th Generation (6G) technology, the emergence of
cell-free networking presents a paradigm shift, revolutionizing user
experiences within densely deployed networks where distributed access points
collaborate. However, the integration of intelligent mechanisms is crucial for
optimizing the efficiency, scalability, and adaptability of these 6G cell-free
networks. One application aiming to optimize spectrum usage is Automatic
Modulation Classification (AMC), a vital component for classifying and
dynamically adjusting modulation schemes. This paper explores different
distributed solutions for AMC in cell-free networks, addressing the training,
computational complexity, and accuracy of two practical approaches. The first
approach addresses scenarios where signal sharing is not feasible due to
privacy concerns or fronthaul limitations. Our findings reveal that maintaining
comparable accuracy is remarkably achievable, yet it comes with an increase in
computational demand. The second approach considers a central model and
multiple distributed models collaboratively classifying the modulation. This
hybrid model leverages diversity gain through signal combining and requires
synchronization and signal sharing. The hybrid model demonstrates superior
performance, achieving a 2.5% improvement in accuracy with equivalent total
computational load. Notably, the hybrid model distributes the computational
load across multiple devices, resulting in a lower individual computational
load.",0
Structural perspective on constraint-based learning of Markov networks,2403.08562v1,http://arxiv.org/abs/2403.08562v1,2024-03-13 14:14:47+00:00,"Markov networks are probabilistic graphical models that employ undirected
graphs to depict conditional independence relationships among variables. Our
focus lies in constraint-based structure learning, which entails learning the
undirected graph from data through the execution of conditional independence
tests. We establish theoretical limits concerning two critical aspects of
constraint-based learning of Markov networks: the number of tests and the sizes
of the conditioning sets. These bounds uncover an exciting interplay between
the structural properties of the graph and the amount of tests required to
learn a Markov network. The starting point of our work is that the graph
parameter maximum pairwise connectivity, $\kappa$, that is, the maximum number
of vertex-disjoint paths connecting a pair of vertices in the graph, is
responsible for the sizes of independence tests required to learn the graph. On
one hand, we show that at least one test with the size of the conditioning set
at least $\kappa$ is always necessary. On the other hand, we prove that any
graph can be learned by performing tests of size at most $\kappa$. This
completely resolves the question of the minimum size of conditioning sets
required to learn the graph. When it comes to the number of tests, our upper
bound on the sizes of conditioning sets implies that every $n$-vertex graph can
be learned by at most $n^{\kappa}$ tests with conditioning sets of sizes at
most $\kappa$. We show that for any upper bound $q$ on the sizes of the
conditioning sets, there exist graphs with $O(n q)$ vertices that require at
least $n^{\Omega(\kappa)}$ tests to learn. This lower bound holds even when the
treewidth and the maximum degree of the graph are at most $\kappa+2$. On the
positive side, we prove that every graph of bounded treewidth can be learned by
a polynomial number of tests with conditioning sets of sizes at most $2\kappa$.",0
Federated Knowledge Graph Unlearning via Diffusion Model,2403.08554v1,http://arxiv.org/abs/2403.08554v1,2024-03-13 14:06:51+00:00,"Federated learning (FL) promotes the development and application of
artificial intelligence technologies by enabling model sharing and
collaboration while safeguarding data privacy. Knowledge graph (KG) embedding
representation provides a foundation for knowledge reasoning and applications
by mapping entities and relations into vector space. Federated KG embedding
enables the utilization of knowledge from diverse client sources while
safeguarding the privacy of local data. However, due to demands such as privacy
protection and the need to adapt to dynamic data changes, investigations into
machine unlearning (MU) have been sparked. However, it is challenging to
maintain the performance of KG embedding models while forgetting the influence
of specific forgotten data on the model. In this paper, we propose FedDM, a
novel framework tailored for machine unlearning in federated knowledge graphs.
Leveraging diffusion models, we generate noisy data to sensibly mitigate the
influence of specific knowledge on FL models while preserving the overall
performance concerning the remaining data. We conduct experimental evaluations
on benchmark datasets to assess the efficacy of the proposed model. Extensive
experiments demonstrate that FedDM yields promising results in knowledge
forgetting.",0
Regret Analysis of Policy Optimization over Submanifolds for Linearly Constrained Online LQG,2403.08553v1,http://arxiv.org/abs/2403.08553v1,2024-03-13 14:06:18+00:00,"Recent advancement in online optimization and control has provided novel
tools to study online linear quadratic regulator (LQR) problems, where cost
matrices are varying adversarially over time. However, the controller
parameterization of existing works may not satisfy practical conditions like
sparsity due to physical connections. In this work, we study online linear
quadratic Gaussian problems with a given linear constraint imposed on the
controller. Inspired by the recent work of [1] which proposed, for a linearly
constrained policy optimization of an offline LQR, a second order method
equipped with a Riemannian metric that emerges naturally in the context of
optimal control problems, we propose online optimistic Newton on manifold
(OONM) which provides an online controller based on the prediction on the first
and second order information of the function sequence. To quantify the proposed
algorithm, we leverage the notion of regret defined as the sub-optimality of
its cumulative cost to that of a (locally) minimizing controller sequence and
provide the regret bound in terms of the path-length of the minimizer sequence.
Simulation results are also provided to verify the property of OONM.",0
"Does one still need to ""shut up and calculate""?",2403.08842v1,http://arxiv.org/abs/2403.08842v1,2024-03-13 14:03:25+00:00,"In learning quantum mechanics, an essential question has always been: How
does one go about developing a ""physical feel"" for quantum phenomena?
Naturally, one needs a basis or ground zero to start from, and that basis must
be unlike anything with which we are already familiar in consequence of our
experiences with the world of classical physics. We argue (channeling Richard
Feynman) that the most elementary and the least cumbersome concept to build
upon is the existence of complex probability amplitudes for physical events. An
event that can take place in multiple alternative ways should be treated by
adding the corresponding amplitudes when the paths are, in principle,
indistinguishable, and by adding the probabilities themselves when the paths
are distinguishable. Once we accept this principle and hone our intuition by
examining quantum phenomena in its light, we will be on the path to
""understanding"" quantum mechanics. Elementary examples from the field of
quantum optics demonstrate how adherence to Feynman's principle could lead to a
better, more ""intuitive"" appreciation for the magic of quantum mechanics.",0
CINA: Conditional Implicit Neural Atlas for Spatio-Temporal Representation of Fetal Brains,2403.08550v1,http://arxiv.org/abs/2403.08550v1,2024-03-13 14:02:42+00:00,"We introduce a conditional implicit neural atlas (CINA) for spatio-temporal
atlas generation from Magnetic Resonance Images (MRI) of the neurotypical and
pathological fetal brain, that is fully independent of affine or non-rigid
registration. During training, CINA learns a general representation of the
fetal brain and encodes subject specific information into latent code. After
training, CINA can construct a faithful atlas with tissue probability maps of
the fetal brain for any gestational age (GA) and anatomical variation covered
within the training domain. Thus, CINA is competent to represent both,
neurotypical and pathological brains. Furthermore, a trained CINA model can be
fit to brain MRI of unseen subjects via test-time optimization of the latent
code. CINA can then produce probabilistic tissue maps tailored to a particular
subject. We evaluate our method on a total of 198 T2 weighted MRI of normal and
abnormal fetal brains from the dHCP and FeTA datasets. We demonstrate CINA's
capability to represent a fetal brain atlas that can be flexibly conditioned on
GA and on anatomical variations like ventricular volume or degree of cortical
folding, making it a suitable tool for modeling both neurotypical and
pathological brains. We quantify the fidelity of our atlas by means of tissue
segmentation and age prediction and compare it to an established baseline. CINA
demonstrates superior accuracy for neurotypical brains and pathological brains
with ventriculomegaly. Moreover, CINA scores a mean absolute error of 0.23
weeks in fetal brain age prediction, further confirming an accurate
representation of fetal brain development.",0
Semantic Segmentation of Solar Radio Spikes at Low Frequencies,2403.08546v1,http://arxiv.org/abs/2403.08546v1,2024-03-13 13:59:40+00:00,"Solar radio spikes are short lived, narrow bandwidth features in low
frequency solar radio observations. The timing of their occurrence and the
number of spikes in a given observation is often unpredictable. The high
temporal and frequency of resolution of modern radio telescopes such as NenuFAR
mean that manually identifying radio spikes is an arduous task. Machine
learning approaches to data exploration in solar radio data is on the rise.
Here we describe a convolutional neural network to identify the per pixel
location of radio spikes as well as determine some simple characteristics of
duration, spectral width and drift rate. The model, which we call SpikeNet, was
trained using an Nvidia Tesla T4 16GB GPU with ~100000 sample spikes in a total
time of 2.2 hours. The segmentation performs well with an intersection over
union in the test set of ~0.85. The root mean squared error for predicted spike
properties is of the order of 23%. Applying the algorithm to unlabelled data
successfully generates segmentation masks although the accuracy of the
predicted properties is less reliable, particularly when more than one spike is
present in the same 64 X 64 pixel time-frequency range. We have successfully
demonstrated that our convolutional neural network can locate and characterise
solar radio spikes in a number of seconds compared to the weeks it would take
for manual identification.",0
Language models scale reliably with over-training and on downstream tasks,2403.08540v1,http://arxiv.org/abs/2403.08540v1,2024-03-13 13:54:00+00:00,"Scaling laws are useful guides for developing language models, but there are
still gaps between current scaling studies and how language models are
ultimately trained and evaluated. For instance, scaling is usually studied in
the compute-optimal training regime (i.e., ""Chinchilla optimal"" regime);
however, in practice, models are often over-trained to reduce inference costs.
Moreover, scaling laws mostly predict loss on next-token prediction, but
ultimately models are compared based on downstream task performance. In this
paper, we address both shortcomings. To do so, we create a testbed of 104
models with 0.011B to 6.9B parameters trained with various numbers of tokens on
three data distributions. First, we investigate scaling in the over-trained
regime. We fit scaling laws that extrapolate in both the number of model
parameters and the ratio of training tokens to parameters. This enables us to
predict the validation loss of a 1.4B parameter, 900B token run (i.e.,
32$\times$ over-trained) and a 6.9B parameter, 138B token
run$\unicode{x2014}$each from experiments that take 300$\times$ less compute.
Second, we relate the perplexity of a language model to its downstream task
performance via a power law. We use this law to predict top-1 error averaged
over downstream tasks for the two aforementioned models using experiments that
take 20$\times$ less compute. Our experiments are available at
https://github.com/mlfoundations/scaling.",0
HOLMES: HOLonym-MEronym based Semantic inspection for Convolutional Image Classifiers,2403.08536v1,http://arxiv.org/abs/2403.08536v1,2024-03-13 13:51:02+00:00,"Convolutional Neural Networks (CNNs) are nowadays the model of choice in
Computer Vision, thanks to their ability to automatize the feature extraction
process in visual tasks. However, the knowledge acquired during training is
fully subsymbolic, and hence difficult to understand and explain to end users.
In this paper, we propose a new technique called HOLMES (HOLonym-MEronym based
Semantic inspection) that decomposes a label into a set of related concepts,
and provides component-level explanations for an image classification model.
Specifically, HOLMES leverages ontologies, web scraping and transfer learning
to automatically construct meronym (parts)-based detectors for a given holonym
(class). Then, it produces heatmaps at the meronym level and finally, by
probing the holonym CNN with occluded images, it highlights the importance of
each part on the classification output. Compared to state-of-the-art saliency
methods, HOLMES takes a step further and provides information about both where
and what the holonym CNN is looking at, without relying on densely annotated
datasets and without forcing concepts to be associated to single computational
units. Extensive experimental evaluation on different categories of objects
(animals, tools and vehicles) shows the feasibility of our approach. On
average, HOLMES explanations include at least two meronyms, and the ablation of
a single meronym roughly halves the holonym model confidence. The resulting
heatmaps were quantitatively evaluated using the
deletion/insertion/preservation curves. All metrics were comparable to those
achieved by GradCAM, while offering the advantage of further decomposing the
heatmap in human-understandable concepts, thus highlighting both the relevance
of meronyms to object classification, as well as HOLMES ability to capture it.
The code is available at https://github.com/FrancesC0de/HOLMES.",0
From Weak to Strong Sound Event Labels using Adaptive Change-Point Detection and Active Learning,2403.08525v1,http://arxiv.org/abs/2403.08525v1,2024-03-13 13:33:35+00:00,"In this work we propose an audio recording segmentation method based on an
adaptive change point detection (A-CPD) for machine guided weak label
annotation of audio recording segments. The goal is to maximize the amount of
information gained about the temporal activation's of the target sounds. For
each unlabeled audio recording, we use a prediction model to derive a
probability curve used to guide annotation. The prediction model is initially
pre-trained on available annotated sound event data with classes that are
disjoint from the classes in the unlabeled dataset. The prediction model then
gradually adapts to the annotations provided by the annotator in an active
learning loop. The queries used to guide the weak label annotator towards
strong labels are derived using change point detection on these probabilities.
We show that it is possible to derive strong labels of high quality even with a
limited annotation budget, and show favorable results for A-CPD when compared
to two baseline query strategies.",0
UniLiDAR: Bridge the domain gap among different LiDARs for continual learning,2403.08512v1,http://arxiv.org/abs/2403.08512v1,2024-03-13 13:23:05+00:00,"LiDAR-based 3D perception algorithms have evolved rapidly alongside the
emergence of large datasets. Nonetheless, considerable performance degradation
often ensues when models trained on a specific dataset are applied to other
datasets or real-world scenarios with different LiDAR. This paper aims to
develop a unified model capable of handling different LiDARs, enabling
continual learning across diverse LiDAR datasets and seamless deployment across
heterogeneous platforms. We observe that the gaps among datasets primarily
manifest in geometric disparities (such as variations in beams and point
counts) and semantic inconsistencies (taxonomy conflicts). To this end, this
paper proposes UniLiDAR, an occupancy prediction pipeline that leverages
geometric realignment and semantic label mapping to facilitate multiple
datasets training and mitigate performance degradation during deployment on
heterogeneous platforms. Moreover, our method can be easily combined with
existing 3D perception models. The efficacy of the proposed approach in
bridging LiDAR domain gaps is verified by comprehensive experiments on two
prominent datasets: OpenOccupancy-nuScenes and SemanticKITTI. UniLiDAR elevates
the mIoU of occupancy prediction by 15.7% and 12.5%, respectively, compared to
the model trained on the directly merged dataset. Moreover, it outperforms
several SOTA methods trained on individual datasets. We expect our research to
facilitate further study of 3D generalization, the code will be available soon.",0
Towards Unified Modeling for Positive and Negative Preferences in Sign-Aware Recommendation,2403.08246v1,http://arxiv.org/abs/2403.08246v1,2024-03-13 05:00:42+00:00,"Recently, sign-aware graph recommendation has drawn much attention as it will
learn users' negative preferences besides positive ones from both positive and
negative interactions (i.e., links in a graph) with items. To accommodate the
different semantics of negative and positive links, existing works utilize two
independent encoders to model users' positive and negative preferences,
respectively. However, these approaches cannot learn the negative preferences
from high-order heterogeneous interactions between users and items formed by
multiple links with different signs, resulting in inaccurate and incomplete
negative user preferences. To cope with these intractable issues, we propose a
novel \textbf{L}ight \textbf{S}igned \textbf{G}raph Convolution Network
specifically for \textbf{Rec}ommendation (\textbf{LSGRec}), which adopts a
unified modeling approach to simultaneously model high-order users' positive
and negative preferences on a signed user-item interaction graph. Specifically,
for the negative preferences within high-order heterogeneous interactions,
first-order negative preferences are captured by the negative links, while
high-order negative preferences are propagated along positive edges. Then,
recommendation results are generated based on positive preferences and
optimized with negative ones. Finally, we train representations of users and
items through different auxiliary tasks. Extensive experiments on three
real-world datasets demonstrate that our method outperforms existing baselines
regarding performance and computational efficiency. Our code is available at
\url{https://anonymous.4open.science/r/LSGRec-BB95}.",0
Content-aware Masked Image Modeling Transformer for Stereo Image Compression,2403.08505v1,http://arxiv.org/abs/2403.08505v1,2024-03-13 13:12:57+00:00,"Existing learning-based stereo image codec adopt sophisticated transformation
with simple entropy models derived from single image codecs to encode latent
representations. However, those entropy models struggle to effectively capture
the spatial-disparity characteristics inherent in stereo images, which leads to
suboptimal rate-distortion results. In this paper, we propose a stereo image
compression framework, named CAMSIC. CAMSIC independently transforms each image
to latent representation and employs a powerful decoder-free Transformer
entropy model to capture both spatial and disparity dependencies, by
introducing a novel content-aware masked image modeling (MIM) technique. Our
content-aware MIM facilitates efficient bidirectional interaction between prior
information and estimated tokens, which naturally obviates the need for an
extra Transformer decoder. Experiments show that our stereo image codec
achieves state-of-the-art rate-distortion performance on two stereo image
datasets Cityscapes and InStereo2K with fast encoding and decoding speed.",0
Highly confined epsilon-near-zero- and surface-phonon polaritons in SrTiO3 membranes,2403.08500v1,http://arxiv.org/abs/2403.08500v1,2024-03-13 13:07:38+00:00,"Recent theoretical studies have suggested that transition metal perovskite
oxide membranes can enable surface phonon polaritons in the infrared range with
low loss and much stronger subwavelength confinement than bulk crystals. Such
modes, however, have not been experimentally observed so far. Here, using a
combination of far-field Fourier-transform infrared (FTIR) spectroscopy and
near-field synchrotron infrared nanospectroscopy (SINS) imaging, we study the
phonon-polaritons in a 100 nm thick freestanding crystalline membrane of SrTiO3
transferred on metallic and dielectric substrates. We observe a
symmetric-antisymmetric mode splitting giving rise to epsilon-near-zero and
Berreman modes as well as highly confined (by a factor of 10) propagating
phonon polaritons, both of which result from the deep-subwavelength thickness
of the membranes. Theoretical modeling based on the analytical finite-dipole
model and numerical finite-difference methods fully corroborate the
experimental results. Our work reveals the potential of oxide membranes as a
promising platform for infrared photonics and polaritonics.",0
Rich Semantic Knowledge Enhanced Large Language Models for Few-shot Chinese Spell Checking,2403.08492v1,http://arxiv.org/abs/2403.08492v1,2024-03-13 12:55:43+00:00,"Chinese Spell Checking (CSC) is a widely used technology, which plays a vital
role in speech to text (STT) and optical character recognition (OCR). Most of
the existing CSC approaches relying on BERT architecture achieve excellent
performance. However, limited by the scale of the foundation model, BERT-based
method does not work well in few-shot scenarios, showing certain limitations in
practical applications. In this paper, we explore using an in-context learning
method named RS-LLM (Rich Semantic based LLMs) to introduce large language
models (LLMs) as the foundation model. Besides, we study the impact of
introducing various Chinese rich semantic information in our framework. We
found that by introducing a small number of specific Chinese rich semantic
structures, LLMs achieve better performance than the BERT-based model on
few-shot CSC task. Furthermore, we conduct experiments on multiple datasets,
and the experimental results verified the superiority of our proposed
framework.",0
SoK: Reducing the Vulnerability of Fine-tuned Language Models to Membership Inference Attacks,2403.08481v1,http://arxiv.org/abs/2403.08481v1,2024-03-13 12:46:51+00:00,"Natural language processing models have experienced a significant upsurge in
recent years, with numerous applications being built upon them. Many of these
applications require fine-tuning generic base models on customized, proprietary
datasets. This fine-tuning data is especially likely to contain personal or
sensitive information about individuals, resulting in increased privacy risk.
Membership inference attacks are the most commonly employed attack to assess
the privacy leakage of a machine learning model. However, limited research is
available on the factors that affect the vulnerability of language models to
this kind of attack, or on the applicability of different defense strategies in
the language domain. We provide the first systematic review of the
vulnerability of fine-tuned large language models to membership inference
attacks, the various factors that come into play, and the effectiveness of
different defense strategies. We find that some training methods provide
significantly reduced privacy risk, with the combination of differential
privacy and low-rank adaptors achieving the best privacy protection against
these attacks.",0
Unleashing the Power of Meta-tuning for Few-shot Generalization Through Sparse Interpolated Experts,2403.08477v1,http://arxiv.org/abs/2403.08477v1,2024-03-13 12:46:03+00:00,"Conventional wisdom suggests parameter-efficient fine-tuning of foundation
models as the state-of-the-art method for transfer learning in vision,
replacing the rich literature of alternatives such as meta-learning. In trying
to harness the best of both worlds, meta-tuning introduces a subsequent
optimization stage of foundation models but has so far only shown limited
success and crucially tends to underperform on out-of-domain (OOD) tasks. In
this paper, we introduce Sparse MetA-Tuning (SMAT), a method inspired by sparse
mixture-of-experts approaches and trained to isolate subsets of pre-trained
parameters automatically for meta-tuning on each task. SMAT successfully
overcomes OOD sensitivity and delivers on the promise of enhancing the transfer
abilities of vision foundation models beyond parameter-efficient finetuning. We
establish new state-of-the-art results on a challenging combination of
Meta-Dataset augmented with additional OOD tasks in both zero-shot and
gradient-based adaptation settings. In addition, we provide a thorough analysis
of the superiority of learned over hand-designed sparsity patterns for sparse
expert methods and the pivotal importance of the sparsity level in balancing
between in-domain and out-of-domain generalization. Our code is publicly
available.",0
An Analysis of Human Alignment of Latent Diffusion Models,2403.08469v1,http://arxiv.org/abs/2403.08469v1,2024-03-13 12:31:08+00:00,"Diffusion models, trained on large amounts of data, showed remarkable
performance for image synthesis. They have high error consistency with humans
and low texture bias when used for classification. Furthermore, prior work
demonstrated the decomposability of their bottleneck layer representations into
semantic directions. In this work, we analyze how well such representations are
aligned to human responses on a triplet odd-one-out task. We find that despite
the aforementioned observations: I) The representational alignment with humans
is comparable to that of models trained only on ImageNet-1k. II) The most
aligned layers of the denoiser U-Net are intermediate layers and not the
bottleneck. III) Text conditioning greatly improves alignment at high noise
levels, hinting at the importance of abstract textual information, especially
in the early stage of generation.",0
Diffusion Models with Implicit Guidance for Medical Anomaly Detection,2403.08464v1,http://arxiv.org/abs/2403.08464v1,2024-03-13 12:26:55+00:00,"Diffusion models have advanced unsupervised anomaly detection by improving
the transformation of pathological images into pseudo-healthy equivalents.
Nonetheless, standard approaches may compromise critical information during
pathology removal, leading to restorations that do not align with unaffected
regions in the original scans. Such discrepancies can inadvertently increase
false positive rates and reduce specificity, complicating radiological
evaluations. This paper introduces Temporal Harmonization for Optimal
Restoration (THOR), which refines the de-noising process by integrating
implicit guidance through temporal anomaly maps. THOR aims to preserve the
integrity of healthy tissue in areas unaffected by pathology. Comparative
evaluations show that THOR surpasses existing diffusion-based methods in
detecting and segmenting anomalies in brain MRIs and wrist X-rays. Code:
https://github.com/ci-ber/THOR_DDPM.",0
Authorship Verification based on the Likelihood Ratio of Grammar Models,2403.08462v1,http://arxiv.org/abs/2403.08462v1,2024-03-13 12:25:47+00:00,"Authorship Verification (AV) is the process of analyzing a set of documents
to determine whether they were written by a specific author. This problem often
arises in forensic scenarios, e.g., in cases where the documents in question
constitute evidence for a crime. Existing state-of-the-art AV methods use
computational solutions that are not supported by a plausible scientific
explanation for their functioning and that are often difficult for analysts to
interpret. To address this, we propose a method relying on calculating a
quantity we call $\lambda_G$ (LambdaG): the ratio between the likelihood of a
document given a model of the Grammar for the candidate author and the
likelihood of the same document given a model of the Grammar for a reference
population. These Grammar Models are estimated using $n$-gram language models
that are trained solely on grammatical features. Despite not needing large
amounts of data for training, LambdaG still outperforms other established AV
methods with higher computational complexity, including a fine-tuned Siamese
Transformer network. Our empirical evaluation based on four baseline methods
applied to twelve datasets shows that LambdaG leads to better results in terms
of both accuracy and AUC in eleven cases and in all twelve cases if considering
only topic-agnostic methods. The algorithm is also highly robust to important
variations in the genre of the reference population in many cross-genre
comparisons. In addition to these properties, we demonstrate how LambdaG is
easier to interpret than the current state-of-the-art. We argue that the
advantage of LambdaG over other methods is due to fact that it is compatible
with Cognitive Linguistic theories of language processing.",0
Towards Dense and Accurate Radar Perception Via Efficient Cross-Modal Diffusion Model,2403.08460v1,http://arxiv.org/abs/2403.08460v1,2024-03-13 12:20:20+00:00,"Millimeter wave (mmWave) radars have attracted significant attention from
both academia and industry due to their capability to operate in extreme
weather conditions. However, they face challenges in terms of sparsity and
noise interference, which hinder their application in the field of micro aerial
vehicle (MAV) autonomous navigation. To this end, this paper proposes a novel
approach to dense and accurate mmWave radar point cloud construction via
cross-modal learning. Specifically, we introduce diffusion models, which
possess state-of-the-art performance in generative modeling, to predict
LiDAR-like point clouds from paired raw radar data. We also incorporate the
most recent diffusion model inference accelerating techniques to ensure that
the proposed method can be implemented on MAVs with limited computing
resources.We validate the proposed method through extensive benchmark
comparisons and real-world experiments, demonstrating its superior performance
and generalization ability. Code and pretrained models will be available at
https://github.com/ZJU-FAST-Lab/Radar-Diffusion.",0
Learning-Enhanced Neighborhood Selection for the Vehicle Routing Problem with Time Windows,2403.08839v1,http://arxiv.org/abs/2403.08839v1,2024-03-13 12:08:27+00:00,"Large Neighborhood Search (LNS) is a universal approach that is broadly
applicable and has proven to be highly efficient in practice for solving
optimization problems. We propose to integrate machine learning (ML) into LNS
to assist in deciding which parts of the solution should be destroyed and
repaired in each iteration of LNS. We refer to our new approach as
Learning-Enhanced Neighborhood Selection (LENS for short). Our approach is
universally applicable, i.e., it can be applied to any LNS algorithm to amplify
the workings of the destroy algorithm. In this paper, we demonstrate the
potential of LENS on the fundamental Vehicle Routing Problem with Time Windows
(VRPTW). We implemented an LNS algorithm for VRPTW and collected data on
generated novel training instances derived from well-known, extensively
utilized benchmark datasets. We trained our LENS approach with this data and
compared the experimental results of our approach with two benchmark
algorithms: a random neighborhood selection method to show that LENS learns to
make informed choices and an oracle neighborhood selection method to
demonstrate the potential of our LENS approach. With LENS, we obtain results
that significantly improve the quality of the solutions.",0
Better Fit: Accommodate Variations in Clothing Types for Virtual Try-on,2403.08453v1,http://arxiv.org/abs/2403.08453v1,2024-03-13 12:07:14+00:00,"Image-based virtual try-on aims to transfer target in-shop clothing to a
dressed model image, the objectives of which are totally taking off original
clothing while preserving the contents outside of the try-on area, naturally
wearing target clothing and correctly inpainting the gap between target
clothing and original clothing. Tremendous efforts have been made to facilitate
this popular research area, but cannot keep the type of target clothing with
the try-on area affected by original clothing. In this paper, we focus on the
unpaired virtual try-on situation where target clothing and original clothing
on the model are different, i.e., the practical scenario. To break the
correlation between the try-on area and the original clothing and make the
model learn the correct information to inpaint, we propose an adaptive mask
training paradigm that dynamically adjusts training masks. It not only improves
the alignment and fit of clothing but also significantly enhances the fidelity
of virtual try-on experience. Furthermore, we for the first time propose two
metrics for unpaired try-on evaluation, the Semantic-Densepose-Ratio (SDR) and
Skeleton-LPIPS (S-LPIPS), to evaluate the correctness of clothing type and the
accuracy of clothing texture. For unpaired try-on validation, we construct a
comprehensive cross-try-on benchmark (Cross-27) with distinctive clothing items
and model physiques, covering a broad try-on scenarios. Experiments demonstrate
the effectiveness of the proposed methods, contributing to the advancement of
virtual try-on technology and offering new insights and tools for future
research in the field. The code, model and benchmark will be publicly released.",0
An Integrated Usability Framework for Evaluating Open Government Data Portals: Comparative Analysis of EU and GCC Countries,2403.08451v1,http://arxiv.org/abs/2403.08451v1,2024-03-13 12:06:42+00:00,"This study explores the critical role of open government data (OGD) portals
in fostering transparency and collaboration between diverse stakeholders.
Recognizing the challenges of usability, communication with diverse
populations, and strategic value creation, this paper develops an integrated
framework for evaluating OGD portal effectiveness that accommodates user
diversity (regardless of their data literacy and language), evaluates
collaboration and participation, and the ability of users to explore and
understand the data provided through them. The framework is validated by
applying it to 33 national portals across European Union and Gulf Cooperation
Council (GCC) countries, as a result of which we rank OGD portals, identify
some good practices that lower-performing portals can learn from, and common
shortcomings. Notably, the study unveils the competitive and innovative nature
of GCC OGD portals, pinpointing specific improvement areas such as multilingual
support and data understandability. The findings underscore the growing trend
of exposing data quality metrics and advocate for enhanced two-way
communication channels between users and portal representatives. Overall, the
study contributes to accelerating the development of user-friendly,
collaborative, and sustainable OGD portals while addressing gaps identified in
previous research.",0
Predictive Clustering of Vessel Behavior Based on Hierarchical Trajectory Representation,2403.08838v1,http://arxiv.org/abs/2403.08838v1,2024-03-13 12:05:02+00:00,"Vessel trajectory clustering, which aims to find similar trajectory patterns,
has been widely leveraged in overwater applications. Most traditional methods
use predefined rules and thresholds to identify discrete vessel behaviors. They
aim for high-quality clustering and conduct clustering on entire sequences,
whether the original trajectory or its sub-trajectories, failing to represent
their evolution. To resolve this problem, we propose a Predictive Clustering of
Hierarchical Vessel Behavior (PC-HiV). PC-HiV first uses hierarchical
representations to transform every trajectory into a behavioral sequence. Then,
it predicts evolution at each timestamp of the sequence based on the
representations. By applying predictive clustering and latent encoding, PC-HiV
improves clustering and predictions simultaneously. Experiments on real AIS
datasets demonstrate PC-HiV's superiority over existing methods, showcasing its
effectiveness in capturing behavioral evolution discrepancies between vessel
types (tramp vs. liner) and within emission control areas. Results show that
our method outperforms NN-Kmeans and Robust DAA by 3.9% and 6.4% of the purity
score.",0
A Physics-driven GraphSAGE Method for Physical Process Simulations Described by Partial Differential Equations,2403.08569v1,http://arxiv.org/abs/2403.08569v1,2024-03-13 14:25:15+00:00,"Physics-informed neural networks (PINNs) have successfully addressed various
computational physics problems based on partial differential equations (PDEs).
However, while tackling issues related to irregularities like singularities and
oscillations, trained solutions usually suffer low accuracy. In addition, most
current works only offer the trained solution for predetermined input
parameters. If any change occurs in input parameters, transfer learning or
retraining is required, and traditional numerical techniques also need an
independent simulation. In this work, a physics-driven GraphSAGE approach
(PD-GraphSAGE) based on the Galerkin method and piecewise polynomial nodal
basis functions is presented to solve computational problems governed by
irregular PDEs and to develop parametric PDE surrogate models. This approach
employs graph representations of physical domains, thereby reducing the demands
for evaluated points due to local refinement. A distance-related edge feature
and a feature mapping strategy are devised to help training and convergence for
singularity and oscillation situations, respectively. The merits of the
proposed method are demonstrated through a couple of cases. Moreover, the
robust PDE surrogate model for heat conduction problems parameterized by the
Gaussian random field source is successfully established, which not only
provides the solution accurately but is several times faster than the finite
element method in our experiments.",0
Caformer: Rethinking Time Series Analysis from Causal Perspective,2403.08572v1,http://arxiv.org/abs/2403.08572v1,2024-03-13 14:28:02+00:00,"Time series analysis is a vital task with broad applications in various
domains. However, effectively capturing cross-dimension and cross-time
dependencies in non-stationary time series poses significant challenges,
particularly in the context of environmental factors. The spurious correlation
induced by the environment confounds the causal relationships between
cross-dimension and cross-time dependencies. In this paper, we introduce a
novel framework called Caformer (\underline{\textbf{Ca}}usal
Trans\underline{\textbf{former}}) for time series analysis from a causal
perspective. Specifically, our framework comprises three components: Dynamic
Learner, Environment Learner, and Dependency Learner. The Dynamic Learner
unveils dynamic interactions among dimensions, the Environment Learner
mitigates spurious correlations caused by environment with a back-door
adjustment, and the Dependency Learner aims to infer robust interactions across
both time and dimensions. Our Caformer demonstrates consistent state-of-the-art
performance across five mainstream time series analysis tasks, including long-
and short-term forecasting, imputation, classification, and anomaly detection,
with proper interpretability.",0
Machine Learning Optimized Orthogonal Basis Piecewise Polynomial Approximation,2403.08579v1,http://arxiv.org/abs/2403.08579v1,2024-03-13 14:34:34+00:00,"Piecewise Polynomials (PPs) are utilized in several engineering disciplines,
like trajectory planning, to approximate position profiles given in the form of
a set of points. While the approximation target along with domain-specific
requirements, like Ck -continuity, can be formulated as a system of equations
and a result can be computed directly, such closed-form solutions posses
limited flexibility with respect to polynomial degrees, polynomial bases or
adding further domain-specific requirements. Sufficiently complex optimization
goals soon call for the use of numerical methods, like gradient descent. Since
gradient descent lies at the heart of training Artificial Neural Networks
(ANNs), modern Machine Learning (ML) frameworks like TensorFlow come with a set
of gradient-based optimizers potentially suitable for a wide range of
optimization problems beyond the training task for ANNs. Our approach is to
utilize the versatility of PP models and combine it with the potential of
modern ML optimizers for the use in function approximation in 1D trajectory
planning in the context of electronic cam design. We utilize available
optimizers of the ML framework TensorFlow directly, outside of the scope of
ANNs, to optimize model parameters of our PP model. In this paper, we show how
an orthogonal polynomial basis contributes to improving approximation and
continuity optimization performance. Utilizing Chebyshev polynomials of the
first kind, we develop a novel regularization approach enabling clearly
improved convergence behavior. We show that, using this regularization
approach, Chebyshev basis performs better than power basis for all relevant
optimizers in the combined approximation and continuity optimization setting
and demonstrate usability of the presented approach within the electronic cam
domain.",0
Local Binary and Multiclass SVMs Trained on a Quantum Annealer,2403.08584v1,http://arxiv.org/abs/2403.08584v1,2024-03-13 14:37:00+00:00,"Support vector machines (SVMs) are widely used machine learning models (e.g.,
in remote sensing), with formulations for both classification and regression
tasks. In the last years, with the advent of working quantum annealers, hybrid
SVM models characterised by quantum training and classical execution have been
introduced. These models have demonstrated comparable performance to their
classical counterparts. However, they are limited in the training set size due
to the restricted connectivity of the current quantum annealers. Hence, to take
advantage of large datasets (like those related to Earth observation), a
strategy is required. In the classical domain, local SVMs, namely, SVMs trained
on the data samples selected by a k-nearest neighbors model, have already
proven successful. Here, the local application of quantum-trained SVM models is
proposed and empirically assessed. In particular, this approach allows
overcoming the constraints on the training set size of the quantum-trained
models while enhancing their performance. In practice, the FaLK-SVM method,
designed for efficient local SVMs, has been combined with quantum-trained SVM
models for binary and multiclass classification. In addition, for comparison,
FaLK-SVM has been interfaced for the first time with a classical single-step
multiclass SVM model (CS SVM). Concerning the empirical evaluation, D-Wave's
quantum annealers and real-world datasets taken from the remote sensing domain
have been employed. The results have shown the effectiveness and scalability of
the proposed approach, but also its practical applicability in a real-world
large-scale scenario.",0
Valuation of Power Purchase Agreements for Corporate Renewable Energy Procurement,2403.08846v1,http://arxiv.org/abs/2403.08846v1,2024-03-13 16:35:07+00:00,"Corporate renewable power purchase agreements (PPAs) are long-term contracts
that enable companies to source renewable energy without having to develop and
operate their own capacities. Typically, producers and consumers agree on a
fixed per-unit price at which power is purchased. The value of the PPA to the
buyer depends on the so called capture price defined as the difference between
this fixed price and the market value of the produced volume during the
duration of the contract. To model the capture price, practitioners often use
either fundamental or statistical approaches to model future market prices,
which both have their inherent limitations. We propose a new approach that
blends the logic of fundamental electricity market models with statistical
learning techniques. In particular, we use regularized inverse optimization in
a quadratic fundamental bottom-up model of the power market to estimate the
marginal costs of different technologies as a parametric function of exogenous
factors. We compare the out-of-sample performance in forecasting the capture
price using market data from three European countries and demonstrate that our
approach outperforms established statistical learning benchmarks. We then
discuss the case of a photovoltaic plant in Spain to illustrate how to use the
model to value a PPA from the buyer's perspective.",0
Bifurcated Attention for Single-Context Large-Batch Sampling,2403.08845v1,http://arxiv.org/abs/2403.08845v1,2024-03-13 16:30:57+00:00,"In our study, we present bifurcated attention, a method developed for
language model inference in single-context batch sampling contexts. This
approach aims to reduce redundant memory IO costs, a significant factor in
latency for high batch sizes and long context lengths. Bifurcated attention
achieves this by dividing the attention mechanism during incremental decoding
into two distinct GEMM operations, focusing on the KV cache from prefill and
the decoding process. This method ensures precise computation and maintains the
usual computational load (FLOPs) of standard attention mechanisms, but with
reduced memory IO. Bifurcated attention is also compatible with multi-query
attention mechanism known for reduced memory IO for KV cache, further enabling
higher batch size and context length. The resulting efficiency leads to lower
latency, improving suitability for real-time applications, e.g., enabling
massively-parallel answer generation without substantially increasing latency,
enhancing performance when integrated with postprocessing techniques such as
reranking.",0
When can we Approximate Wide Contrastive Models with Neural Tangent Kernels and Principal Component Analysis?,2403.08673v1,http://arxiv.org/abs/2403.08673v1,2024-03-13 16:25:55+00:00,"Contrastive learning is a paradigm for learning representations from
unlabelled data that has been highly successful for image and text data.
Several recent works have examined contrastive losses to claim that contrastive
models effectively learn spectral embeddings, while few works show relations
between (wide) contrastive models and kernel principal component analysis
(PCA). However, it is not known if trained contrastive models indeed correspond
to kernel methods or PCA. In this work, we analyze the training dynamics of
two-layer contrastive models, with non-linear activation, and answer when these
models are close to PCA or kernel methods. It is well known in the supervised
setting that neural networks are equivalent to neural tangent kernel (NTK)
machines, and that the NTK of infinitely wide networks remains constant during
training. We provide the first convergence results of NTK for contrastive
losses, and present a nuanced picture: NTK of wide networks remains almost
constant for cosine similarity based contrastive losses, but not for losses
based on dot product similarity. We further study the training dynamics of
contrastive models with orthogonality constraints on output layer, which is
implicitly assumed in works relating contrastive learning to spectral
embedding. Our deviation bounds suggest that representations learned by
contrastive models are close to the principal components of a certain matrix
computed from random features. We empirically show that our theoretical results
possibly hold beyond two-layer networks.",0
Zero-shot and Few-shot Generation Strategies for Artificial Clinical Records,2403.08664v2,http://arxiv.org/abs/2403.08664v2,2024-03-13 16:17:09+00:00,"The challenge of accessing historical patient data for clinical research,
while adhering to privacy regulations, is a significant obstacle in medical
science. An innovative approach to circumvent this issue involves utilising
synthetic medical records that mirror real patient data without compromising
individual privacy. The creation of these synthetic datasets, particularly
without using actual patient data to train Large Language Models (LLMs),
presents a novel solution as gaining access to sensitive patient information to
train models is also a challenge. This study assesses the capability of the
Llama 2 LLM to create synthetic medical records that accurately reflect real
patient information, employing zero-shot and few-shot prompting strategies for
comparison against fine-tuned methodologies that do require sensitive patient
data during training. We focus on generating synthetic narratives for the
History of Present Illness section, utilising data from the MIMIC-IV dataset
for comparison. In this work introduce a novel prompting technique that
leverages a chain-of-thought approach, enhancing the model's ability to
generate more accurate and contextually relevant medical narratives without
prior fine-tuning. Our findings suggest that this chain-of-thought prompted
approach allows the zero-shot model to achieve results on par with those of
fine-tuned models, based on Rouge metrics evaluation.",0
Self-Supervised Learning for Covariance Estimation,2403.08662v1,http://arxiv.org/abs/2403.08662v1,2024-03-13 16:16:20+00:00,"We consider the use of deep learning for covariance estimation. We propose to
globally learn a neural network that will then be applied locally at inference
time. Leveraging recent advancements in self-supervised foundational models, we
train the network without any labeling by simply masking different samples and
learning to predict their covariance given their surrounding neighbors. The
architecture is based on the popular attention mechanism. Its main advantage
over classical methods is the automatic exploitation of global characteristics
without any distributional assumptions or regularization. It can be pre-trained
as a foundation model and then be repurposed for various downstream tasks,
e.g., adaptive target detection in radar or hyperspectral imagery.",0
Predicting long timescale kinetics under variable experimental conditions with Kinetica.jl,2403.08657v1,http://arxiv.org/abs/2403.08657v1,2024-03-13 16:10:11+00:00,"Predicting the degradation processes of molecules over long timescales is a
key aspect of industrial materials design. However, it is made computationally
challenging by the need to construct large networks of chemical reactions that
are relevant to the experimental conditions that kinetic models must mirror,
with every reaction requiring accurate kinetic data. Here we showcase
Kinetica.jl, a new software package for constructing large-scale chemical
reaction networks in a fully-automated fashion by exploring chemical reaction
space with a kinetics-driven algorithm; coupled to efficient machine-learning
models of activation energies for sampled elementary reactions, we show how
this approach readily enables generation and kinetic characterization of
networks containing $\sim10^{3}$ chemical species and $10^{4}$ - $10^{5}$
reactions. Symbolic-numeric modelling of the generated reaction networks is
used to allow for flexible, efficient computation of kinetic profiles under
experimentally-realizable conditions such as continuously-variable temperature
regimes, enabling direct connection between bottom-up reaction networks and
experimental observations. Highly efficient propagation of long-timescale
kinetic profiles is required for automated reaction network refinement and is
enabled here by a new discrete kinetic approximation. The resulting Kinetica.jl
simulation package therefore enables automated generation, characterization,
and long-timescale modelling of complex chemical reaction systems. We
demonstrate this for hydrocarbon pyrolysis simulated over timescales of
seconds, using transient temperature profiles representing those of tubular
flow reactor experiments.",0
An Efficient End-to-End Approach to Noise Invariant Speech Features via Multi-Task Learning,2403.08654v1,http://arxiv.org/abs/2403.08654v1,2024-03-13 16:08:59+00:00,"Self-supervised speech representation learning enables the extraction of
meaningful features from raw waveforms. These features can then be efficiently
used across multiple downstream tasks. However, two significant issues arise
when considering the deployment of such methods ``in-the-wild"": (i) Their large
size, which can be prohibitive for edge applications; and (ii) their robustness
to detrimental factors, such as noise and/or reverberation, that can heavily
degrade the performance of such systems. In this work, we propose
RobustDistiller, a novel knowledge distillation mechanism that tackles both
problems jointly. Simultaneously to the distillation recipe, we apply a
multi-task learning objective to encourage the network to learn noise-invariant
representations by denoising the input. The proposed mechanism is evaluated on
twelve different downstream tasks. It outperforms several benchmarks regardless
of noise type, or noise and reverberation levels. Experimental results show
that the new Student model with 23M parameters can achieve results comparable
to the Teacher model with 95M parameters. Lastly, we show that the proposed
recipe can be applied to other distillation methodologies, such as the recent
DPWavLM. For reproducibility, code and model checkpoints will be made available
at \mbox{\url{https://github.com/Hguimaraes/robustdistiller}}.",0
"Extracting Explanations, Justification, and Uncertainty from Black-Box Deep Neural Networks",2403.08652v1,http://arxiv.org/abs/2403.08652v1,2024-03-13 16:06:26+00:00,"Deep Neural Networks (DNNs) do not inherently compute or exhibit
empirically-justified task confidence. In mission critical applications, it is
important to both understand associated DNN reasoning and its supporting
evidence. In this paper, we propose a novel Bayesian approach to extract
explanations, justifications, and uncertainty estimates from DNNs. Our approach
is efficient both in terms of memory and computation, and can be applied to any
black box DNN without any retraining, including applications to anomaly
detection and out-of-distribution detection tasks. We validate our approach on
the CIFAR-10 dataset, and show that it can significantly improve the
interpretability and reliability of DNNs.",0
Data Augmentation in Human-Centric Vision,2403.08650v1,http://arxiv.org/abs/2403.08650v1,2024-03-13 16:05:18+00:00,"This survey presents a comprehensive analysis of data augmentation techniques
in human-centric vision tasks, a first of its kind in the field. It delves into
a wide range of research areas including person ReID, human parsing, human pose
estimation, and pedestrian detection, addressing the significant challenges
posed by overfitting and limited training data in these domains. Our work
categorizes data augmentation methods into two main types: data generation and
data perturbation. Data generation covers techniques like graphic engine-based
generation, generative model-based generation, and data recombination, while
data perturbation is divided into image-level and human-level perturbations.
Each method is tailored to the unique requirements of human-centric tasks, with
some applicable across multiple areas. Our contributions include an extensive
literature review, providing deep insights into the influence of these
augmentation techniques in human-centric vision and highlighting the nuances of
each method. We also discuss open issues and future directions, such as the
integration of advanced generative models like Latent Diffusion Models, for
creating more realistic and diverse training data. This survey not only
encapsulates the current state of data augmentation in human-centric vision but
also charts a course for future research, aiming to develop more robust,
accurate, and efficient human-centric vision systems.",0
A Causal Inspired Early-Branching Structure for Domain Generalization,2403.08649v1,http://arxiv.org/abs/2403.08649v1,2024-03-13 16:04:29+00:00,"Learning domain-invariant semantic representations is crucial for achieving
domain generalization (DG), where a model is required to perform well on unseen
target domains. One critical challenge is that standard training often results
in entangled semantic and domain-specific features. Previous works suggest
formulating the problem from a causal perspective and solving the entanglement
problem by enforcing marginal independence between the causal (\ie semantic)
and non-causal (\ie domain-specific) features. Despite its simplicity, the
basic marginal independent-based idea alone may be insufficient to identify the
causal feature. By d-separation, we observe that the causal feature can be
further characterized by being independent of the domain conditioned on the
object, and we propose the following two strategies as complements for the
basic framework.
  First, the observation implicitly implies that for the same object, the
causal feature should not be associated with the non-causal feature, revealing
that the common practice of obtaining the two features with a shared base
feature extractor and two lightweight prediction heads might be inappropriate.
To meet the constraint, we propose a simple early-branching structure, where
the causal and non-causal feature obtaining branches share the first few blocks
while diverging thereafter, for better structure design; Second, the
observation implies that the causal feature remains invariant across different
domains for the same object. To this end, we suggest that augmentation should
be incorporated into the framework to better characterize the causal feature,
and we further suggest an effective random domain sampling scheme to fulfill
the task. Theoretical and experimental results show that the two strategies are
beneficial for the basic marginal independent-based framework. Code is
available at \url{https://github.com/liangchen527/CausEB}.",0
Meta Reinforcement Learning for Resource Allocation in Aerial Active-RIS-assisted Networks with Rate-Splitting Multiple Access,2403.08648v1,http://arxiv.org/abs/2403.08648v1,2024-03-13 16:02:18+00:00,"Mounting a reconfigurable intelligent surface (RIS) on an unmanned aerial
vehicle (UAV) holds promise for improving traditional terrestrial network
performance. Unlike conventional methods deploying passive RIS on UAVs, this
study delves into the efficacy of an aerial active RIS (AARIS). Specifically,
the downlink transmission of an AARIS network is investigated, where the base
station (BS) leverages rate-splitting multiple access (RSMA) for effective
interference management and benefits from the support of an AARIS for jointly
amplifying and reflecting the BS's transmit signals. Considering both the
non-trivial energy consumption of the active RIS and the limited energy storage
of the UAV, we propose an innovative element selection strategy for optimizing
the on/off status of RIS elements, which adaptively and remarkably manages the
system's power consumption. To this end, a resource management problem is
formulated, aiming to maximize the system energy efficiency (EE) by jointly
optimizing the transmit beamforming at the BS, the element activation, the
phase shift and the amplification factor at the RIS, the RSMA common data rate
at users, as well as the UAV's trajectory. Due to the dynamicity nature of UAV
and user mobility, a deep reinforcement learning (DRL) algorithm is designed
for resource allocation, utilizing meta-learning to adaptively handle fast
time-varying system dynamics. Simulations indicate that incorporating an active
RIS at the UAV leads to substantial EE gain, compared to passive RIS-aided UAV.
We observe the superiority of the RSMA-based AARIS system in terms of EE,
compared to existing approaches adopting non-orthogonal multiple access (NOMA).",0
HIMap: HybrId Representation Learning for End-to-end Vectorized HD Map Construction,2403.08639v1,http://arxiv.org/abs/2403.08639v1,2024-03-13 15:51:23+00:00,"Vectorized High-Definition (HD) map construction requires predictions of the
category and point coordinates of map elements (e.g. road boundary, lane
divider, pedestrian crossing, etc.). State-of-the-art methods are mainly based
on point-level representation learning for regressing accurate point
coordinates. However, this pipeline has limitations in obtaining element-level
information and handling element-level failures, e.g. erroneous element shape
or entanglement between elements. To tackle the above issues, we propose a
simple yet effective HybrId framework named HIMap to sufficiently learn and
interact both point-level and element-level information. Concretely, we
introduce a hybrid representation called HIQuery to represent all map elements,
and propose a point-element interactor to interactively extract and encode the
hybrid information of elements, e.g. point position and element shape, into the
HIQuery. Additionally, we present a point-element consistency constraint to
enhance the consistency between the point-level and element-level information.
Finally, the output point-element integrated HIQuery can be directly converted
into map elements' class, point coordinates, and mask. We conduct extensive
experiments and consistently outperform previous methods on both nuScenes and
Argoverse2 datasets. Notably, our method achieves $77.8$ mAP on the nuScenes
dataset, remarkably superior to previous SOTAs by $8.3$ mAP at least.",0
Disparate Effect Of Missing Mediators On Transportability of Causal Effects,2403.08638v1,http://arxiv.org/abs/2403.08638v1,2024-03-13 15:51:03+00:00,"Transported mediation effects provide an avenue to understand how upstream
interventions (such as improved neighborhood conditions like green spaces)
would work differently when applied to different populations as a result of
factors that mediate the effects. However, when mediators are missing in the
population where the effect is to be transported, these estimates could be
biased. We study this issue of missing mediators, motivated by challenges in
public health, wherein mediators can be missing, not at random. We propose a
sensitivity analysis framework that quantifies the impact of missing mediator
data on transported mediation effects. This framework enables us to identify
the settings under which the conditional transported mediation effect is
rendered insignificant for the subgroup with missing mediator data.
Specifically, we provide the bounds on the transported mediation effect as a
function of missingness. We then apply the framework to longitudinal data from
the Moving to Opportunity Study, a large-scale housing voucher experiment, to
quantify the effect of missing mediators on transport effect estimates of
voucher receipt, an upstream intervention on living location, in childhood on
subsequent risk of mental health or substance use disorder mediated through
parental health across sites. Our findings provide a tangible understanding of
how much missing data can be withstood for unbiased effect estimates.",0
Human Alignment of Large Language Models through Online Preference Optimisation,2403.08635v1,http://arxiv.org/abs/2403.08635v1,2024-03-13 15:47:26+00:00,"Ensuring alignment of language models' outputs with human preferences is
critical to guarantee a useful, safe, and pleasant user experience. Thus, human
alignment has been extensively studied recently and several methods such as
Reinforcement Learning from Human Feedback (RLHF), Direct Policy Optimisation
(DPO) and Sequence Likelihood Calibration (SLiC) have emerged. In this paper,
our contribution is two-fold. First, we show the equivalence between two recent
alignment methods, namely Identity Policy Optimisation (IPO) and Nash Mirror
Descent (Nash-MD). Second, we introduce a generalisation of IPO, named IPO-MD,
that leverages the regularised sampling approach proposed by Nash-MD.
  This equivalence may seem surprising at first sight, since IPO is an offline
method whereas Nash-MD is an online method using a preference model. However,
this equivalence can be proven when we consider the online version of IPO, that
is when both generations are sampled by the online policy and annotated by a
trained preference model. Optimising the IPO loss with such a stream of data
becomes then equivalent to finding the Nash equilibrium of the preference model
through self-play. Building on this equivalence, we introduce the IPO-MD
algorithm that generates data with a mixture policy (between the online and
reference policy) similarly as the general Nash-MD algorithm. We compare
online-IPO and IPO-MD to different online versions of existing losses on
preference data such as DPO and SLiC on a summarisation task.",0
A Decade's Battle on Dataset Bias: Are We There Yet?,2403.08632v1,http://arxiv.org/abs/2403.08632v1,2024-03-13 15:46:37+00:00,"We revisit the ""dataset classification"" experiment suggested by Torralba and
Efros a decade ago, in the new era with large-scale, diverse, and hopefully
less biased datasets as well as more capable neural network architectures.
Surprisingly, we observe that modern neural networks can achieve excellent
accuracy in classifying which dataset an image is from: e.g., we report 84.7%
accuracy on held-out validation data for the three-way classification problem
consisting of the YFCC, CC, and DataComp datasets. Our further experiments show
that such a dataset classifier could learn semantic features that are
generalizable and transferable, which cannot be simply explained by
memorization. We hope our discovery will inspire the community to rethink the
issue involving dataset bias and model capabilities.",0
Leveraging Non-Decimated Wavelet Packet Features and Transformer Models for Time Series Forecasting,2403.08630v1,http://arxiv.org/abs/2403.08630v1,2024-03-13 15:45:29+00:00,"This article combines wavelet analysis techniques with machine learning
methods for univariate time series forecasting, focusing on three main
contributions. Firstly, we consider the use of Daubechies wavelets with
different numbers of vanishing moments as input features to both non-temporal
and temporal forecasting methods, by selecting these numbers during the
cross-validation phase. Secondly, we compare the use of both the non-decimated
wavelet transform and the non-decimated wavelet packet transform for computing
these features, the latter providing a much larger set of potentially useful
coefficient vectors. The wavelet coefficients are computed using a shifted
version of the typical pyramidal algorithm to ensure no leakage of future
information into these inputs. Thirdly, we evaluate the use of these wavelet
features on a significantly wider set of forecasting methods than previous
studies, including both temporal and non-temporal models, and both statistical
and deep learning-based methods. The latter include state-of-the-art
transformer-based neural network architectures. Our experiments suggest
significant benefit in replacing higher-order lagged features with wavelet
features across all examined non-temporal methods for one-step-forward
forecasting, and modest benefit when used as inputs for temporal deep
learning-based models for long-horizon forecasting.",0
Optimal sub-Gaussian variance proxy for truncated Gaussian and exponential random variables,2403.08628v1,http://arxiv.org/abs/2403.08628v1,2024-03-13 15:41:20+00:00,"This paper establishes the optimal sub-Gaussian variance proxy for truncated
Gaussian and truncated exponential random variables. The proofs rely on first
characterizing the optimal variance proxy as the unique solution to a set of
two equations and then observing that for these two truncated distributions,
one may find explicit solutions to this set of equations. Moreover, we
establish the conditions under which the optimal variance proxy coincides with
the variance, thereby characterizing the strict sub-Gaussianity of the
truncated random variables. Specifically, we demonstrate that truncated
Gaussian variables exhibit strict sub-Gaussian behavior if and only if they are
symmetric, meaning their truncation is symmetric with respect to the mean.
Conversely, truncated exponential variables are shown to never exhibit strict
sub-Gaussian properties. These findings contribute to the understanding of
these prevalent probability distributions in statistics and machine learning,
providing a valuable foundation for improved and optimal modeling and
decision-making processes.",0
Multifidelity linear regression for scientific machine learning from scarce data,2403.08627v1,http://arxiv.org/abs/2403.08627v1,2024-03-13 15:40:17+00:00,"Machine learning (ML) methods, which fit to data the parameters of a given
parameterized model class, have garnered significant interest as potential
methods for learning surrogate models for complex engineering systems for which
traditional simulation is expensive. However, in many scientific and
engineering settings, generating high-fidelity data on which to train ML models
is expensive, and the available budget for generating training data is limited.
ML models trained on the resulting scarce high-fidelity data have high variance
and are sensitive to vagaries of the training data set. We propose a new
multifidelity training approach for scientific machine learning that exploits
the scientific context where data of varying fidelities and costs are
available; for example high-fidelity data may be generated by an expensive
fully resolved physics simulation whereas lower-fidelity data may arise from a
cheaper model based on simplifying assumptions. We use the multifidelity data
to define new multifidelity Monte Carlo estimators for the unknown parameters
of linear regression models, and provide theoretical analyses that guarantee
the approach's accuracy and improved robustness to small training budgets.
Numerical results verify the theoretical analysis and demonstrate that
multifidelity learned models trained on scarce high-fidelity data and
additional low-fidelity data achieve order-of-magnitude lower model variance
than standard models trained on only high-fidelity data of comparable cost.
This illustrates that in the scarce data regime, our multifidelity training
strategy yields models with lower expected error than standard training
approaches.",0
Measurements of the charge ratio and polarization of cosmic-ray muons with the Super-Kamiokande detector,2403.08619v1,http://arxiv.org/abs/2403.08619v1,2024-03-13 15:32:26+00:00,"We present the results of the charge ratio ($R$) and polarization
($P^{\mu}_{0}$) measurements using the decay electron events collected from
2008 September to 2022 June by the Super-Kamiokande detector. Because of its
underground location and long operation, we performed high precision
measurements by accumulating cosmic-ray muons. We measured the muon charge
ratio to be $R=1.32 \pm 0.02$ $(\mathrm{stat.}{+}\mathrm{syst.})$ at
$E_{\mu}\cos \theta_{\mathrm{Zenith}}=0.7^{+0.3}_{-0.2}$ $\mathrm{TeV}$, where
$E_{\mu}$ is the muon energy and $\theta_{\mathrm{Zenith}}$ is the zenith angle
of incoming cosmic-ray muons. This result is consistent with the Honda flux
model while this suggests a tension with the $\pi K$ model of $1.9\sigma$. We
also measured the muon polarization at the production location to be
$P^{\mu}_{0}=0.52 \pm 0.02$ $(\mathrm{stat.}{+}\mathrm{syst.})$ at the muon
momentum of $0.9^{+0.6}_{-0.1}$ $\mathrm{TeV}/c$ at the surface of the
mountain; this also suggests a tension with the Honda flux model of
$1.5\sigma$. This is the most precise measurement ever to experimentally
determine the cosmic-ray muon polarization near $1~\mathrm{TeV}/c$. These
measurement results are useful to improve the atmospheric neutrino simulations.",0
Verifix: Post-Training Correction to Improve Label Noise Robustness with Verified Samples,2403.08618v1,http://arxiv.org/abs/2403.08618v1,2024-03-13 15:32:08+00:00,"Label corruption, where training samples have incorrect labels, can
significantly degrade the performance of machine learning models. This
corruption often arises from non-expert labeling or adversarial attacks.
Acquiring large, perfectly labeled datasets is costly, and retraining large
models from scratch when a clean dataset becomes available is computationally
expensive. To address this challenge, we propose Post-Training Correction, a
new paradigm that adjusts model parameters after initial training to mitigate
label noise, eliminating the need for retraining. We introduce Verifix, a novel
Singular Value Decomposition (SVD) based algorithm that leverages a small,
verified dataset to correct the model weights using a single update. Verifix
uses SVD to estimate a Clean Activation Space and then projects the model's
weights onto this space to suppress activations corresponding to corrupted
data. We demonstrate Verifix's effectiveness on both synthetic and real-world
label noise. Experiments on the CIFAR dataset with 25% synthetic corruption
show 7.36% generalization improvements on average. Additionally, we observe
generalization improvements of up to 2.63% on naturally corrupted datasets like
WebVision1.0 and Clothing1M.",0
Link Prediction for Social Networks using Representation Learning and Heuristic-based Features,2403.08613v1,http://arxiv.org/abs/2403.08613v1,2024-03-13 15:23:55+00:00,"The exponential growth in scale and relevance of social networks enable them
to provide expansive insights. Predicting missing links in social networks
efficiently can help in various modern-day business applications ranging from
generating recommendations to influence analysis. Several categories of
solutions exist for the same. Here, we explore various feature extraction
techniques to generate representations of nodes and edges in a social network
that allow us to predict missing links. We compare the results of using ten
feature extraction techniques categorized across Structural embeddings,
Neighborhood-based embeddings, Graph Neural Networks, and Graph Heuristics,
followed by modeling with ensemble classifiers and custom Neural Networks.
Further, we propose combining heuristic-based features and learned
representations that demonstrate improved performance for the link prediction
task on social network datasets. Using this method to generate accurate
recommendations for many applications is a matter of further study that appears
very promising. The code for all the experiments has been made public.",0
On the Convergence of Locally Adaptive and Scalable Diffusion-Based Sampling Methods for Deep Bayesian Neural Network Posteriors,2403.08609v2,http://arxiv.org/abs/2403.08609v2,2024-03-13 15:21:14+00:00,"Achieving robust uncertainty quantification for deep neural networks
represents an important requirement in many real-world applications of deep
learning such as medical imaging where it is necessary to assess the
reliability of a neural network's prediction. Bayesian neural networks are a
promising approach for modeling uncertainties in deep neural networks.
Unfortunately, generating samples from the posterior distribution of neural
networks is a major challenge. One significant advance in that direction would
be the incorporation of adaptive step sizes, similar to modern neural network
optimizers, into Monte Carlo Markov chain sampling algorithms without
significantly increasing computational demand. Over the past years, several
papers have introduced sampling algorithms with claims that they achieve this
property. However, do they indeed converge to the correct distribution? In this
paper, we demonstrate that these methods can have a substantial bias in the
distribution they sample, even in the limit of vanishing step sizes and at full
batch size.",0
Patching-based Deep Learning model for the Inpainting of Bragg Coherent Diffraction patterns affected by detectors' gaps,2403.08596v1,http://arxiv.org/abs/2403.08596v1,2024-03-13 15:03:13+00:00,"We propose a deep learning algorithm for the inpainting of Bragg Coherent
Diffraction Imaging (BCDI) patterns affected by detector gaps. These regions of
missing intensity can compromise the accuracy of reconstruction algorithms,
inducing artifacts in the final result. It is thus desirable to restore the
intensity in these regions in order to ensure more reliable reconstructions.
The key aspect of our method lies in the choice of training the neural network
with cropped sections of both experimental diffraction data and simulated data
and subsequently patching the predictions generated by the model along the gap,
thus completing the full diffraction peak. This provides us with more
experimental training data and allows for a faster model training due to the
limited size, while the neural network can be applied to arbitrarily larger
BCDI datasets. Moreover, our method not only broadens the scope of application
but also ensures the preservation of data integrity and reliability in the face
of challenging experimental conditions.",0
Data-Efficient Sleep Staging with Synthetic Time Series Pretraining,2403.08592v1,http://arxiv.org/abs/2403.08592v1,2024-03-13 14:57:10+00:00,"Analyzing electroencephalographic (EEG) time series can be challenging,
especially with deep neural networks, due to the large variability among human
subjects and often small datasets. To address these challenges, various
strategies, such as self-supervised learning, have been suggested, but they
typically rely on extensive empirical datasets. Inspired by recent advances in
computer vision, we propose a pretraining task termed ""frequency pretraining""
to pretrain a neural network for sleep staging by predicting the frequency
content of randomly generated synthetic time series. Our experiments
demonstrate that our method surpasses fully supervised learning in scenarios
with limited data and few subjects, and matches its performance in regimes with
many subjects. Furthermore, our results underline the relevance of frequency
information for sleep stage scoring, while also demonstrating that deep neural
networks utilize information beyond frequencies to enhance sleep staging
performance, which is consistent with previous research. We anticipate that our
approach will be advantageous across a broad spectrum of applications where EEG
data is limited or derived from a small number of subjects, including the
domain of brain-computer interfaces.",0
ActionDiffusion: An Action-aware Diffusion Model for Procedure Planning in Instructional Videos,2403.08591v1,http://arxiv.org/abs/2403.08591v1,2024-03-13 14:54:04+00:00,"We present ActionDiffusion -- a novel diffusion model for procedure planning
in instructional videos that is the first to take temporal inter-dependencies
between actions into account in a diffusion model for procedure planning. This
approach is in stark contrast to existing methods that fail to exploit the rich
information content available in the particular order in which actions are
performed. Our method unifies the learning of temporal dependencies between
actions and denoising of the action plan in the diffusion process by projecting
the action information into the noise space. This is achieved 1) by adding
action embeddings in the noise masks in the noise-adding phase and 2) by
introducing an attention mechanism in the noise prediction network to learn the
correlations between different action steps. We report extensive experiments on
three instructional video benchmark datasets (CrossTask, Coin, and NIV) and
show that our method outperforms previous state-of-the-art methods on all
metrics on CrossTask and NIV and all metrics except accuracy on Coin dataset.
We show that by adding action embeddings into the noise mask the diffusion
model can better learn action temporal dependencies and increase the
performances on procedure planning.",0
Can physical information aid the generalization ability of Neural Networks for hydraulic modeling?,2403.08589v1,http://arxiv.org/abs/2403.08589v1,2024-03-13 14:51:16+00:00,"Application of Neural Networks to river hydraulics is fledgling, despite the
field suffering from data scarcity, a challenge for machine learning
techniques. Consequently, many purely data-driven Neural Networks proved to
lack predictive capabilities. In this work, we propose to mitigate such problem
by introducing physical information into the training phase. The idea is
borrowed from Physics-Informed Neural Networks which have been recently
proposed in other contexts. Physics-Informed Neural Networks embed physical
information in the form of the residual of the Partial Differential Equations
(PDEs) governing the phenomenon and, as such, are conceived as neural solvers,
i.e. an alternative to traditional numerical solvers. Such approach is seldom
suitable for environmental hydraulics, where epistemic uncertainties are large,
and computing residuals of PDEs exhibits difficulties similar to those faced by
classical numerical methods. Instead, we envisaged the employment of Neural
Networks as neural operators, featuring physical constraints formulated without
resorting to PDEs. The proposed novel methodology shares similarities with data
augmentation and regularization. We show that incorporating such soft physical
information can improve predictive capabilities.",0
Improving Implicit Regularization of SGD with Preconditioning for Least Square Problems,2403.08585v1,http://arxiv.org/abs/2403.08585v1,2024-03-13 14:42:06+00:00,"Stochastic gradient descent (SGD) exhibits strong algorithmic regularization
effects in practice and plays an important role in the generalization of modern
machine learning. However, prior research has revealed instances where the
generalization performance of SGD is worse than ridge regression due to uneven
optimization along different dimensions. Preconditioning offers a natural
solution to this issue by rebalancing optimization across different directions.
Yet, the extent to which preconditioning can enhance the generalization
performance of SGD and whether it can bridge the existing gap with ridge
regression remains uncertain. In this paper, we study the generalization
performance of SGD with preconditioning for the least squared problem. We make
a comprehensive comparison between preconditioned SGD and (standard \&
preconditioned) ridge regression. Our study makes several key contributions
toward understanding and improving SGD with preconditioning. First, we
establish excess risk bounds (generalization performance) for preconditioned
SGD and ridge regression under an arbitrary preconditions matrix. Second,
leveraging the excessive risk characterization of preconditioned SGD and ridge
regression, we show that (through construction) there exists a simple
preconditioned matrix that can outperform (standard \& preconditioned) ridge
regression. Finally, we show that our proposed preconditioning matrix is
straightforward enough to allow robust estimation from finite samples while
maintaining a theoretical advantage over ridge regression. Our empirical
results align with our theoretical findings, collectively showcasing the
enhanced regularization effect of preconditioned SGD.",0
Actor-Critic Physics-informed Neural Lyapunov Control,2403.08448v1,http://arxiv.org/abs/2403.08448v1,2024-03-13 12:03:27+00:00,"Designing control policies for stabilization tasks with provable guarantees
is a long-standing problem in nonlinear control. A crucial performance metric
is the size of the resulting region of attraction, which essentially serves as
a robustness ""margin"" of the closed-loop system against uncertainties. In this
paper, we propose a new method to train a stabilizing neural network controller
along with its corresponding Lyapunov certificate, aiming to maximize the
resulting region of attraction while respecting the actuation constraints.
Crucial to our approach is the use of Zubov's Partial Differential Equation
(PDE), which precisely characterizes the true region of attraction of a given
control policy. Our framework follows an actor-critic pattern where we
alternate between improving the control policy (actor) and learning a Zubov
function (critic). Finally, we compute the largest certifiable region of
attraction by invoking an SMT solver after the training procedure. Our
numerical experiments on several design problems show consistent and
significant improvements in the size of the resulting region of attraction.",0
COSTREAM: Learned Cost Models for Operator Placement in Edge-Cloud Environments,2403.08444v1,http://arxiv.org/abs/2403.08444v1,2024-03-13 11:56:10+00:00,"In this work, we present COSTREAM, a novel learned cost model for Distributed
Stream Processing Systems that provides accurate predictions of the execution
costs of a streaming query in an edge-cloud environment. The cost model can be
used to find an initial placement of operators across heterogeneous hardware,
which is particularly important in these environments. In our evaluation, we
demonstrate that COSTREAM can produce highly accurate cost estimates for the
initial operator placement and even generalize to unseen placements, queries,
and hardware. When using COSTREAM to optimize the placements of streaming
operators, a median speed-up of around 21x can be achieved compared to
baselines.",0
Reproducibility and Geometric Intrinsic Dimensionality: An Investigation on Graph Neural Network Research,2403.08438v1,http://arxiv.org/abs/2403.08438v1,2024-03-13 11:44:30+00:00,"Difficulties in replication and reproducibility of empirical evidences in
machine learning research have become a prominent topic in recent years.
Ensuring that machine learning research results are sound and reliable requires
reproducibility, which verifies the reliability of research findings using the
same code and data. This promotes open and accessible research, robust
experimental workflows, and the rapid integration of new findings. Evaluating
the degree to which research publications support these different aspects of
reproducibility is one goal of the present work. For this we introduce an
ontology of reproducibility in machine learning and apply it to methods for
graph neural networks. Building on these efforts we turn towards another
critical challenge in machine learning, namely the curse of dimensionality,
which poses challenges in data collection, representation, and analysis, making
it harder to find representative data and impeding the training and inference
processes. Using the closely linked concept of geometric intrinsic dimension we
investigate to which extend the used machine learning models are influenced by
the intrinsic dimension of the data sets they are trained on.",0
Structural Positional Encoding for knowledge integration in transformer-based medical process monitoring,2403.08836v1,http://arxiv.org/abs/2403.08836v1,2024-03-13 08:15:18+00:00,"Predictive process monitoring is a process mining task aimed at forecasting
information about a running process trace, such as the most correct next
activity to be executed. In medical domains, predictive process monitoring can
provide valuable decision support in atypical and nontrivial situations.
Decision support and quality assessment in medicine cannot ignore domain
knowledge, in order to be grounded on all the available information (which is
not limited to data) and to be really acceptable by end users.
  In this paper, we propose a predictive process monitoring approach relying on
the use of a {\em transformer}, a deep learning architecture based on the
attention mechanism. A major contribution of our work lies in the incorporation
of ontological domain-specific knowledge, carried out through a graph
positional encoding technique. The paper presents and discusses the encouraging
experimental result we are collecting in the domain of stroke management.",0
Stacking-based deep neural network for player scouting in football 1,2403.08835v1,http://arxiv.org/abs/2403.08835v1,2024-03-13 08:10:18+00:00,"Datascouting is one of the most known data applications in professional
sport, and specifically football. Its objective is to analyze huge database of
players in order to detect high potentials that can be then individually
considered by human scouts. In this paper, we propose a stacking-based deep
learning model to detect high potential football players. Applied on
open-source database, our model obtains significantly better results that
classical statistical methods.",0
ManiGaussian: Dynamic Gaussian Splatting for Multi-task Robotic Manipulation,2403.08321v1,http://arxiv.org/abs/2403.08321v1,2024-03-13 08:06:41+00:00,"Performing language-conditioned robotic manipulation tasks in unstructured
environments is highly demanded for general intelligent robots. Conventional
robotic manipulation methods usually learn semantic representation of the
observation for action prediction, which ignores the scene-level spatiotemporal
dynamics for human goal completion. In this paper, we propose a dynamic
Gaussian Splatting method named ManiGaussian for multi-task robotic
manipulation, which mines scene dynamics via future scene reconstruction.
Specifically, we first formulate the dynamic Gaussian Splatting framework that
infers the semantics propagation in the Gaussian embedding space, where the
semantic representation is leveraged to predict the optimal robot action. Then,
we build a Gaussian world model to parameterize the distribution in our dynamic
Gaussian Splatting framework, which provides informative supervision in the
interactive environment via future scene reconstruction. We evaluate our
ManiGaussian on 10 RLBench tasks with 166 variations, and the results
demonstrate our framework can outperform the state-of-the-art methods by 13.1\%
in average success rate.",0
Predictive Analysis of Tuberculosis Treatment Outcomes Using Machine Learning: A Karnataka TB Data Study at a Scale,2403.08834v1,http://arxiv.org/abs/2403.08834v1,2024-03-13 08:04:00+00:00,"Tuberculosis (TB) remains a global health threat, ranking among the leading
causes of mortality worldwide. In this context, machine learning (ML) has
emerged as a transformative force, providing innovative solutions to the
complexities associated with TB treatment.This study explores how machine
learning, especially with tabular data, can be used to predict Tuberculosis
(TB) treatment outcomes more accurately. It transforms this prediction task
into a binary classification problem, generating risk scores from patient data
sourced from NIKSHAY, India's national TB control program, which includes over
500,000 patient records.
  Data preprocessing is a critical component of the study, and the model
achieved an recall of 98% and an AUC-ROC score of 0.95 on the validation set,
which includes 20,000 patient records.We also explore the use of Natural
Language Processing (NLP) for improved model learning. Our results,
corroborated by various metrics and ablation studies, validate the
effectiveness of our approach. The study concludes by discussing the potential
ramifications of our research on TB eradication efforts and proposing potential
avenues for future work. This study marks a significant stride in the battle
against TB, showcasing the potential of machine learning in healthcare.",0
Knowledge Conflicts for LLMs: A Survey,2403.08319v1,http://arxiv.org/abs/2403.08319v1,2024-03-13 08:02:23+00:00,"This survey provides an in-depth analysis of knowledge conflicts for large
language models (LLMs), highlighting the complex challenges they encounter when
blending contextual and parametric knowledge. Our focus is on three categories
of knowledge conflicts: context-memory, inter-context, and intra-memory
conflict. These conflicts can significantly impact the trustworthiness and
performance of LLMs, especially in real-world applications where noise and
misinformation are common. By categorizing these conflicts, exploring the
causes, examining the behaviors of LLMs under such conflicts, and reviewing
available solutions, this survey aims to shed light on strategies for improving
the robustness of LLMs, thereby serving as a valuable resource for advancing
research in this evolving area.",0
DrFER: Learning Disentangled Representations for 3D Facial Expression Recognition,2403.08318v1,http://arxiv.org/abs/2403.08318v1,2024-03-13 08:00:07+00:00,"Facial Expression Recognition (FER) has consistently been a focal point in
the field of facial analysis. In the context of existing methodologies for 3D
FER or 2D+3D FER, the extraction of expression features often gets entangled
with identity information, compromising the distinctiveness of these features.
To tackle this challenge, we introduce the innovative DrFER method, which
brings the concept of disentangled representation learning to the field of 3D
FER. DrFER employs a dual-branch framework to effectively disentangle
expression information from identity information. Diverging from prior
disentanglement endeavors in the 3D facial domain, we have carefully
reconfigured both the loss functions and network structure to make the overall
framework adaptable to point cloud data. This adaptation enhances the
capability of the framework in recognizing facial expressions, even in cases
involving varying head poses. Extensive evaluations conducted on the BU-3DFE
and Bosphorus datasets substantiate that DrFER surpasses the performance of
other 3D FER methods.",0
From Channel Measurement to Training Data for PHY Layer AI Applications,2403.08317v1,http://arxiv.org/abs/2403.08317v1,2024-03-13 07:54:01+00:00,"Learning-based techniques such as artificial intelligence (AI) and machine
learning (ML) play an increasingly important role in the development of future
communication networks. The success of a learning algorithm depends on the
quality and quantity of the available training data. In the physical layer
(PHY), channel information data can be obtained either through measurement
campaigns or through simulations based on predefined channel models. Performing
measurements can be time consuming while only gaining information about one
specific position or scenario. Simulated data, on the other hand, are more
generalized and reflect in most cases not a real environment but instead, a
statistical approximation based on a mathematical model. This paper presents a
procedure for acquiring channel data by means of fast and flexible software
defined radio (SDR) based channel measurements along with a method for a
parameter extraction that provides configuration input to the simulator. The
procedure from the measurement to the simulated channel data is demonstrated in
two exemplary propagation scenarios. It is shown, that in both cases the
simulated data is in good accordance to the measurements",0
Is Context Helpful for Chat Translation Evaluation?,2403.08314v1,http://arxiv.org/abs/2403.08314v1,2024-03-13 07:49:50+00:00,"Despite the recent success of automatic metrics for assessing translation
quality, their application in evaluating the quality of machine-translated
chats has been limited. Unlike more structured texts like news, chat
conversations are often unstructured, short, and heavily reliant on contextual
information. This poses questions about the reliability of existing
sentence-level metrics in this domain as well as the role of context in
assessing the translation quality. Motivated by this, we conduct a
meta-evaluation of existing sentence-level automatic metrics, primarily
designed for structured domains such as news, to assess the quality of
machine-translated chats. We find that reference-free metrics lag behind
reference-based ones, especially when evaluating translation quality in
out-of-English settings. We then investigate how incorporating conversational
contextual information in these metrics affects their performance. Our findings
show that augmenting neural learned metrics with contextual information helps
improve correlation with human judgments in the reference-free scenario and
when evaluating translations in out-of-English settings. Finally, we propose a
new evaluation metric, Context-MQM, that utilizes bilingual context with a
large language model (LLM) and further validate that adding context helps even
for LLM-based evaluation metrics.",0
StreamingDialogue: Prolonged Dialogue Learning via Long Context Compression with Minimal Losses,2403.08312v1,http://arxiv.org/abs/2403.08312v1,2024-03-13 07:44:14+00:00,"Standard Large Language Models (LLMs) struggle with handling dialogues with
long contexts due to efficiency and consistency issues. According to our
observation, dialogue contexts are highly structured, and the special token of
\textit{End-of-Utterance} (EoU) in dialogues has the potential to aggregate
information. We refer to the EoU tokens as ``conversational attention sinks''
(conv-attn sinks). Accordingly, we introduce StreamingDialogue, which
compresses long dialogue history into conv-attn sinks with minimal losses, and
thus reduces computational complexity quadratically with the number of sinks
(i.e., the number of utterances). Current LLMs already demonstrate the ability
to handle long context window, e.g., a window size of 200k or more. To this
end, by compressing utterances into EoUs, our method has the potential to
handle more than 200k of utterances, resulting in a prolonged dialogue
learning. In order to minimize information losses from reconstruction after
compression, we design two learning strategies of short-memory reconstruction
(SMR) and long-memory reactivation (LMR). Our method outperforms strong
baselines in dialogue tasks and achieves a 4 $\times$ speedup while reducing
memory usage by 18 $\times$ compared to dense attention recomputation.",0
When Code Smells Meet ML: On the Lifecycle of ML-specific Code Smells in ML-enabled Systems,2403.08311v1,http://arxiv.org/abs/2403.08311v1,2024-03-13 07:43:45+00:00,"Context. The adoption of Machine Learning (ML)--enabled systems is steadily
increasing. Nevertheless, there is a shortage of ML-specific quality assurance
approaches, possibly because of the limited knowledge of how quality-related
concerns emerge and evolve in ML-enabled systems. Objective. We aim to
investigate the emergence and evolution of specific types of quality-related
concerns known as ML-specific code smells, i.e., sub-optimal implementation
solutions applied on ML pipelines that may significantly decrease both the
quality and maintainability of ML-enabled systems. More specifically, we
present a plan to study ML-specific code smells by empirically analyzing (i)
their prevalence in real ML-enabled systems, (ii) how they are introduced and
removed, and (iii) their survivability. Method. We will conduct an exploratory
study, mining a large dataset of ML-enabled systems and analyzing over 400k
commits about 337 projects. We will track and inspect the introduction and
evolution of ML smells through CodeSmile, a novel ML smell detector that we
will build to enable our investigation and to detect ML-specific code smells.",0
StyleDyRF: Zero-shot 4D Style Transfer for Dynamic Neural Radiance Fields,2403.08310v1,http://arxiv.org/abs/2403.08310v1,2024-03-13 07:42:21+00:00,"4D style transfer aims at transferring arbitrary visual style to the
synthesized novel views of a dynamic 4D scene with varying viewpoints and
times. Existing efforts on 3D style transfer can effectively combine the visual
features of style images and neural radiance fields (NeRF) but fail to handle
the 4D dynamic scenes limited by the static scene assumption. Consequently, we
aim to handle the novel challenging problem of 4D style transfer for the first
time, which further requires the consistency of stylized results on dynamic
objects. In this paper, we introduce StyleDyRF, a method that represents the 4D
feature space by deforming a canonical feature volume and learns a linear style
transformation matrix on the feature volume in a data-driven fashion. To obtain
the canonical feature volume, the rays at each time step are deformed with the
geometric prior of a pre-trained dynamic NeRF to render the feature map under
the supervision of pre-trained visual encoders. With the content and style cues
in the canonical feature volume and the style image, we can learn the style
transformation matrix from their covariance matrices with lightweight neural
networks. The learned style transformation matrix can reflect a direct matching
of feature covariance from the content volume to the given style pattern, in
analogy with the optimization of the Gram matrix in traditional 2D neural style
transfer. The experimental results show that our method not only renders 4D
photorealistic style transfer results in a zero-shot manner but also
outperforms existing methods in terms of visual quality and consistency.",0
HRLAIF: Improvements in Helpfulness and Harmlessness in Open-domain Reinforcement Learning From AI Feedback,2403.08309v2,http://arxiv.org/abs/2403.08309v2,2024-03-13 07:38:20+00:00,"Reinforcement Learning from AI Feedback (RLAIF) has the advantages of shorter
annotation cycles and lower costs over Reinforcement Learning from Human
Feedback (RLHF), making it highly efficient during the rapid strategy iteration
periods of large language model (LLM) training. Using ChatGPT as a labeler to
provide feedback on open-domain prompts in RLAIF training, we observe an
increase in human evaluators' preference win ratio for model responses, but a
decrease in evaluators' satisfaction rate. Analysis suggests that the decrease
in satisfaction rate is mainly due to some responses becoming less helpful,
particularly in terms of correctness and truthfulness, highlighting practical
limitations of basic RLAIF. In this paper, we propose Hybrid Reinforcement
Learning from AI Feedback (HRLAIF). This method enhances the accuracy of AI
annotations for responses, making the model's helpfulness more robust in
training process. Additionally, it employs AI for Red Teaming, further
improving the model's harmlessness. Human evaluation results show that HRLAIF
inherits the ability of RLAIF to enhance human preference for outcomes at a low
cost while also improving the satisfaction rate of responses. Compared to the
policy model before Reinforcement Learning (RL), it achieves an increase of
2.08\% in satisfaction rate, effectively addressing the issue of a decrease of
4.58\% in satisfaction rate after basic RLAIF.",0
Probing the stellar populations and star formation history of early-type galaxies at $0 < z < 1.1$ in the rest-frame ultraviolet,2403.08301v1,http://arxiv.org/abs/2403.08301v1,2024-03-13 07:19:11+00:00,"We measure the evolution of the rest-frame $NUV-V$ colors for early-type
galaxies in clusters at $0<z<1.1$ using data from the Hyper Suprime-Cam Subaru
Strategic Program (HSC-SSP), CFHT Large Area U-band Deep Survey (CLAUDS) and
local SDSS clusters observed with GALEX. Our results show that there is an
excess in the ultraviolet spectrum in most quiescent galaxies (compared to the
expectations from models fitting their optical/infrared colors and spectra)
below $z\sim0.6$, beyond which the excess UV emission fades rapidly. This
evolution of the UV color is only consistent with the presence of a highly
evolved, hot horizontal branch sub-population in these galaxies (amongst the
majority cool and optically bright stars), comprising on average 10\% of the
total stellar mass and forming at $z>3$. The blue UV colors of early-type
galaxies at low-intermediate redshifts are likely driven by this sub-population
being enriched in helium up to $\sim44\%$. At $z>0.8$ (when the extra UV
component has not yet appeared) the data allows us to constrain the star
formation histories of galaxies by fitting models to the evolution of their UV
colors: we find that the epoch at which the stellar populations formed ranges
between $3<z_{form}<10$ (corresponding to $0.5-2.2$ Gyrs after the Big Bang)
with a star-formation e-folding timescale of $\tau=0.35-0.7$ Gyr, suggesting
that these galaxies formed the majority of stars at very high redshift, with a
brief yet intense burst of star-formation activity. The star formation history
and chemical evolution of early-type galaxies resemble those of globular
clusters, albeit on much larger scales.",0
Physics-Informed Deep Learning for Motion-Corrected Reconstruction of Quantitative Brain MRI,2403.08298v1,http://arxiv.org/abs/2403.08298v1,2024-03-13 07:10:24+00:00,"We propose PHIMO, a physics-informed learning-based motion correction method
tailored to quantitative MRI. PHIMO leverages information from the signal
evolution to exclude motion-corrupted k-space lines from a data-consistent
reconstruction. We demonstrate the potential of PHIMO for the application of
T2* quantification from gradient echo MRI, which is particularly sensitive to
motion due to its sensitivity to magnetic field inhomogeneities. A
state-of-the-art technique for motion correction requires redundant acquisition
of the k-space center, prolonging the acquisition. We show that PHIMO can
detect and exclude intra-scan motion events and, thus, correct for severe
motion artifacts. PHIMO approaches the performance of the state-of-the-art
motion correction method, while substantially reducing the acquisition time by
over 40%, facilitating clinical applicability. Our code is available at
https://github.com/HannahEichhorn/PHIMO.",0
CleanAgent: Automating Data Standardization with LLM-based Agents,2403.08291v1,http://arxiv.org/abs/2403.08291v1,2024-03-13 06:54:15+00:00,"Data standardization is a crucial part in data science life cycle. While
tools like Pandas offer robust functionalities, their complexity and the manual
effort required for customizing code to diverse column types pose significant
challenges. Although large language models (LLMs) like ChatGPT have shown
promise in automating this process through natural language understanding and
code generation, it still demands expert-level programming knowledge and
continuous interaction for prompt refinement. To solve these challenges, our
key idea is to propose a Python library with declarative, unified APIs for
standardizing column types, simplifying the code generation of LLM with concise
API calls. We first propose Dataprep.Clean which is written as a component of
the Dataprep Library, offers a significant reduction in complexity by enabling
the standardization of specific column types with a single line of code. Then
we introduce the CleanAgent framework integrating Dataprep.Clean and LLM-based
agents to automate the data standardization process. With CleanAgent, data
scientists need only provide their requirements once, allowing for a
hands-free, automatic standardization process.",0
Performance assessment of the effective core potentials under the Fermionic neural network: first and second row elements,2403.08287v1,http://arxiv.org/abs/2403.08287v1,2024-03-13 06:48:05+00:00,"The rapid development of deep learning techniques has driven the emergence of
a neural network-based variational Monte Carlo method (referred to as
FermiNet), which has manifested high accuracy and strong predictive power in
the electronic structure calculations of atoms, molecules as well as some
periodic systems. Recently, the implementation of the effective core potential
(ECP) scheme in it further facilitates more efficient calculations in practice.
But there still lack a more comprehensive assessment on the ECP's performance
under the FermiNet. In this work, we set sail to fill this gap by conducting
extensive tests on the first two row elements regarding their atomic spectral
and molecular properties. Our major finding is that in general the qualities of
ECPs have been correctly reflected under the FermiNet, and a more recently
built ECP, ccECP, seems to prevail on the overall performance. Meanwhile, a
variation of the transferability between different ECPs has also been observed
in the results of the hydrides. On the other hand, the high accuracy of the
all-electron calculations is hindered by the absence of relativistic effects as
one gets to the second row. Meanwhile the numerical instabilities are more
often seen in the all-electron calculations, which could be another source of
errors. Finally, with more in-depth discussions, we generate possible
directions for developing and improving the FermiNet in the near future.",0
MGIC: A Multi-Label Gradient Inversion Attack based on Canny Edge Detection on Federated Learning,2403.08284v1,http://arxiv.org/abs/2403.08284v1,2024-03-13 06:34:49+00:00,"As a new distributed computing framework that can protect data privacy,
federated learning (FL) has attracted more and more attention in recent years.
It receives gradients from users to train the global model and releases the
trained global model to working users. Nonetheless, the gradient inversion (GI)
attack reflects the risk of privacy leakage in federated learning. Attackers
only need to use gradients through hundreds of thousands of simple iterations
to obtain relatively accurate private data stored on users' local devices. For
this, some works propose simple but effective strategies to obtain user data
under a single-label dataset. However, these strategies induce a satisfactory
visual effect of the inversion image at the expense of higher time costs. Due
to the semantic limitation of a single label, the image obtained by gradient
inversion may have semantic errors. We present a novel gradient inversion
strategy based on canny edge detection (MGIC) in both the multi-label and
single-label datasets. To reduce semantic errors caused by a single label, we
add new convolution layers' blocks in the trained model to obtain the image's
multi-label. Through multi-label representation, serious semantic errors in
inversion images are reduced. Then, we analyze the impact of parameters on the
difficulty of input image reconstruction and discuss how image multi-subjects
affect the inversion performance. Our proposed strategy has better visual
inversion image results than the most widely used ones, saving more than 78% of
time costs in the ImageNet dataset.",0
Optimized Detection and Classification on GTRSB: Advancing Traffic Sign Recognition with Convolutional Neural Networks,2403.08283v1,http://arxiv.org/abs/2403.08283v1,2024-03-13 06:28:37+00:00,"In the rapidly evolving landscape of transportation, the proliferation of
automobiles has made road traffic more complex, necessitating advanced
vision-assisted technologies for enhanced safety and navigation. These
technologies are imperative for providing critical traffic sign information,
influencing driver behavior, and supporting vehicle control, especially for
drivers with disabilities and in the burgeoning field of autonomous vehicles.
Traffic sign detection and recognition have emerged as key areas of research
due to their essential roles in ensuring road safety and compliance with
traffic regulations. Traditional computer vision methods have faced challenges
in achieving optimal accuracy and speed due to real-world variabilities.
However, the advent of deep learning and Convolutional Neural Networks (CNNs)
has revolutionized this domain, offering solutions that significantly surpass
previous capabilities in terms of speed and reliability. This paper presents an
innovative approach leveraging CNNs that achieves an accuracy of nearly 96\%,
highlighting the potential for even greater precision through advanced
localization techniques. Our findings not only contribute to the ongoing
advancement of traffic sign recognition technology but also underscore the
critical impact of these developments on road safety and the future of
autonomous driving.",0
VIGFace: Virtual Identity Generation Model for Face Image Synthesis,2403.08277v1,http://arxiv.org/abs/2403.08277v1,2024-03-13 06:11:41+00:00,"Deep learning-based face recognition continues to face challenges due to its
reliance on huge datasets obtained from web crawling, which can be costly to
gather and raise significant real-world privacy concerns. To address this
issue, we propose VIGFace, a novel framework capable of generating synthetic
facial images. Initially, we train the face recognition model using a real face
dataset and create a feature space for both real and virtual IDs where virtual
prototypes are orthogonal to other prototypes. Subsequently, we generate
synthetic images by using the diffusion model based on the feature space. Our
proposed framework provides two significant benefits. Firstly, it allows for
creating virtual facial images without concerns about portrait rights,
guaranteeing that the generated virtual face images are clearly differentiated
from existing individuals. Secondly, it serves as an effective augmentation
method by incorporating real existing images. Further experiments demonstrate
the efficacy of our framework, achieving state-of-the-art results from both
perspectives without any external data.",0
Efficient Prompt Tuning of Large Vision-Language Model for Fine-Grained Ship Classification,2403.08271v1,http://arxiv.org/abs/2403.08271v1,2024-03-13 05:48:58+00:00,"Fine-grained ship classification in remote sensing (RS-FGSC) poses a
significant challenge due to the high similarity between classes and the
limited availability of labeled data, limiting the effectiveness of traditional
supervised classification methods. Recent advancements in large pre-trained
Vision-Language Models (VLMs) have demonstrated impressive capabilities in
few-shot or zero-shot learning, particularly in understanding image content.
This study delves into harnessing the potential of VLMs to enhance
classification accuracy for unseen ship categories, which holds considerable
significance in scenarios with restricted data due to cost or privacy
constraints. Directly fine-tuning VLMs for RS-FGSC often encounters the
challenge of overfitting the seen classes, resulting in suboptimal
generalization to unseen classes, which highlights the difficulty in
differentiating complex backgrounds and capturing distinct ship features. To
address these issues, we introduce a novel prompt tuning technique that employs
a hierarchical, multi-granularity prompt design. Our approach integrates remote
sensing ship priors through bias terms, learned from a small trainable network.
This strategy enhances the model's generalization capabilities while improving
its ability to discern intricate backgrounds and learn discriminative ship
features. Furthermore, we contribute to the field by introducing a
comprehensive dataset, FGSCM-52, significantly expanding existing datasets with
more extensive data and detailed annotations for less common ship classes.
Extensive experimental evaluations demonstrate the superiority of our proposed
method over current state-of-the-art techniques. The source code will be made
publicly available.",0
Identity-aware Dual-constraint Network for Cloth-Changing Person Re-identification,2403.08270v1,http://arxiv.org/abs/2403.08270v1,2024-03-13 05:46:36+00:00,"Cloth-Changing Person Re-Identification (CC-ReID) aims to accurately identify
the target person in more realistic surveillance scenarios, where pedestrians
usually change their clothing. Despite great progress, limited cloth-changing
training samples in existing CC-ReID datasets still prevent the model from
adequately learning cloth-irrelevant features. In addition, due to the absence
of explicit supervision to keep the model constantly focused on
cloth-irrelevant areas, existing methods are still hampered by the disruption
of clothing variations. To solve the above issues, we propose an Identity-aware
Dual-constraint Network (IDNet) for the CC-ReID task. Specifically, to help the
model extract cloth-irrelevant clues, we propose a Clothes Diversity
Augmentation (CDA), which generates more realistic cloth-changing samples by
enriching the clothing color while preserving the texture. In addition, a
Multi-scale Constraint Block (MCB) is designed, which extracts fine-grained
identity-related features and effectively transfers cloth-irrelevant knowledge.
Moreover, a Counterfactual-guided Attention Module (CAM) is presented, which
learns cloth-irrelevant features from channel and space dimensions and utilizes
the counterfactual intervention for supervising the attention map to highlight
identity-related regions. Finally, a Semantic Alignment Constraint (SAC) is
designed to facilitate high-level semantic feature interaction. Comprehensive
experiments on four CC-ReID datasets indicate that our method outperforms prior
state-of-the-art approaches.",0
SNOW-SCA: ML-assisted Side-Channel Attack on SNOW-V,2403.08267v1,http://arxiv.org/abs/2403.08267v1,2024-03-13 05:35:55+00:00,"This paper presents SNOW-SCA, the first power side-channel analysis (SCA)
attack of a 5G mobile communication security standard candidate, SNOW-V,
running on a 32-bit ARM Cortex-M4 microcontroller. First, we perform a generic
known-key correlation (KKC) analysis to identify the leakage points. Next, a
correlation power analysis (CPA) attack is performed, which reduces the attack
complexity to two key guesses for each key byte. The correct secret key is then
uniquely identified utilizing linear discriminant analysis (LDA). The profiled
SCA attack with LDA achieves 100% accuracy after training with $<200$ traces,
which means the attack succeeds with just a single trace. Overall, using the
\textit{combined CPA and LDA attack} model, the correct secret key byte is
recovered with <50 traces collected using the ChipWhisperer platform. The
entire 256-bit secret key of SNOW-V can be recovered incrementally using the
proposed SCA attack. Finally, we suggest low-overhead countermeasures that can
be used to prevent these SCA attacks.",0
Sketch2Manga: Shaded Manga Screening from Sketch with Diffusion Models,2403.08266v1,http://arxiv.org/abs/2403.08266v1,2024-03-13 05:33:52+00:00,"While manga is a popular entertainment form, creating manga is tedious,
especially adding screentones to the created sketch, namely manga screening.
Unfortunately, there is no existing method that tailors for automatic manga
screening, probably due to the difficulty of generating high-quality shaded
high-frequency screentones. The classic manga screening approaches generally
require user input to provide screentone exemplars or a reference manga image.
The recent deep learning models enables the automatic generation by learning
from a large-scale dataset. However, the state-of-the-art models still fail to
generate high-quality shaded screentones due to the lack of a tailored model
and high-quality manga training data. In this paper, we propose a novel
sketch-to-manga framework that first generates a color illustration from the
sketch and then generates a screentoned manga based on the intensity guidance.
Our method significantly outperforms existing methods in generating
high-quality manga with shaded high-frequency screentones.",0
Random Search as a Baseline for Sparse Neural Network Architecture Search,2403.08265v2,http://arxiv.org/abs/2403.08265v2,2024-03-13 05:32:13+00:00,"Sparse neural networks have shown similar or better generalization
performance than their dense counterparts while having higher parameter
efficiency. This has motivated a number of works to learn or search for high
performing sparse networks. While reports of task performance or efficiency
gains are impressive, standard baselines are lacking leading to poor
comparability and unreliable reproducibility across methods. In this work, we
propose Random Search as a baseline algorithm for finding good sparse
configurations and study its performance. We apply Random Search on the node
space of an overparameterized network with the goal of finding better
initialized sparse sub-networks that are positioned more advantageously in the
loss landscape. We record the post-training performances of the found sparse
networks and at various levels of sparsity, and compare against both their
fully connected parent networks and random sparse configurations at the same
sparsity levels. First, we demonstrate performance at different levels of
sparsity and highlight that a significant level of performance can still be
preserved even when the network is highly sparse. Second, we observe that for
this sparse architecture search task, initialized sparse networks found by
Random Search neither perform better nor converge more efficiently than their
random counterparts. Thus we conclude that Random Search may be viewed as a
reasonable neutral baseline for sparsity search methods.",0
"TINA: Think, Interaction, and Action Framework for Zero-Shot Vision Language Navigation",2403.08833v1,http://arxiv.org/abs/2403.08833v1,2024-03-13 05:22:39+00:00,"Zero-shot navigation is a critical challenge in Vision-Language Navigation
(VLN) tasks, where the ability to adapt to unfamiliar instructions and to act
in unknown environments is essential. Existing supervised learning-based
models, trained using annotated data through reinforcement learning, exhibit
limitations in generalization capabilities. Large Language Models (LLMs), with
their extensive knowledge and emergent reasoning abilities, present a potential
pathway for achieving zero-shot navigation. This paper presents a VLN agent
based on LLMs, exploring approaches to the zero-shot navigation problem. To
compensate for the shortcomings of LLMs in environmental perception, we propose
the Thinking, Interacting, and Action (TINA) framework. TINA enables the agent
to scrutinize perceptual information and autonomously query key clues within
the environment through an introduced question-answering module, thereby
aligning instructions with specific perceptual data. The navigation agent's
perceptual abilities are enhanced through the TINA framework, while the
explicit thought and query processes also improve the navigational procedure's
explainability and transparency. We evaluate the performance of our method on
the Room-to-Room dataset. The experiment results indicate that our approach
improves the navigation performance of LLM-based agents. Our approach also
outperformed some supervised learning-based methods, highlighting its efficacy
in zero-shot navigation.",0
Skipformer: A Skip-and-Recover Strategy for Efficient Speech Recognition,2403.08258v1,http://arxiv.org/abs/2403.08258v1,2024-03-13 05:20:45+00:00,"Conformer-based attention models have become the de facto backbone model for
Automatic Speech Recognition tasks. A blank symbol is usually introduced to
align the input and output sequences for CTC or RNN-T models. Unfortunately,
the long input length overloads computational budget and memory consumption
quadratically by attention mechanism. In this work, we propose a
""Skip-and-Recover"" Conformer architecture, named Skipformer, to squeeze
sequence input length dynamically and inhomogeneously. Skipformer uses an
intermediate CTC output as criteria to split frames into three groups: crucial,
skipping and ignoring. The crucial group feeds into next conformer blocks and
its output joint with skipping group by original temporal order as the final
encoder output. Experiments show that our model reduces the input sequence
length by 31 times on Aishell-1 and 22 times on Librispeech corpus. Meanwhile,
the model can achieve better recognition accuracy and faster inference speed
than recent baseline models. Our code is open-sourced and available online.",0
"Machine Unlearning: Taxonomy, Metrics, Applications, Challenges, and Prospects",2403.08254v1,http://arxiv.org/abs/2403.08254v1,2024-03-13 05:11:24+00:00,"Personal digital data is a critical asset, and governments worldwide have
enforced laws and regulations to protect data privacy. Data users have been
endowed with the right to be forgotten of their data. In the course of machine
learning (ML), the forgotten right requires a model provider to delete user
data and its subsequent impact on ML models upon user requests. Machine
unlearning emerges to address this, which has garnered ever-increasing
attention from both industry and academia. While the area has developed
rapidly, there is a lack of comprehensive surveys to capture the latest
advancements. Recognizing this shortage, we conduct an extensive exploration to
map the landscape of machine unlearning including the (fine-grained) taxonomy
of unlearning algorithms under centralized and distributed settings, debate on
approximate unlearning, verification and evaluation metrics, challenges and
solutions for unlearning under different applications, as well as attacks
targeting machine unlearning. The survey concludes by outlining potential
directions for future research, hoping to serve as a guide for interested
scholars.",0
CoPa: General Robotic Manipulation through Spatial Constraints of Parts with Foundation Models,2403.08248v1,http://arxiv.org/abs/2403.08248v1,2024-03-13 05:03:58+00:00,"Foundation models pre-trained on web-scale data are shown to encapsulate
extensive world knowledge beneficial for robotic manipulation in the form of
task planning. However, the actual physical implementation of these plans often
relies on task-specific learning methods, which require significant data
collection and struggle with generalizability. In this work, we introduce
Robotic Manipulation through Spatial Constraints of Parts (CoPa), a novel
framework that leverages the common sense knowledge embedded within foundation
models to generate a sequence of 6-DoF end-effector poses for open-world
robotic manipulation. Specifically, we decompose the manipulation process into
two phases: task-oriented grasping and task-aware motion planning. In the
task-oriented grasping phase, we employ foundation vision-language models
(VLMs) to select the object's grasping part through a novel coarse-to-fine
grounding mechanism. During the task-aware motion planning phase, VLMs are
utilized again to identify the spatial geometry constraints of task-relevant
object parts, which are then used to derive post-grasp poses. We also
demonstrate how CoPa can be seamlessly integrated with existing robotic
planning algorithms to accomplish complex, long-horizon tasks. Our
comprehensive real-world experiments show that CoPa possesses a fine-grained
physical understanding of scenes, capable of handling open-set instructions and
objects with minimal prompt engineering and without additional training.
Project page: https://copa-2024.github.io/",0
Sparse Bayesian Learning-Based Hierarchical Construction for 3D Radio Environment Maps Incorporating Channel Shadowing,2403.08323v1,http://arxiv.org/abs/2403.08323v1,2024-03-13 08:11:27+00:00,"The radio environment map (REM) visually displays the spectrum information
over the geographical map and plays a significant role in monitoring,
management, and security of spectrum resources.In this paper, we present an
efficient 3D REM construction scheme based on the sparse Bayesian learning
(SBL), which aims to recovery the accurate REM with limited and optimized
sampling data.In order to reduce the number of sampling sensors, an efficient
sparse sampling method for unknown scenarios is proposed. For the given
construction accuracy and the priority of each location, the quantity and
sampling locations can be jointly optimized.With the sparse sampled data, by
mining the spectrum situation sparsity and channel propagation characteristics,
an SBL-based spectrum data hierarchical recovery algorithm is developed to
estimate the missing data of unsampled locations.Finally, the simulated 3D REM
data in the campus scenario are used to verify the proposed methods as well as
to compare with the state-of-the-art. We also analyze the recovery performance
and the impact of different parameters on the constructed REMs. Numerical
results demonstrate that the proposed scheme can ensure the construction
accuracy and improve the computational efficiency under the low sampling rate.",0
Activating Wider Areas in Image Super-Resolution,2403.08330v1,http://arxiv.org/abs/2403.08330v1,2024-03-13 08:29:58+00:00,"The prevalence of convolution neural networks (CNNs) and vision transformers
(ViTs) has markedly revolutionized the area of single-image super-resolution
(SISR). To further boost the SR performances, several techniques, such as
residual learning and attention mechanism, are introduced, which can be largely
attributed to a wider range of activated area, that is, the input pixels that
strongly influence the SR results. However, the possibility of further
improving SR performance through another versatile vision backbone remains an
unresolved challenge. To address this issue, in this paper, we unleash the
representation potential of the modern state space model, i.e., Vision Mamba
(Vim), in the context of SISR. Specifically, we present three recipes for
better utilization of Vim-based models: 1) Integration into a MetaFormer-style
block; 2) Pre-training on a larger and broader dataset; 3) Employing
complementary attention mechanism, upon which we introduce the MMA. The
resulting network MMA is capable of finding the most relevant and
representative input pixels to reconstruct the corresponding high-resolution
images. Comprehensive experimental analysis reveals that MMA not only achieves
competitive or even superior performance compared to state-of-the-art SISR
methods but also maintains relatively low memory and computational overheads
(e.g., +0.5 dB PSNR elevation on Manga109 dataset with 19.8 M parameters at the
scale of 2). Furthermore, MMA proves its versatility in lightweight SR
applications. Through this work, we aim to illuminate the potential
applications of state space models in the broader realm of image processing
rather than SISR, encouraging further exploration in this innovative direction.",0
PFStorer: Personalized Face Restoration and Super-Resolution,2403.08436v1,http://arxiv.org/abs/2403.08436v1,2024-03-13 11:39:30+00:00,"Recent developments in face restoration have achieved remarkable results in
producing high-quality and lifelike outputs. The stunning results however often
fail to be faithful with respect to the identity of the person as the models
lack necessary context. In this paper, we explore the potential of personalized
face restoration with diffusion models. In our approach a restoration model is
personalized using a few images of the identity, leading to tailored
restoration with respect to the identity while retaining fine-grained details.
By using independent trainable blocks for personalization, the rich prior of a
base restoration model can be exploited to its fullest. To avoid the model
relying on parts of identity left in the conditioning low-quality images, a
generative regularizer is employed. With a learnable parameter, the model
learns to balance between the details generated based on the input image and
the degree of personalization. Moreover, we improve the training pipeline of
face restoration models to enable an alignment-free approach. We showcase the
robust capabilities of our approach in several real-world scenarios with
multiple identities, demonstrating our method's ability to generate
fine-grained details with faithful restoration. In the user study we evaluate
the perceptual quality and faithfulness of the genereated details, with our
method being voted best 61% of the time compared to the second best with 25% of
the votes.",0
Bayesian Optimization that Limits Search Region to Lower Dimensions Utilizing Local GPR,2403.08331v1,http://arxiv.org/abs/2403.08331v1,2024-03-13 08:34:40+00:00,"Optimization of product and system characteristics is required in many
fields, including design and control. Bayesian optimization (BO) is often used
when there are high observing costs, because BO theoretically guarantees an
upper bound on regret. However, computational costs increase exponentially with
the number of parameters to be optimized, decreasing search efficiency. We
propose a BO that limits the search region to lower dimensions and utilizes
local Gaussian process regression (LGPR) to scale the BO to higher dimensions.
LGPR treats the low-dimensional search region as ""local,"" improving prediction
accuracies there. The LGPR model is trained on a local subset of data specific
to that region. This improves prediction accuracy and search efficiency and
reduces the time complexity of matrix inversion in the Gaussian process
regression. In evaluations with 20D Ackley and Rosenbrock functions, search
efficiencies are equal to or higher than those of the compared methods,
improved by about 69% and 40% from the case without LGPR. We apply our method
to an automatic design task for a power semiconductor device. We successfully
reduce the specific on-resistance to 25% better than a conventional method and
3.4% better than without LGPR.",0
Spatially resolved emission lines in galaxies at $4\leq z < 10$ from the JADES survey: evidence for enhanced central star formation,2403.08431v1,http://arxiv.org/abs/2403.08431v1,2024-03-13 11:31:08+00:00,"We present the first statistical investigation of spatially resolved
emission-line properties in a sample of 63 low-mass galaxies at $4\leq z<10$,
using JWST/NIRSpec MSA data from the JWST Advanced Deep Extragalactic (JADES)
survey focusing on deep, spatially resolved spectroscopy in the GOODS-S
extragalactic field. By performing a stacking of the 2D spectra of the galaxies
in our sample, we find an increasing or flat radial trend with increasing
radius for [OIII]$\lambda5007$/H$\beta$ and a decreasing one for
[NeIII]$\lambda3869$/[OII]$\lambda3727$ (3--4 $\sigma$ significance). These
results are still valid when stacking the sample in two redshift bins (i.e.,
$4\leq z<5.5$ and $5.5\leq z<10$). The comparison with star-formation
photoionization models suggests that the ionization parameter increases by
$\sim 0.5$ dex with redshift. We find a tentative metallicity gradient that
increases with radius (i.e., 'inverted') in both redshift bins. Moreover, our
analysis reveals strong negative gradients for the equivalent width of \Hbeta
(7$\sigma$ significance). This trend persists even after removing known AGN
candidates, therefore, it is consistent with a radial gradient primarily in
stellar age and secondarily in metallicity. Taken all together, our results
suggest that the sample is dominated by active central star formation, with
possibly inverted metallicity gradients sustained by recent episodes of
accretion of pristine gas or strong radial flows. Deeper observations and
larger samples are needed to confirm these preliminary results and to validate
our interpretation.",0
Search-based Optimisation of LLM Learning Shots for Story Point Estimation,2403.08430v1,http://arxiv.org/abs/2403.08430v1,2024-03-13 11:29:37+00:00,"One of the ways Large Language Models (LLMs) are used to perform machine
learning tasks is to provide them with a few examples before asking them to
produce a prediction. This is a meta-learning process known as few-shot
learning. In this paper, we use available Search-Based methods to optimise the
number and combination of examples that can improve an LLM's estimation
performance, when it is used to estimate story points for new agile tasks. Our
preliminary results show that our SBSE technique improves the estimation
performance of the LLM by 59.34% on average (in terms of mean absolute error of
the estimation) over three datasets against a zero-shot setting.",0
DeepCSHAP: Utilizing Shapley Values to Explain Deep Complex-Valued Neural Networks,2403.08428v1,http://arxiv.org/abs/2403.08428v1,2024-03-13 11:26:43+00:00,"Deep Neural Networks are widely used in academy as well as corporate and
public applications, including safety critical applications such as health care
and autonomous driving. The ability to explain their output is critical for
safety reasons as well as acceptance among applicants. A multitude of methods
have been proposed to explain real-valued neural networks. Recently,
complex-valued neural networks have emerged as a new class of neural networks
dealing with complex-valued input data without the necessity of projecting them
onto $\mathbb{R}^2$. This brings up the need to develop explanation algorithms
for this kind of neural networks. In this paper we provide these developments.
While we focus on adapting the widely used DeepSHAP algorithm to the complex
domain, we also present versions of four gradient based explanation methods
suitable for use in complex-valued neural networks. We evaluate the explanation
quality of all presented algorithms and provide all of them as an open source
library adaptable to most recent complex-valued neural network architectures.",0
Specification Overfitting in Artificial Intelligence,2403.08425v1,http://arxiv.org/abs/2403.08425v1,2024-03-13 11:20:34+00:00,"Machine learning (ML) and artificial intelligence (AI) approaches are often
criticized for their inherent bias and for their lack of control,
accountability, and transparency. Consequently, regulatory bodies struggle with
containing this technology's potential negative side effects. High-level
requirements such as fairness and robustness need to be formalized into
concrete specification metrics, imperfect proxies that capture isolated aspects
of the underlying requirements. Given possible trade-offs between different
metrics and their vulnerability to over-optimization, integrating specification
metrics in system development processes is not trivial. This paper defines
specification overfitting, a scenario where systems focus excessively on
specified metrics to the detriment of high-level requirements and task
performance. We present an extensive literature survey to categorize how
researchers propose, measure, and optimize specification metrics in several AI
fields (e.g., natural language processing, computer vision, reinforcement
learning). Using a keyword-based search on papers from major AI conferences and
journals between 2018 and mid-2023, we identify and analyze 74 papers that
propose or optimize specification metrics. We find that although most papers
implicitly address specification overfitting (e.g., by reporting more than one
specification metric), they rarely discuss which role specification metrics
should play in system development or explicitly define the scope and
assumptions behind metric formulations.",0
The Development and Performance of a Machine Learning Based Mobile Platform for Visually Determining the Etiology of Penile Pathology,2403.08417v1,http://arxiv.org/abs/2403.08417v1,2024-03-13 11:05:40+00:00,"Machine-learning algorithms can facilitate low-cost, user-guided visual
diagnostic platforms for addressing disparities in access to sexual health
services. We developed a clinical image dataset using original and augmented
images for five penile diseases: herpes eruption, syphilitic chancres, penile
candidiasis, penile cancer, and genital warts. We used a U-net architecture
model for semantic pixel segmentation into background or subject image, the
Inception-ResNet version 2 neural architecture to classify each pixel as
diseased or non-diseased, and a salience map using GradCAM++. We trained the
model on a random 91% sample of the image database using 150 epochs per image,
and evaluated the model on the remaining 9% of images, assessing recall (or
sensitivity), precision, specificity, and F1-score (accuracy). Of the 239
images in the validation dataset, 45 (18.8%) were of genital warts, 43 (18.0%)
were of HSV infection, 29 (12.1%) were of penile cancer, 40 (16.7%) were of
penile candidiasis, 37 (15.5%) were of syphilitic chancres, and 45 (18.8%) were
of non-diseased penises. The overall accuracy of the model for correctly
classifying the diseased image was 0.944. Between July 1st and October 1st
2023, there were 2,640 unique users of the mobile platform. Among a random
sample of submissions (n=437), 271 (62.0%) were from the United States, 64
(14.6%) from Singapore, 41 (9.4%) from Candia, 40 (9.2%) from the United
Kingdom, and 21 (4.8%) from Vietnam. The majority (n=277 [63.4%]) were between
18 and 30 years old. We report on the development of a machine-learning model
for classifying five penile diseases, which demonstrated excellent performance
on a validation dataset. That model is currently in use globally and has the
potential to improve access to diagnostic services for penile diseases.",0
Causal Graph Neural Networks for Wildfire Danger Prediction,2403.08414v1,http://arxiv.org/abs/2403.08414v1,2024-03-13 10:58:55+00:00,"Wildfire forecasting is notoriously hard due to the complex interplay of
different factors such as weather conditions, vegetation types and human
activities. Deep learning models show promise in dealing with this complexity
by learning directly from data. However, to inform critical decision making, we
argue that we need models that are right for the right reasons; that is, the
implicit rules learned should be grounded by the underlying processes driving
wildfires. In that direction, we propose integrating causality with Graph
Neural Networks (GNNs) that explicitly model the causal mechanism among complex
variables via graph learning. The causal adjacency matrix considers the
synergistic effect among variables and removes the spurious links from highly
correlated impacts. Our methodology's effectiveness is demonstrated through
superior performance forecasting wildfire patterns in the European boreal and
mediterranean biome. The gain is especially prominent in a highly imbalanced
dataset, showcasing an enhanced robustness of the model to adapt to regime
shifts in functional relationships. Furthermore, SHAP values from our trained
model further enhance our understanding of the model's inner workings.",0
Robust Distributed Compression with Learned Heegard-Berger Scheme,2403.08411v1,http://arxiv.org/abs/2403.08411v1,2024-03-13 10:57:22+00:00,"We consider lossy compression of an information source when decoder-only side
information may be absent. This setup, also referred to as the Heegard-Berger
or Kaspi problem, is a special case of robust distributed source coding.
Building upon previous works on neural network-based distributed compressors
developed for the decoder-only side information (Wyner-Ziv) case, we propose
learning-based schemes that are amenable to the availability of side
information. We find that our learned compressors mimic the achievability part
of the Heegard-Berger theorem and yield interpretable results operating close
to information-theoretic bounds. Depending on the availability of the side
information, our neural compressors recover characteristics of the
point-to-point (i.e., with no side information) and the Wyner-Ziv coding
strategies that include binning in the source space, although no structure
exploiting knowledge of the source and side information was imposed into the
design.",0
Reduced Jeffries-Matusita distance: A Novel Loss Function to Improve Generalization Performance of Deep Classification Models,2403.08408v1,http://arxiv.org/abs/2403.08408v1,2024-03-13 10:51:38+00:00,"The generalization performance of deep neural networks in classification
tasks is a major concern in machine learning research. Despite widespread
techniques used to diminish the over-fitting issue such as data augmentation,
pseudo-labeling, regularization, and ensemble learning, this performance still
needs to be enhanced with other approaches. In recent years, it has been
theoretically demonstrated that the loss function characteristics i.e. its
Lipschitzness and maximum value affect the generalization performance of deep
neural networks which can be utilized as a guidance to propose novel distance
measures. In this paper, by analyzing the aforementioned characteristics, we
introduce a distance called Reduced Jeffries-Matusita as a loss function for
training deep classification models to reduce the over-fitting issue. In our
experiments, we evaluate the new loss function in two different problems: image
classification in computer vision and node classification in the context of
graph learning. The results show that the new distance measure stabilizes the
training process significantly, enhances the generalization ability, and
improves the performance of the models in the Accuracy and F1-score metrics,
even if the training set size is small.",0
FSDR: A Novel Deep Learning-based Feature Selection Algorithm for Pseudo Time-Series Data using Discrete Relaxation,2403.08403v1,http://arxiv.org/abs/2403.08403v1,2024-03-13 10:37:52+00:00,"Conventional feature selection algorithms applied to Pseudo Time-Series (PTS)
data, which consists of observations arranged in sequential order without
adhering to a conventional temporal dimension, often exhibit impractical
computational complexities with high dimensional data. To address this
challenge, we introduce a Deep Learning (DL)-based feature selection algorithm:
Feature Selection through Discrete Relaxation (FSDR), tailored for PTS data.
Unlike the existing feature selection algorithms, FSDR learns the important
features as model parameters using discrete relaxation, which refers to the
process of approximating a discrete optimisation problem with a continuous one.
FSDR is capable of accommodating a high number of feature dimensions, a
capability beyond the reach of existing DL-based or traditional methods.
Through testing on a hyperspectral dataset (i.e., a type of PTS data), our
experimental results demonstrate that FSDR outperforms three commonly used
feature selection algorithms, taking into account a balance among execution
time, $R^2$, and $RMSE$.",0
Interactive environments for training children's curiosity through the practice of metacognitive skills: a pilot study,2403.08397v1,http://arxiv.org/abs/2403.08397v1,2024-03-13 10:21:32+00:00,"Curiosity-driven learning has shown significant positive effects on students'
learning experiences and outcomes. But despite this importance, reports show
that children lack this skill, especially in formal educational settings. To
address this challenge, we propose an 8-session workshop that aims to enhance
children's curiosity through training a set of specific metacognitive skills we
hypothesize are involved in its process. Our workshop contains animated videos
presenting declarative knowledge about curiosity and the said metacognitive
skills as well as practice sessions to apply these skills during a
reading-comprehension task, using a web platform designed for this study (e.g.
expressing uncertainty, formulating questions, etc). We conduct a pilot study
with 15 primary school students, aged between 8 and 10. Our first results show
a positive impact on children's metacognitive efficiency and their ability to
express their curiosity through question-asking behaviors.",0
Optimizing Risk-averse Human-AI Hybrid Teams,2403.08386v1,http://arxiv.org/abs/2403.08386v1,2024-03-13 09:49:26+00:00,"We anticipate increased instances of humans and AI systems working together
in what we refer to as a hybrid team. The increase in collaboration is expected
as AI systems gain proficiency and their adoption becomes more widespread.
However, their behavior is not error-free, making hybrid teams a very suitable
solution. As such, we consider methods for improving performance for these
teams of humans and AI systems. For hybrid teams, we will refer to both the
humans and AI systems as agents. To improve team performance over that seen for
agents operating individually, we propose a manager which learns, through a
standard Reinforcement Learning scheme, how to best delegate, over time, the
responsibility of taking a decision to any of the agents. We further guide the
manager's learning so they also minimize how many changes in delegation are
made resulting from undesirable team behavior. We demonstrate the optimality of
our manager's performance in several grid environments which include failure
states which terminate an episode and should be avoided. We perform our
experiments with teams of agents with varying degrees of acceptable risk, in
the form of proximity to a failure state, and measure the manager's ability to
make effective delegation decisions with respect to its own risk-based
constraints, then compare these to the optimal decisions. Our results show our
manager can successfully learn desirable delegations which result in team paths
near/exactly optimal with respect to path length and number of delegations.",0
"RAF-GI: Towards Robust, Accurate and Fast-Convergent Gradient Inversion Attack in Federated Learning",2403.08383v1,http://arxiv.org/abs/2403.08383v1,2024-03-13 09:48:04+00:00,"Federated learning (FL) empowers privacy-preservation in model training by
only exposing users' model gradients. Yet, FL users are susceptible to the
gradient inversion (GI) attack which can reconstruct ground-truth training data
such as images based on model gradients. However, reconstructing
high-resolution images by existing GI attack works faces two challenges:
inferior accuracy and slow-convergence, especially when the context is
complicated, e.g., the training batch size is much greater than 1 on each FL
user. To address these challenges, we present a Robust, Accurate and
Fast-convergent GI attack algorithm, called RAF-GI, with two components: 1)
Additional Convolution Block (ACB) which can restore labels with up to 20%
improvement compared with existing works; 2) Total variance, three-channel mEan
and cAnny edge detection regularization term (TEA), which is a white-box attack
strategy to reconstruct images based on labels inferred by ACB. Moreover,
RAF-GI is robust that can still accurately reconstruct ground-truth data when
the users' training batch size is no more than 48. Our experimental results
manifest that RAF-GI can diminish 94% time costs while achieving superb
inversion quality in ImageNet dataset. Notably, with a batch size of 1, RAF-GI
exhibits a 7.89 higher Peak Signal-to-Noise Ratio (PSNR) compared to the
state-of-the-art baselines.",0
Learning to Describe for Predicting Zero-shot Drug-Drug Interactions,2403.08377v1,http://arxiv.org/abs/2403.08377v1,2024-03-13 09:42:46+00:00,"Adverse drug-drug interactions~(DDIs) can compromise the effectiveness of
concurrent drug administration, posing a significant challenge in healthcare.
As the development of new drugs continues, the potential for unknown adverse
effects resulting from DDIs becomes a growing concern. Traditional
computational methods for DDI prediction may fail to capture interactions for
new drugs due to the lack of knowledge. In this paper, we introduce a new
problem setup as zero-shot DDI prediction that deals with the case of new
drugs. Leveraging textual information from online databases like DrugBank and
PubChem, we propose an innovative approach TextDDI with a language model-based
DDI predictor and a reinforcement learning~(RL)-based information selector,
enabling the selection of concise and pertinent text for accurate DDI
prediction on new drugs. Empirical results show the benefits of the proposed
approach on several settings including zero-shot and few-shot DDI prediction,
and the selected texts are semantically relevant. Our code and data are
available at \url{https://github.com/zhufq00/DDIs-Prediction}.",0
Nonlinear Manifold Learning Determines Microgel Size from Raman Spectroscopy,2403.08376v1,http://arxiv.org/abs/2403.08376v1,2024-03-13 09:39:15+00:00,"Polymer particle size constitutes a crucial characteristic of product quality
in polymerization. Raman spectroscopy is an established and reliable process
analytical technology for in-line concentration monitoring. Recent approaches
and some theoretical considerations show a correlation between Raman signals
and particle sizes but do not determine polymer size from Raman spectroscopic
measurements accurately and reliably. With this in mind, we propose three
alternative machine learning workflows to perform this task, all involving
diffusion maps, a nonlinear manifold learning technique for dimensionality
reduction: (i) directly from diffusion maps, (ii) alternating diffusion maps,
and (iii) conformal autoencoder neural networks. We apply the workflows to a
data set of Raman spectra with associated size measured via dynamic light
scattering of 47 microgel (cross-linked polymer) samples in a diameter range of
208nm to 483 nm. The conformal autoencoders substantially outperform
state-of-the-art methods and results for the first time in a promising
prediction of polymer size from Raman spectra.",0
SMART: Submodular Data Mixture Strategy for Instruction Tuning,2403.08370v1,http://arxiv.org/abs/2403.08370v1,2024-03-13 09:31:50+00:00,"Instruction Tuning involves finetuning a language model on a collection of
instruction-formatted datasets in order to enhance the generalizability of the
model to unseen tasks. Studies have shown the importance of balancing different
task proportions during finetuning, but finding the right balance remains
challenging. Unfortunately, there's currently no systematic method beyond
manual tuning or relying on practitioners' intuition. In this paper, we
introduce SMART (Submodular data Mixture strAtegy for instRuction Tuning) - a
novel data mixture strategy which makes use of a submodular function to assign
importance scores to tasks which are then used to determine the mixture
weights. Given a fine-tuning budget, SMART redistributes the budget among tasks
and selects non-redundant samples from each task. Experimental results
demonstrate that SMART significantly outperforms traditional methods such as
examples proportional mixing and equal mixing. Furthermore, SMART facilitates
the creation of data mixtures based on a few representative subsets of tasks
alone and through task pruning analysis, we reveal that in a limited budget
setting, allocating budget among a subset of representative tasks yields
superior performance compared to distributing the budget among all tasks. The
code for reproducing our results is open-sourced at
https://github.com/kowndinya-renduchintala/SMART.",0
METER: a mobile vision transformer architecture for monocular depth estimation,2403.08368v1,http://arxiv.org/abs/2403.08368v1,2024-03-13 09:30:08+00:00,"Depth estimation is a fundamental knowledge for autonomous systems that need
to assess their own state and perceive the surrounding environment. Deep
learning algorithms for depth estimation have gained significant interest in
recent years, owing to the potential benefits of this methodology in overcoming
the limitations of active depth sensing systems. Moreover, due to the low cost
and size of monocular cameras, researchers have focused their attention on
monocular depth estimation (MDE), which consists in estimating a dense depth
map from a single RGB video frame. State of the art MDE models typically rely
on vision transformers (ViT) architectures that are highly deep and complex,
making them unsuitable for fast inference on devices with hardware constraints.
Purposely, in this paper, we address the problem of exploiting ViT in MDE on
embedded devices. Those systems are usually characterized by limited memory
capabilities and low-power CPU/GPU. We propose METER, a novel lightweight
vision transformer architecture capable of achieving state of the art
estimations and low latency inference performances on the considered embedded
hardwares: NVIDIA Jetson TX1 and NVIDIA Jetson Nano. We provide a solution
consisting of three alternative configurations of METER, a novel loss function
to balance pixel estimation and reconstruction of image details, and a new data
augmentation strategy to improve the overall final predictions. The proposed
method outperforms previous lightweight works over the two benchmark datasets:
the indoor NYU Depth v2 and the outdoor KITTI.",0
Decoupled Federated Learning on Long-Tailed and Non-IID data with Feature Statistics,2403.08364v1,http://arxiv.org/abs/2403.08364v1,2024-03-13 09:24:59+00:00,"Federated learning is designed to enhance data security and privacy, but
faces challenges when dealing with heterogeneous data in long-tailed and
non-IID distributions. This paper explores an overlooked scenario where tail
classes are sparsely distributed over a few clients, causing the models trained
with these classes to have a lower probability of being selected during client
aggregation, leading to slower convergence rates and poorer model performance.
To address this issue, we propose a two-stage Decoupled Federated learning
framework using Feature Statistics (DFL-FS). In the first stage, the server
estimates the client's class coverage distributions through masked local
feature statistics clustering to select models for aggregation to accelerate
convergence and enhance feature learning without privacy leakage. In the second
stage, DFL-FS employs federated feature regeneration based on global feature
statistics and utilizes resampling and weighted covariance to calibrate the
global classifier to enhance the model's adaptability to long-tailed data
distributions. We conducted experiments on CIFAR10-LT and CIFAR100-LT datasets
with various long-tailed rates. The results demonstrate that our method
outperforms state-of-the-art methods in both accuracy and convergence rate.",0
Mean-Field Microcanonical Gradient Descent,2403.08362v1,http://arxiv.org/abs/2403.08362v1,2024-03-13 09:22:30+00:00,"Microcanonical gradient descent is a sampling procedure for energy-based
models allowing for efficient sampling of distributions in high dimension. It
works by transporting samples from a high-entropy distribution, such as
Gaussian white noise, to a low-energy region using gradient descent. We put
this model in the framework of normalizing flows, showing how it can often
overfit by losing an unnecessary amount of entropy in the descent. As a remedy,
we propose a mean-field microcanonical gradient descent that samples several
weakly coupled data points simultaneously, allowing for better control of the
entropy loss while paying little in terms of likelihood fit. We study these
models in the context of financial time series, illustrating the improvements
on both synthetic and real data.",0
NaturalVLM: Leveraging Fine-grained Natural Language for Affordance-Guided Visual Manipulation,2403.08355v1,http://arxiv.org/abs/2403.08355v1,2024-03-13 09:12:16+00:00,"Enabling home-assistant robots to perceive and manipulate a diverse range of
3D objects based on human language instructions is a pivotal challenge. Prior
research has predominantly focused on simplistic and task-oriented
instructions, i.e., ""Slide the top drawer open"". However, many real-world tasks
demand intricate multi-step reasoning, and without human instructions, these
will become extremely difficult for robot manipulation. To address these
challenges, we introduce a comprehensive benchmark, NrVLM, comprising 15
distinct manipulation tasks, containing over 4500 episodes meticulously
annotated with fine-grained language instructions. We split the long-term task
process into several steps, with each step having a natural language
instruction. Moreover, we propose a novel learning framework that completes the
manipulation task step-by-step according to the fine-grained instructions.
Specifically, we first identify the instruction to execute, taking into account
visual observations and the end-effector's current state. Subsequently, our
approach facilitates explicit learning through action-prompts and
perception-prompts to promote manipulation-aware cross-modality alignment.
Leveraging both visual observations and linguistic guidance, our model outputs
a sequence of actionable predictions for manipulation, including contact points
and end-effector poses. We evaluate our method and baselines using the proposed
benchmark NrVLM. The experimental results demonstrate the effectiveness of our
approach. For additional details, please refer to
https://sites.google.com/view/naturalvlm.",0
Data augmentation with automated machine learning: approaches and performance comparison with classical data augmentation methods,2403.08352v1,http://arxiv.org/abs/2403.08352v1,2024-03-13 09:00:38+00:00,"Data augmentation is arguably the most important regularization technique
commonly used to improve generalization performance of machine learning models.
It primarily involves the application of appropriate data transformation
operations to create new data samples with desired properties. Despite its
effectiveness, the process is often challenging because of the time-consuming
trial and error procedures for creating and testing different candidate
augmentations and their hyperparameters manually. Automated data augmentation
methods aim to automate the process. State-of-the-art approaches typically rely
on automated machine learning (AutoML) principles. This work presents a
comprehensive survey of AutoML-based data augmentation techniques. We discuss
various approaches for accomplishing data augmentation with AutoML, including
data manipulation, data integration and data synthesis techniques. We present
extensive discussion of techniques for realizing each of the major subtasks of
the data augmentation process: search space design, hyperparameter optimization
and model evaluation. Finally, we carried out an extensive comparison and
analysis of the performance of automated data augmentation techniques and
state-of-the-art methods based on classical augmentation approaches. The
results show that AutoML methods for data augmentation currently outperform
state-of-the-art techniques based on conventional approaches.",0
From human experts to machines: An LLM supported approach to ontology and knowledge graph construction,2403.08345v1,http://arxiv.org/abs/2403.08345v1,2024-03-13 08:50:15+00:00,"The conventional process of building Ontologies and Knowledge Graphs (KGs)
heavily relies on human domain experts to define entities and relationship
types, establish hierarchies, maintain relevance to the domain, fill the ABox
(or populate with instances), and ensure data quality (including amongst others
accuracy and completeness). On the other hand, Large Language Models (LLMs)
have recently gained popularity for their ability to understand and generate
human-like natural language, offering promising ways to automate aspects of
this process. This work explores the (semi-)automatic construction of KGs
facilitated by open-source LLMs. Our pipeline involves formulating competency
questions (CQs), developing an ontology (TBox) based on these CQs, constructing
KGs using the developed ontology, and evaluating the resultant KG with minimal
to no involvement of human experts. We showcase the feasibility of our
semi-automated pipeline by creating a KG on deep learning methodologies by
exploiting scholarly publications. To evaluate the answers generated via
Retrieval-Augmented-Generation (RAG) as well as the KG concepts automatically
extracted using LLMs, we design a judge LLM, which rates the generated content
based on ground truth. Our findings suggest that employing LLMs could
potentially reduce the human effort involved in the construction of KGs,
although a human-in-the-loop approach is recommended to evaluate automatically
generated KGs.",0
STMPL: Human Soft-Tissue Simulation,2403.08344v1,http://arxiv.org/abs/2403.08344v1,2024-03-13 08:49:40+00:00,"In various applications, such as virtual reality and gaming, simulating the
deformation of soft tissues in the human body during interactions with external
objects is essential. Traditionally, Finite Element Methods (FEM) have been
employed for this purpose, but they tend to be slow and resource-intensive. In
this paper, we propose a unified representation of human body shape and soft
tissue with a data-driven simulator of non-rigid deformations. This approach
enables rapid simulation of realistic interactions.
  Our method builds upon the SMPL model, which generates human body shapes
considering rigid transformations. We extend SMPL by incorporating a soft
tissue layer and an intuitive representation of external forces applied to the
body during object interactions. Specifically, we mapped the 3D body shape and
soft tissue and applied external forces to 2D UV maps. Leveraging a UNET
architecture designed for 2D data, our approach achieves high-accuracy
inference in real time. Our experiment shows that our method achieves plausible
deformation of the soft tissue layer, even for unseen scenarios.",0
Suppression of diffraction in deep-inelastic scattering on nuclei and dynamical mechanism of leading twist nuclear shadowing,2403.08342v1,http://arxiv.org/abs/2403.08342v1,2024-03-13 08:45:36+00:00,"Using the leading twist approach (LTA) to nuclear shadowing, we calculate the
ratios of diffractive and usual parton distributions for a heavy nucleus (Pb)
and the proton, $R_{A/p}=(f_{i/A}^{D(3)}/f_{i/A})/(f_{i/p}^{D(3)}/f_{i/p})$,
for coherent and summed (coherent plus quasi-elastic) nuclear deep-inelastic
scattering. We find that $R_{A/p} \approx 0.5-1$ for quarks and $R_{A/p}
\approx 0.5-1.3$ for gluons in a broad range of $x$, which reaffirms the
difference from the nuclear enhancement of $R_{A/p}$ predicted in the gluon
saturation framework. We demonstrate that the magnitude of $R_{A/p}$ is
controlled by the cross section of the interaction of hadronic fluctuations of
the virtual photon with target nucleons, which explains an enhancement of
$R_{A/p}$ in the color dipole model and its suppression in LTA. We argue that
the black disk limit of LTA corresponds to $R_{A/p}=1$ and $R^{\rm
coh}_{A/p}=0.86$ for the summed and coherent scattering, respectively. Relying
on an intuitive definition of the saturation scale, we show that the ratio of
the saturation scales of a heavy nucleus and proton $Q_{sA}^2(b)/Q_{sp}^2(b)
\approx 1$ at small impact parameters $b$ due to the strong leading twist
nuclear shadowing and diluteness of the nuclear density.",0
LLM-Assisted Light: Leveraging Large Language Model Capabilities for Human-Mimetic Traffic Signal Control in Complex Urban Environments,2403.08337v1,http://arxiv.org/abs/2403.08337v1,2024-03-13 08:41:55+00:00,"Traffic congestion in metropolitan areas presents a formidable challenge with
far-reaching economic, environmental, and societal ramifications. Therefore,
effective congestion management is imperative, with traffic signal control
(TSC) systems being pivotal in this endeavor. Conventional TSC systems,
designed upon rule-based algorithms or reinforcement learning (RL), frequently
exhibit deficiencies in managing the complexities and variabilities of urban
traffic flows, constrained by their limited capacity for adaptation to
unfamiliar scenarios. In response to these limitations, this work introduces an
innovative approach that integrates Large Language Models (LLMs) into TSC,
harnessing their advanced reasoning and decision-making faculties.
Specifically, a hybrid framework that augments LLMs with a suite of perception
and decision-making tools is proposed, facilitating the interrogation of both
the static and dynamic traffic information. This design places the LLM at the
center of the decision-making process, combining external traffic data with
established TSC methods. Moreover, a simulation platform is developed to
corroborate the efficacy of the proposed framework. The findings from our
simulations attest to the system's adeptness in adjusting to a multiplicity of
traffic environments without the need for additional training. Notably, in
cases of Sensor Outage (SO), our approach surpasses conventional RL-based
systems by reducing the average waiting time by $20.4\%$. This research
signifies a notable advance in TSC strategies and paves the way for the
integration of LLMs into real-world, dynamic scenarios, highlighting their
potential to revolutionize traffic management. The related code is available at
\href{https://github.com/Traffic-Alpha/LLM-Assisted-Light}{https://github.com/Traffic-Alpha/LLM-Assisted-Light}.",0
A Sparsity Principle for Partially Observable Causal Representation Learning,2403.08335v1,http://arxiv.org/abs/2403.08335v1,2024-03-13 08:40:49+00:00,"Causal representation learning aims at identifying high-level causal
variables from perceptual data. Most methods assume that all latent causal
variables are captured in the high-dimensional observations. We instead
consider a partially observed setting, in which each measurement only provides
information about a subset of the underlying causal state. Prior work has
studied this setting with multiple domains or views, each depending on a fixed
subset of latents. Here, we focus on learning from unpaired observations from a
dataset with an instance-dependent partial observability pattern. Our main
contribution is to establish two identifiability results for this setting: one
for linear mixing functions without parametric assumptions on the underlying
causal model, and one for piecewise linear mixing functions with Gaussian
latent causal variables. Based on these insights, we propose two methods for
estimating the underlying causal variables by enforcing sparsity in the
inferred representation. Experiments on different simulated datasets and
established benchmarks highlight the effectiveness of our approach in
recovering the ground-truth latents.",0
Cyclic Data Parallelism for Efficient Parallelism of Deep Neural Networks,2403.08837v1,http://arxiv.org/abs/2403.08837v1,2024-03-13 08:39:21+00:00,"Training large deep learning models requires parallelization techniques to
scale. In existing methods such as Data Parallelism or ZeRO-DP, micro-batches
of data are processed in parallel, which creates two drawbacks: the total
memory required to store the model's activations peaks at the end of the
forward pass, and gradients must be simultaneously averaged at the end of the
backpropagation step. We propose Cyclic Data Parallelism, a novel paradigm
shifting the execution of the micro-batches from simultaneous to sequential,
with a uniform delay. At the cost of a slight gradient delay, the total memory
taken by activations is constant, and the gradient communications are balanced
during the training step. With Model Parallelism, our technique reduces the
number of GPUs needed, by sharing GPUs across micro-batches. Within the ZeRO-DP
framework, our technique allows communication of the model states with
point-to-point operations rather than a collective broadcast operation. We
illustrate the strength of our approach on the CIFAR-10 and ImageNet datasets.",0
Fast Inference of Removal-Based Node Influence,2403.08333v1,http://arxiv.org/abs/2403.08333v1,2024-03-13 08:37:31+00:00,"Graph neural networks (GNNs) are widely utilized to capture the information
spreading patterns in graphs. While remarkable performance has been achieved,
there is a new trending topic of evaluating node influence. We propose a new
method of evaluating node influence, which measures the prediction change of a
trained GNN model caused by removing a node. A real-world application is, ""In
the task of predicting Twitter accounts' polarity, had a particular account
been removed, how would others' polarity change?"". We use the GNN as a
surrogate model whose prediction could simulate the change of nodes or edges
caused by node removal. To obtain the influence for every node, a
straightforward way is to alternately remove every node and apply the trained
GNN on the modified graph. It is reliable but time-consuming, so we need an
efficient method. The related lines of work, such as graph adversarial attack
and counterfactual explanation, cannot directly satisfy our needs, since they
do not focus on the global influence score for every node. We propose an
efficient and intuitive method, NOde-Removal-based fAst GNN inference (NORA),
which uses the gradient to approximate the node-removal influence. It only
costs one forward propagation and one backpropagation to approximate the
influence score for all nodes. Extensive experiments on six datasets and six
GNN models verify the effectiveness of NORA. Our code is available at
https://github.com/weikai-li/NORA.git.",0
"Federated Learning: Attacks, Defenses, Opportunities, and Challenges",2403.06067v1,http://arxiv.org/abs/2403.06067v1,2024-03-10 03:05:59+00:00,"Using dispersed data and training, federated learning (FL) moves AI
capabilities to edge devices or does tasks locally. Many consider FL the start
of a new era in AI, yet it is still immature. FL has not garnered the
community's trust since its security and privacy implications are
controversial. FL's security and privacy concerns must be discovered, analyzed,
and recorded before widespread usage and adoption. A solid comprehension of
risk variables allows an FL practitioner to construct a secure environment and
provide researchers with a clear perspective of potential study fields, making
FL the best solution in situations where security and privacy are primary
issues. This research aims to deliver a complete overview of FL's security and
privacy features to help bridge the gap between current federated AI and broad
adoption in the future. In this paper, we present a comprehensive overview of
the attack surface to investigate FL's existing challenges and defense measures
to evaluate its robustness and reliability. According to our study, security
concerns regarding FL are more frequent than privacy issues. Communication
bottlenecks, poisoning, and backdoor attacks represent FL's privacy's most
significant security threats. In the final part, we detail future research that
will assist FL in adapting to real-world settings.",0
